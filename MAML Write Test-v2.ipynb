{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/30656777/how-to-call-module-written-with-argparse-in-ipython-notebook\n",
    "# REMOVE THIS ONCE YOU PUT IT IN A MODULE!\n",
    "import sys\n",
    "sys.argv = ['']\n",
    "# END\n",
    "\n",
    "import os, time\n",
    "from pytorch_transformers import AdamW\n",
    "from fp16 import FP16_Module, FP16_Optimizer\n",
    "import json, csv\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "import logging\n",
    "import warnings\n",
    "from metrics import compute_metrics\n",
    "\n",
    "from knight_utils import (\n",
    "    FILL_VAL, MODEL_BASE_DIR, TASK_DICT , QADataset, DEVICE, \n",
    "    args, MODEL_CONFIG, MODEL_CLASS, TOKENIZER, CONFIG_CLASS, SPECIAL_TOKEN_IDS, SPECIAL_TOKENS, TOKENS_WEIGHT, \n",
    "    create_dataloader, get_gen_token, CONFIG_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/root/LAMOL/lamol_data\"\n",
    "MODEL_BASE_DIR = \"/data/model_runs\"\n",
    "# MODEL_DIR_NAME = \"20210630T184801_mbs_SEQ_MAML\"\n",
    "# MODEL_DIR_NAME = \"20210701T033914_mbs_SEQ\"\n",
    "MODEL_DIR_NAME = \"20210701T073948_msb_SEQ_MAML\"\n",
    "# MODEL_DIR_NAME = \"20210702T034545_msb_SEQ\"\n",
    "# MODEL_DIR_NAME = \"20210701T180911_sbm_SEQ_MAML\"\n",
    "\n",
    "MODEL_DIR_NAME = \"20210824T030642_mbs_SEQ_MAML\" # Special of movie_steps101.model. Save special in between\n",
    "MODEL_DIR_NAME = \"20210824T035319_mbs_SEQ_MAML_savenetpibetween\" # special of movie_steps101.model \n",
    "MODEL_DIR_NAME = \"20210824T053734_mbs_SEQ_MAML_loadstatedict\" # special load state dict\n",
    "MODEL_DIR_NAME = \"20210829T003703_mbs_SEQ_MAML_v2\" # special load state dict\n",
    "\n",
    "MODEL_DIR = os.path.join(MODEL_BASE_DIR,MODEL_DIR_NAME)\n",
    "\n",
    "# tasks = ['movie',  'scifact', 'boolq']\n",
    "tasks = ['movie', 'boolq',  'scifact']\n",
    "\n",
    "DEVICE = 'cuda:0'\n",
    "temperature_qa = 1.0\n",
    "n_train_epochs = 1\n",
    "num_updates = 5\n",
    "\n",
    "train_batch_size = 3\n",
    "test_batch_size = 1\n",
    "\n",
    "gen_lm_sample_percentage = 0.05\n",
    "\n",
    "top_k_qa = 20\n",
    "top_p_qa = 0.\n",
    "\n",
    "FILL_VAL = -1\n",
    "n_gpus = 1\n",
    "device_ids = [1]\n",
    "\n",
    "logging.basicConfig(filename=f'{MODEL_DIR}/test_run.log', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_id(idx, need_process, all_pasts):\n",
    "    assert idx in need_process\n",
    "    del need_process[idx]\n",
    "    for layer_id in range(MODEL_CONFIG.n_layer):\n",
    "        all_pasts[layer_id][idx] = 0\n",
    "        \n",
    "        \n",
    "def sample_sequence(model, need_process, qa_results, all_pasts, max_tot_lens):\n",
    "    while len(need_process) > 0:                       # While there is still any need_process\n",
    "        first_id = next(iter(need_process))            # The first one to process\n",
    "        shortest_len = len(qa_results[first_id])       # The shortest length is the length of itself?\n",
    "#         decode_batch_size = int(args.memory_sizes[0] * MEMORY_FACTOR[args.seq_train_type] // (shortest_len+1)**LEN_FACTOR)\n",
    "        decode_batch_size = test_batch_size\n",
    "        it = iter(need_process)                        # it is iterable of need_process\n",
    "        stop = False\n",
    "        remove_ids = []\n",
    "        while not stop:\n",
    "            batch_ids, input_ids, past = [], [], [[] for _ in range(MODEL_CONFIG.n_layer)]\n",
    "            while True:\n",
    "                try:\n",
    "                    cur_id = next(it)                   # let the current id be the next batch of need_process\n",
    "                    if len(qa_results[cur_id]) > shortest_len:  # if the length is too long, just stop\n",
    "                        stop = True\n",
    "                        break\n",
    "                    batch_ids.append(cur_id)            \n",
    "                    input_ids.append(qa_results[cur_id][-1:])\n",
    "                    for layer_id in range(MODEL_CONFIG.n_layer):\n",
    "                        past[layer_id].append(all_pasts[layer_id][cur_id])\n",
    "                    if len(input_ids) == decode_batch_size:\n",
    "                        break\n",
    "                except StopIteration:                    # if there is no more id in need_process, just stop\n",
    "                    stop = True\n",
    "                    break\n",
    "\n",
    "            n_inputs = len(input_ids)\n",
    "            if n_inputs == 0:\n",
    "                break\n",
    "            input_ids = torch.stack(input_ids)\n",
    "            for layer_id in range(MODEL_CONFIG.n_layer):\n",
    "                past[layer_id] = torch.stack(past[layer_id], dim=1)\n",
    "            all_outputs = model(input_ids=input_ids.cuda(), past=past)\n",
    "\n",
    "            outputs = all_outputs[0]\n",
    "            pasts = all_outputs[1]\n",
    "\n",
    "            next_logits = outputs[..., -1, :] / temperature_qa\n",
    "            next_tokens = logits_to_tokens(next_logits).cpu()\n",
    "\n",
    "            for i, cur_id in enumerate(batch_ids):\n",
    "                if next_tokens[i] == SPECIAL_TOKEN_IDS[\"eos_token\"]:\n",
    "                    remove_ids.append(cur_id)\n",
    "                else:\n",
    "                    qa_results[cur_id] = torch.cat((qa_results[cur_id], next_tokens[i]))\n",
    "                    if len(qa_results[cur_id]) in [max_tot_lens[cur_id], max_len]:\n",
    "                        remove_ids.append(cur_id)\n",
    "                    else:\n",
    "                        for layer_id in range(MODEL_CONFIG.n_layer):\n",
    "                            all_pasts[layer_id][cur_id] = pasts[layer_id][:, i].type(torch.half)\n",
    "        for idx in remove_ids:\n",
    "            remove_id(idx, need_process, all_pasts)\n",
    "\n",
    "def get_gen_token(task):\n",
    "    return '__' + task + '__'\n",
    "\n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "    \"\"\"\n",
    "    # assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits\n",
    "\n",
    "def logits_to_tokens(next_logits):\n",
    "    filtered_logits = top_k_top_p_filtering(next_logits, top_k=top_k_qa, top_p=top_p_qa)\n",
    "    log_probs = F.softmax(filtered_logits, dim=-1)\n",
    "    next_tokens = torch.multinomial(log_probs, num_samples=1)\n",
    "    return next_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_score(task_eval,qa_results,score_dict):\n",
    "\n",
    "    score = compute_metrics(\n",
    "            qa_results,\n",
    "            bleu='iwslt.en.de' in task_eval or 'multinli.in.out' in task_eval,\n",
    "            dialogue='woz.en' in task_eval,\n",
    "            rouge='cnn_dailymail' in task_eval,\n",
    "            logical_form='wikisql' in task_eval,\n",
    "            corpus_f1='zre' in task_eval\n",
    "    )\n",
    "    score_dict[task_eval] = score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_to_one(task_load, task_eval, model, score_dict):\n",
    "    tic_TASK = time.time()\n",
    "    logger.info(\"start to test { task: %s (load) %s (eval)}\" % (task_load, task_eval))\n",
    "    print(\"start to test { task: %s (load) %s (eval)}\" % (task_load, task_eval))\n",
    "\n",
    "    \n",
    "    # Test Dataset : Support (Train QAData) Query (Test QAData)\n",
    "    support_qadata = QADataset(TASK_DICT[task_eval][\"train\"], \"train\", SPECIAL_TOKEN_IDS[task_load])\n",
    "    test_qadata = QADataset(TASK_DICT[task_eval][\"test\"] , \"test\", SPECIAL_TOKEN_IDS[task_load]).sort()\n",
    "    \n",
    "    max_a_len = test_qadata.max_a_len\n",
    "    n_examples = len(test_qadata)\n",
    "    logger.info(\"len of test dataset: {}\".format(n_examples))\n",
    "    print(\"len of test dataset: {}\".format(n_examples))\n",
    "    \n",
    "    ##### Make dataloaders for that particular dataset #####\n",
    "    support_dataloader = create_dataloader(support_qadata, \"train\")\n",
    "    test_dataloader = create_dataloader(test_qadata, \"test\")\n",
    "    \n",
    "    \n",
    "    ##### Stream from that dataset's dataloader #####\n",
    "    iter_support_dataloader = iter(support_dataloader)\n",
    "    iter_test_dataloader = iter(test_dataloader)\n",
    "    \n",
    "\n",
    "    need_process = OrderedDict()\n",
    "    # qa_results is qa_results[cnt]\n",
    "    qa_results = [0 for _ in range(n_examples)]\n",
    "    # All pasts is shape all_pasts[layer_id][cnt]\n",
    "    all_pasts = [[0 for _ in range(n_examples)] for __ in range(MODEL_CONFIG.n_layer)]\n",
    "    # max_tot_lens is qa_results[cnt]\n",
    "    max_tot_lens = [0 for _ in range(n_examples)]\n",
    "\n",
    "    cnt = 0\n",
    "    n_steps = 0\n",
    "    \n",
    "    while True:\n",
    "#     for n_steps, (cq, len_cq, cqa, len_cqa, Y, genX, genY) in enumerate(test_dataloader):\n",
    "        # 1. Get the support data from the train dataloader\n",
    "        #    and the query data from the test dataloader\n",
    "        # Assume that query data >> support data!\n",
    "        try:\n",
    "            \n",
    "            _, _, support_x, _, support_y, _, _ = next(iter_support_dataloader)\n",
    "            query_x, query_x_len, query_x_cqa, _, query_y, _, _ = next(iter_test_dataloader) # Let query get the CQ!\n",
    "\n",
    "            # Different inputs for train and test -> train with batch 3 and test with batch 1\n",
    "            n_inputs_train = sum(_cqa.shape[0] for _cqa in support_x)\n",
    "            n_inputs = sum(_cqa.shape[0] for _cqa in query_x)\n",
    "\n",
    "            # Since we only have 1 GPU, just use the first one, it will separate batches according to the device IDS\n",
    "            support_x = support_x[0]\n",
    "            support_y = support_y[0]\n",
    "            query_x = query_x[0]\n",
    "            query_y = query_y[0]\n",
    "            query_x_len = query_x_len[0] # an array of query x lengths, but test batch size is only1??\n",
    "            query_x_cqa = query_x_cqa[0] #EXTRA DEBUG\n",
    "\n",
    "            support_x = support_x.to(DEVICE)\n",
    "            support_y = support_y.to(DEVICE)\n",
    "            query_x = query_x.to(DEVICE)\n",
    "            query_y = query_y\n",
    "            query_x_cqa = query_x_cqa.to(DEVICE) #EXTRA DEBUG\n",
    "\n",
    "        except StopIteration:\n",
    "            break\n",
    "            \n",
    "    \n",
    "        \n",
    "        # assume n_gpus == 1\n",
    "#         cqs = cqs[0]\n",
    "#         len_cqs = len_cqs[0]\n",
    "#         n_inputs = cqs.shape[0]\n",
    "        \n",
    "        ### START Adaptation Phase ###\n",
    "        # 2. Reinitialize model with parameters from model_path\n",
    "        state_dict = torch.load(model.model_path, map_location='cuda:0')\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.train()\n",
    "        \n",
    "        # Training loss function\n",
    "        train_loss_fct = CrossEntropyLoss(ignore_index=FILL_VAL, weight=TOKENS_WEIGHT)\n",
    "        \n",
    "        # Optimizer\n",
    "        max_grad_norm=1\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=6.25e-5, eps=1e-4)\n",
    "        optimizer = FP16_Optimizer(optimizer, static_loss_scale=None, dynamic_loss_scale=True,\n",
    "                                           dynamic_loss_args={'scale_window': 100, 'min_scale': 1, 'delayed_shift': 2})\n",
    "\n",
    "        \n",
    "        \n",
    "        # 3. Update the weights with the support set\n",
    "        # May update for several steps\n",
    "        for i in range(num_updates):\n",
    "\n",
    "            qa_logits = model(support_x)\n",
    "            # Somehow it also returns attentions in [1]?, this is selecting 0 of what WrapModel is doing \n",
    "            qa_logits = qa_logits[0]\n",
    "            qa_loss = train_loss_fct(qa_logits.transpose(1,2), support_y)\n",
    "            loss = qa_loss\n",
    "\n",
    "            logger.info(f\"[DEBUG] Adaptation loss: {qa_loss.item()}\")\n",
    "            # Update Optimizer\n",
    "            optimizer.backward(loss, update_master_grads=False) # instead of loss.backward() for fp16\n",
    "            optimizer.update_master_grads()\n",
    "            optimizer.clip_master_grads(max_grad_norm)\n",
    "            optimizer.step()\n",
    "            # Ignore this for now\n",
    "#             if not optimizer.overflow:\n",
    "#                 for i in range(n_inputs):\n",
    "#                     scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        ### END Adaptation Phase ###\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        ### START Meta-Learning Phase ###\n",
    "        # 4. After Adaptation, use the query set for test (CQ ONLY)\n",
    "        # model() returns Tuple of length 2: \n",
    "        #  The [0] is a  torch.Size([1, 225, 50260]), and the [1] is 12 of torch.Size([2, 1, 12, 225, 64])\n",
    "        # Thinking that the [0] is the actual output and [1] is the pasts?\n",
    "        all_outputs = model(query_x)\n",
    "        outputs = all_outputs[0]\n",
    "        pasts = all_outputs[1]\n",
    "        next_logits = outputs[range(n_inputs), query_x_len-1, :] / temperature_qa\n",
    "        next_tokens = logits_to_tokens(next_logits).cpu()\n",
    "        \n",
    "        \n",
    "        # EXTRA FOR COMPARE\n",
    "        qa_logits = model(query_x_cqa)[0]\n",
    "        qa_loss = train_loss_fct(qa_logits.transpose(1,2), query_y.to(DEVICE))\n",
    "        logger.info(f\"[DEBUG] QUERY LOSS: {qa_loss.item()}\")\n",
    "        \n",
    "        # Maybe this is not needed in testing since n_inputs is only 1??\n",
    "        for batch_i in range(n_inputs):\n",
    "            # max total length = max answer length + length of cq\n",
    "            max_tot_lens[cnt] = max_a_len + test_qadata[cnt][1] \n",
    "            # add the cq of that particular batch to qa_results (Change it to cpu first!)\n",
    "            qa_results[cnt] = query_x.cpu()[batch_i][:query_x_len[batch_i]]\n",
    "            \n",
    "            # If the next tokens is not eos\n",
    "            if next_tokens[batch_i] != SPECIAL_TOKEN_IDS[\"eos_token\"]:\n",
    "                # Concat the result\n",
    "                qa_results[cnt] = torch.cat((qa_results[cnt], next_tokens[batch_i]))\n",
    "                # if the length is not max yet -> MAXTOT 225 1024\n",
    "                if len(qa_results[cnt]) not in [max_tot_lens[cnt], max_len]:\n",
    "                    # Append need_process of that cnt\n",
    "                    need_process.update([[cnt, None]])\n",
    "                    # Update all pasts\n",
    "                    for layer_id in range(MODEL_CONFIG.n_layer):\n",
    "                        all_pasts[layer_id][cnt] = pasts[layer_id][:, batch_i, ..., :query_x_len[batch_i], :].type(torch.half)\n",
    "            \n",
    "            # Try sample_sequence here! it will get all need_process (should be only 1 batch, and generate all!)\n",
    "            sample_sequence(model, need_process, qa_results, all_pasts, max_tot_lens)\n",
    "            \n",
    "            \n",
    "            \n",
    "            logger.info(f\"[ERROR_ANALYSIS] {task_eval} {cnt}/{n_examples} Predicted Answer {TOKENIZER.decode(qa_results[cnt].tolist())}\")\n",
    "            logger.info(f\"[ERROR_ANALYSIS] {task_eval} {cnt}/{n_examples} Predicted Tokens {qa_results[cnt].tolist()[query_x_len:]}\")\n",
    "\n",
    "            # Do the score calculation here\n",
    "            # The answer of that particular batch to list\n",
    "            Y = query_y[batch_i].tolist()\n",
    "            Y = list(filter(lambda x: x != -1, Y))[:-1]  # remove eos from the answer\n",
    "            logger.info(f\"[ERROR_ANALYSIS] {task_eval} {cnt}/{n_examples} Actual Tokens {Y}\")\n",
    "            Y = ' '.join([str(y) for y in Y]).split(str(SPECIAL_TOKEN_IDS[\"pad_token\"]))\n",
    "            Y = [TOKENIZER.decode(list(map(int, y.split()))) for y in Y]\n",
    "            # Change the QA Results to a decoded version of real answer and predicted answer\n",
    "            qa_results[cnt] = [TOKENIZER.decode(qa_results[cnt].tolist()[query_x_len:]), Y]\n",
    "            print(f\"Predict vs Actual {cnt}/{n_examples}\", qa_results[cnt])\n",
    "            logger.info(f\"[ERROR_ANALYSIS] {task_eval} {cnt}/{n_examples} Actual Answer {Y}\")\n",
    "            logger.info(f\"[ERROR_ANALYSIS] {task_eval} {cnt}/{n_examples} Predict vs Actual {qa_results[cnt]}\")\n",
    "            \n",
    "            cnt += 1\n",
    "        n_steps += 1\n",
    "    \n",
    "    toc_TASK = time.time() - tic_TASK\n",
    "    logger.info(f'[TIME] TASK {(task_load, task_eval)} {toc_TASK}')\n",
    "    \n",
    "    get_test_score(task_eval, qa_results, score_dict)\n",
    "    print(score_dict)\n",
    "\n",
    "    model_dir = model.model_dir\n",
    "    results_path = os.path.join(model_dir,f\"qa_{task_eval}.csv\")\n",
    "    with open(results_path, \"w\",encoding=\"utf-8\") as f:\n",
    "        qa_writer = csv.writer(f,delimiter=',')\n",
    "        qa_writer.writerow([\"y\",\"pred\"])\n",
    "        for pred, y in qa_results:\n",
    "            qa_writer.writerow([y,pred]) \n",
    "\n",
    "    return model, score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-50aab007eadd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mSPECIAL_TOKEN_IDS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTOKENIZER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#     model_config = CONFIG_CLASS.from_json_file(config_path) # Already defined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMODEL_CLASS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Don't load state dict here, load for every adaptation phase!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_config' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for task in tasks:\n",
    "    \n",
    "    model_path = os.path.join(MODEL_DIR, f\"{task}.model\")\n",
    "    config_path = os.path.join(MODEL_DIR,CONFIG_NAME)\n",
    "\n",
    "    gen_token = get_gen_token(task)\n",
    "    TOKENIZER.add_tokens([gen_token])\n",
    "    SPECIAL_TOKENS[task] = gen_token\n",
    "    SPECIAL_TOKEN_IDS[task] = TOKENIZER.convert_tokens_to_ids(gen_token\n",
    "    model = MODEL_CLASS(MODEL_CONFIG).cuda()\n",
    "    # Don't load state dict here, load for every adaptation phase!\n",
    "    \n",
    "#     print(model)\n",
    "    print(model_path)\n",
    "    \n",
    "    global TOKENS_WEIGHT\n",
    "    if len(TOKENIZER) != TOKENS_WEIGHT.shape[0]:\n",
    "        TOKENS_WEIGHT = torch.cat((TOKENS_WEIGHT, torch.ones([1]).cuda()))\n",
    "    \n",
    "    model.resize_token_embeddings(len(TOKENIZER))\n",
    "    model = FP16_Module(model)\n",
    "    \n",
    "    \n",
    "    model.model_dir = MODEL_DIR\n",
    "    model.model_path = model_path\n",
    "    # Try Loading the state dict like this!\n",
    "    model.state_dict = torch.load(model.model_path, map_location='cuda:0')\n",
    "    logger.info(f\"task: {task}\")\n",
    "    score_dict = {k:None for k in tasks}\n",
    "    \n",
    "    for task_eval in tasks:\n",
    "        test_one_to_one(task, task_eval, model, score_dict)\n",
    "    logger.info(\"score: {}\".format(score_dict))\n",
    "\n",
    "    with open(os.path.join(MODEL_DIR, f\"metrics-{task}.json\"),\"w\") as f:\n",
    "        json.dump(score_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
