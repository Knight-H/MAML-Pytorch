{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, CONFIG_NAME \n",
    "from pytorch_transformers import AdamW\n",
    "from fp16 import FP16_Module, FP16_Optimizer\n",
    "from torch import nn\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "import json\n",
    "from multiprocessing import Pool\n",
    "from parallel import DataParallelModel, DataParallelCriterion\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from scheduler import AnnealingLR\n",
    "import warnings\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "# import GPUtil\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20210630T181500'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks = ['movie', 'boolq', 'scifact']\n",
    "TASK_TRIAD = ''.join([task_name[0] for task_name in tasks])\n",
    "DATETIME = datetime.today().strftime('%Y%m%dT%H%M%S')\n",
    "\n",
    "RUN_ID = DATETIME + \"_\" + TASK_TRIAD\n",
    "\n",
    "logging.basicConfig(filename=f'{RUN_ID}.log', encoding='utf-8', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILL_VAL = -1\n",
    "\n",
    "n_gpus = 1\n",
    "device_ids = [1]\n",
    "\n",
    "train_batch_size = 3\n",
    "test_batch_size = 3\n",
    "\n",
    "data_dir = \"/root/LAMOL/lamol_data\"\n",
    "MODEL_BASE_DIR = \"./model\"\n",
    "n_train_epochs  = 5 \n",
    "\n",
    "min_n_steps = 1500\n",
    "min_batch_size = 4\n",
    "n_train_epochs = 3\n",
    "\n",
    "# Adaptation Phase\n",
    "num_updates = 5\n",
    "\n",
    "DEVICE = 'cuda:0'\n",
    "\n",
    "model_dir = os.path.join(MODEL_BASE_DIR, RUN_ID)\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CLASSES = {\n",
    "    'gpt2': (GPT2LMHeadModel, GPT2Tokenizer, GPT2Config),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_DICT = {\n",
    "    \"movie\": {\n",
    "               \"train\":os.path.join(data_dir,\"movie_train.json\"),\n",
    "               \"eval\":os.path.join(data_dir,\"movie_dev.json\"),\n",
    "               \"test\":os.path.join(data_dir,\"movie_test.json\"),\n",
    "               \"n_train_epochs\": n_train_epochs \n",
    "    },\n",
    "    \"boolq\": {\n",
    "               \"train\":os.path.join(data_dir,\"boolq_train.json\"),\n",
    "               \"eval\":os.path.join(data_dir,\"boolq_dev.json\"),\n",
    "               \"test\":os.path.join(data_dir,\"boolq_test.json\"),\n",
    "               \"n_train_epochs\": n_train_epochs \n",
    "    },\n",
    "    \"scifact\": {\n",
    "               \"train\":os.path.join(data_dir,\"scifact_train.json\"),\n",
    "               \"eval\":os.path.join(data_dir,\"scifact_dev.json\"),\n",
    "               \"test\":os.path.join(data_dir,\"scifact_test.json\"),\n",
    "               \"n_train_epochs\": n_train_epochs \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In settings.py\n",
    "special_tokens = {\"ans_token\":'__ans__', \"pad_token\":'__pad__', \"unk_token\":'__unk__', \"eos_token\": '<|endoftext|>'}\n",
    "\n",
    "model_class, tokenizer_class, config_class = MODEL_CLASSES['gpt2']\n",
    "tokenizer = tokenizer_class.from_pretrained('gpt2')\n",
    "tokenizer.add_tokens(list(special_tokens.values()))\n",
    "special_token_ids = {k:tokenizer.convert_tokens_to_ids(v) for k,v in special_tokens.items()}\n",
    "\n",
    "\n",
    "model_config = config_class.from_pretrained('gpt2')\n",
    "model_config.vocab_size = len(tokenizer)\n",
    "\n",
    "tokens_weight = torch.ones([model_config.vocab_size], dtype=torch.float).to(DEVICE)\n",
    "tokens_weight[special_token_ids[\"ans_token\"]] = 5\n",
    "\n",
    "\n",
    "MODEL_CLASS = model_class\n",
    "TOKENIZER = tokenizer\n",
    "SPECIAL_TOKENS = special_tokens\n",
    "SPECIAL_TOKEN_IDS = special_token_ids\n",
    "TOKENS_WEIGHT = tokens_weight\n",
    "\n",
    "model_config = config_class.from_pretrained('gpt2')\n",
    "max_len = model_config.n_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_losses(parallel_model, cqa, Y, gen_X, gen_Y, loss_fct):\n",
    "    qa_logits = parallel_model(cqa)\n",
    "    lm_logits = parallel_model(gen_X)\n",
    "    qa_loss = loss_fct([torch.transpose(l, 1, 2) for l in qa_logits], Y)\n",
    "    lm_loss = loss_fct([torch.transpose(l, 1, 2) for l in lm_logits], gen_Y)\n",
    "    return torch.mean(qa_loss), args.lm_lambda * torch.mean(lm_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "    def __init__(self, data_paths, data_type, gen_token, extra_data=[]):\n",
    "        self.data_type = data_type\n",
    "        self.gen_token = gen_token\n",
    "        self.ans_token = SPECIAL_TOKEN_IDS[\"ans_token\"]\n",
    "        self.eos_token = SPECIAL_TOKEN_IDS[\"eos_token\"]\n",
    "        self.pad_token = SPECIAL_TOKEN_IDS[\"pad_token\"]\n",
    "\n",
    "        if not isinstance(data_paths, list):\n",
    "            data_paths = [data_paths]\n",
    "\n",
    "        data = []\n",
    "        for data_path in data_paths:\n",
    "            if not data_path:\n",
    "                continue\n",
    "            with open(data_path, \"r\") as f:\n",
    "                raw_ds = json.load(f)\n",
    "            raw_ds = map(lambda x: x[\"paragraphs\"], raw_ds[\"data\"])\n",
    "            d = []\n",
    "            for raw_d in raw_ds:\n",
    "                d.extend(raw_d)\n",
    "            data += d\n",
    "        \n",
    "        self.data = []\n",
    "        self.max_a_len = 0\n",
    "        if len(data) > 0:\n",
    "            self.data_tokenization(data)\n",
    "\n",
    "        if len(extra_data) > 0:\n",
    "            extra_data = map(lambda x: self.etl_single_extra_data(x), extra_data)\n",
    "            extra_data = list(filter(lambda x:x, extra_data))\n",
    "            if args.gen_lm_sample_percentage > 0. and len(extra_data) == 0:\n",
    "                logger.warning(\"No good extra data but sample percentage > 0!\")\n",
    "            self.data += extra_data\n",
    "\n",
    "\n",
    "    def etl_single_extra_data(self, data):\n",
    "        gen_token = data[0]\n",
    "        data = ' '.join([str(datum) for datum in data[1:]])\n",
    "        try:\n",
    "            context = \"\"\n",
    "            qa = data\n",
    "            question, answer = re.split(str(SPECIAL_TOKEN_IDS[\"ans_token\"]), qa)\n",
    "            context = [int(c) for c in context.strip().split()]\n",
    "            question = [int(q) for q in question.strip().split()]\n",
    "            answer = [int(a) for a in re.sub(str(SPECIAL_TOKEN_IDS[\"eos_token\"]), \"\", answer).strip().split()]\n",
    "            uid = uuid.uuid1().hex\n",
    "            data = self.parse_example(gen_token, context, question, answer, uid)\n",
    "        except ValueError:\n",
    "            return\n",
    "        return data\n",
    "\n",
    "    def concat_example(self, gen_token, c, sep_token, q, ans_token, a, eos_token):\n",
    "        example = sep_token + q + ans_token + a\n",
    "        if len(example) + 1 > max_len:\n",
    "            logger.warning('an example with len {} is too long!'.format(len(example) + 1))\n",
    "            return\n",
    "        example = gen_token + c[:max_len-len(example)-1] + example + eos_token\n",
    "        return example\n",
    "\n",
    "    def parse_example(self, gen_token, context, question, answer, idx):\n",
    "        cq_example = self.concat_example([], context, [], question, [self.ans_token], [], [])\n",
    "        cqa_example = self.concat_example([], context, [], question, [self.ans_token], answer, [])\n",
    "        Y_example = self.concat_example([], [], [], [], [], answer, [self.eos_token])\n",
    "        Y_example = [FILL_VAL] * (len(cqa_example) - len(Y_example)) + Y_example\n",
    "        gen_X_example = self.concat_example([gen_token], context, [], question, [self.ans_token], answer, [])\n",
    "        gen_Y_example = self.concat_example([], context, [], question, [self.ans_token], answer, [self.eos_token])\n",
    "        return cq_example, len(cq_example), cqa_example, len(cqa_example), Y_example, gen_X_example, gen_Y_example, idx\n",
    "\n",
    "    def parallel_tokenization(self, d):\n",
    "        # ADD MAX LENGTH FOR MODEL SO IT DOESNT SHOW WARNING\n",
    "        # OLD VERSION OF PYTORCH HUGGINGFACE DOESNT HAVE MAX LENGTH!!!!!\n",
    "        # Suppress the warnings instead! https://stackoverflow.com/questions/14463277/how-to-disable-python-warnings\n",
    "        # Still doesn't work. idk what to do. we can delete all the warnings catches here\n",
    "        examples = []\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            context = TOKENIZER.encode(d[\"context\"])\n",
    "        max_a_len = 0\n",
    "        for qa in d[\"qas\"]:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                question = TOKENIZER.encode(qa[\"question\"])\n",
    "\n",
    "            raw_answers = qa[\"answers\"]\n",
    "            if len(raw_answers) == 0:\n",
    "                assert qa[\"is_impossible\"]\n",
    "                raw_answers.append({\"text\": \"\"})\n",
    "\n",
    "            answer = []\n",
    "            for i, raw_answer in enumerate(raw_answers):\n",
    "                answer.extend(TOKENIZER.encode(raw_answer[\"text\"]))\n",
    "                if i != len(raw_answers) - 1:\n",
    "                    answer.append(self.pad_token)\n",
    "            max_a_len = max(max_a_len, len(answer))\n",
    "\n",
    "            examples.append(self.parse_example(self.gen_token, context, question, answer, qa.get(\"id\", 0)))\n",
    "        return examples, max_a_len\n",
    "\n",
    "    def data_tokenization(self, data):\n",
    "        with Pool(4) as pool:\n",
    "            data = pool.map(self.parallel_tokenization, data)\n",
    "        for datum, max_a_len in data:\n",
    "            self.data.extend(datum)\n",
    "            self.max_a_len = max(self.max_a_len, max_a_len)\n",
    "\n",
    "    def sort(self):\n",
    "        self.data.sort(key=lambda x: len(x[0]))\n",
    "        return self\n",
    "\n",
    "    def sort_by_index(self):\n",
    "        self.data.sort(key=lambda x: x[-1])\n",
    "\n",
    "    def get_indices(self):\n",
    "        return [d[-1] for d in self.data]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicBatchSampler(Sampler):\n",
    "    def __init__(self, dataset, data_type, max_batch_size):\n",
    "        self.dataset = dataset\n",
    "        self.data_type = data_type\n",
    "        if data_type == \"train\":\n",
    "            self.batch_size = train_batch_size\n",
    "        else:\n",
    "            self.batch_size = test_batch_size\n",
    "        self.n_samples = len(dataset)\n",
    "        self.max_batch_size = max_batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.data_type == \"test\":\n",
    "            indices = range(self.n_samples)\n",
    "        else:\n",
    "            indices = np.random.permutation(self.n_samples)\n",
    "        max_len, cnt, st = 0, 0, 0\n",
    "        batch = []\n",
    "        for ed, idx in enumerate(indices):\n",
    "            ln = len(self.dataset[idx][2])\n",
    "            if max(max_len, ln)**LEN_FACTOR * (ed - st + 1) > self.batch_size[cnt]:\n",
    "                st = ed\n",
    "                cnt += 1\n",
    "                max_len = 0\n",
    "                if cnt == args.n_gpus:\n",
    "                    yield batch\n",
    "                    cnt = 0\n",
    "                    batch = []\n",
    "            max_len = max(max_len, ln)\n",
    "            batch.append(idx)\n",
    "            if len(batch) == self.max_batch_size and self.data_type == \"train\":\n",
    "                yield batch\n",
    "                cnt, max_len, st = 0, 0, ed\n",
    "                batch = []\n",
    "        if len(batch) > 0:\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_collate_fn(data, batch_size):\n",
    "\n",
    "    def local_collate():\n",
    "        null_counter = 0\n",
    "        _cqs, _len_cqs, _cqas, _len_cqas, _Ys, _gen_Xs, _gen_Ys = [], [], [], [], [], [], []\n",
    "        Y_max_len = max(len(data[j][4]) for j in range(st, ed))\n",
    "        cq_max_len = max(len(data[j][0]) for j in range(st, ed))\n",
    "        for j in range(st, ed):\n",
    "            if None in data[j] or [] in data[j]:\n",
    "                null_counter+=1\n",
    "                logger.warning('null example in collate_fn, count: {}'.format(null_counter))\n",
    "                continue\n",
    "\n",
    "            pad_len = cqa_max_len - len(data[j][2])\n",
    "\n",
    "            _cqs.append(pad_to_max_len(data[j][0], cq_max_len-len(data[j][0]), SPECIAL_TOKEN_IDS[\"pad_token\"]))\n",
    "            _len_cqs.append(data[j][1])\n",
    "            _cqas.append(pad_to_max_len(data[j][2], pad_len, SPECIAL_TOKEN_IDS[\"pad_token\"]))\n",
    "            _len_cqas.append(data[j][3])\n",
    "            _Ys.append(pad_to_max_len(data[j][4], Y_max_len - len(data[j][4]), FILL_VAL))\n",
    "            _gen_Xs.append(pad_to_max_len(data[j][5], pad_len, SPECIAL_TOKEN_IDS[\"pad_token\"]))\n",
    "            _gen_Ys.append(pad_to_max_len(data[j][6], pad_len, FILL_VAL))\n",
    "\n",
    "        cqs.append(torch.tensor(_cqs))\n",
    "        len_cqs.append(torch.tensor(_len_cqs))\n",
    "        cqas.append(torch.tensor(_cqas))\n",
    "        len_cqas.append(torch.tensor(_len_cqas))\n",
    "        Ys.append(torch.tensor(_Ys))\n",
    "        gen_Xs.append(torch.tensor(_gen_Xs))\n",
    "        gen_Ys.append(torch.tensor(_gen_Ys))\n",
    "\n",
    "    cqs, len_cqs, cqas, len_cqas, Ys, gen_Xs, gen_Ys = [], [], [], [], [], [], []\n",
    "    cqa_max_len, cnt, st = 0, 0, 0\n",
    "    for ed, datum in enumerate(data):\n",
    "        ln = len(datum[2]) # use cqas to calibrate\n",
    "        if max(cqa_max_len, ln)**LEN_FACTOR * (ed - st + 1) > batch_size[cnt]:\n",
    "            local_collate()\n",
    "            cnt += 1\n",
    "            cqa_max_len = 0\n",
    "            st = ed\n",
    "        cqa_max_len = max(cqa_max_len, ln)\n",
    "    ed += 1  # otherwise ed will be len(data)-1\n",
    "    local_collate()\n",
    "\n",
    "    return cqs, len_cqs, cqas, len_cqas, Ys, gen_Xs, gen_Ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def varlen_collate_fn(data):\n",
    "    batch_size = (len(data) + n_gpus - 1) // n_gpus\n",
    "    cqs = torch.tensor(pad_all_to_max_len([datum[0] for datum in data], SPECIAL_TOKEN_IDS[\"pad_token\"])).split(batch_size)\n",
    "    len_cqs = torch.tensor([datum[1] for datum in data]).split(batch_size)\n",
    "    cqas = torch.tensor(pad_all_to_max_len([datum[2] for datum in data], SPECIAL_TOKEN_IDS[\"pad_token\"])).split(batch_size)\n",
    "    len_cqas = torch.tensor([datum[3] for datum in data]).split(batch_size)\n",
    "    Ys = torch.tensor(pad_all_to_max_len([datum[4] for datum in data], FILL_VAL)).split(batch_size)\n",
    "    gen_Xs = torch.tensor(pad_all_to_max_len([datum[5] for datum in data], SPECIAL_TOKEN_IDS[\"pad_token\"])).split(batch_size)\n",
    "    gen_Ys = torch.tensor(pad_all_to_max_len([datum[6] for datum in data], FILL_VAL)).split(batch_size)\n",
    "    return list(cqs), list(len_cqs), list(cqas), list(len_cqas), list(Ys), list(gen_Xs), list(gen_Ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_max_len(l, pad_len, val):\n",
    "    return l + [val] * pad_len\n",
    "def pad_all_to_max_len(ls, val):\n",
    "    max_len = max(len(l) for l in ls)\n",
    "    return [pad_to_max_len(l, max_len-len(l), val) for l in ls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(dataset, data_type, max_batch_size=1000000000):\n",
    "    if data_type == \"train\":\n",
    "        batch_size = train_batch_size\n",
    "    else:\n",
    "        batch_size = test_batch_size\n",
    "\n",
    "    if isinstance(batch_size, list):\n",
    "        collate_fn=lambda x,bs=batch_size: dynamic_collate_fn(x, bs)\n",
    "        shuffle = False\n",
    "        batch_size = 1\n",
    "        batch_sampler = DynamicBatchSampler(dataset, data_type, max_batch_size)\n",
    "    else:\n",
    "        collate_fn=lambda x: varlen_collate_fn(x)\n",
    "#         shuffle = not (data_type != \"train\" or args.debug)\n",
    "        shuffle = False\n",
    "        batch_sampler = None\n",
    "\n",
    "    dataloader =  DataLoader(dataset, num_workers=4,\n",
    "                             collate_fn=collate_fn,\n",
    "                             shuffle=shuffle,\n",
    "                             batch_size=batch_size,\n",
    "                             batch_sampler=batch_sampler)\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gen_token(task):\n",
    "    return '__' + task + '__'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAMOL Dataset\n",
    "\n",
    "`train_qadata` is a pytorch Dataset.  \n",
    "A single datapoint of a returns a list of length 8.  \n",
    "```python\n",
    "return cq_example, len(cq_example), cqa_example, len(cqa_example), Y_example, gen_X_example, gen_Y_example, idx\n",
    "           0                1               2           3                   4       5              6          7\n",
    "# 0 cq_example is context+question+__ans__. ie. [7110, 25, 734, 6036, 11886, 467, 284, 257, 4928, 2151]\n",
    "# 1 len(cq_example) is the length ie. 901\n",
    "# 2 cqa_example is context+question+__ans__+answer ie. [7110, 25, 734, 6036, 11886, 467, 284, 257, 4928, 2151]\n",
    "# 3 len(cqa_example) is the length ie. 903\n",
    "# 4 Y_example is FILL_VALUE+answer only. ie. [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
    "# 5 gen_X_example is __gen__+context+question+__ans__+answer ie. [50260, 7110, 25, 734, 6036, 11886, 467, 284, 257, 4928]\n",
    "# 6 gen_Y_example is context+question+__ans__+answer ie. [7110, 25, 734, 6036, 11886, 467, 284, 257, 4928, 2151]\n",
    "# 7 idx is id (supposed to be uuid? but i don't see it) ie. 0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1185 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1808 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2222 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1044 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1099 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1141 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1073 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1113 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2420 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1091 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1216 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1268 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1165 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1085 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1400 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1373 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1127 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1735 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1168 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1642 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1182 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1244 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1523 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1100 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1638 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1031 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1480 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1037 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1219 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2057 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1381 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1749 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1100 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1181 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1027 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1448 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1309 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1659 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1091 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1050 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1183 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1509 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1218 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1213 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1329 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1324 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1043 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1107 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1198 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1263 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1275 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1164 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1672 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1077 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1150 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1614 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1055 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1181 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1049 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1445 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1105 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1057 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1065 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1101 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1051 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1397 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1218 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1262 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1100 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1072 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1281 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1289 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1324 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1187 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1608 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1156 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1528 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1075 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1556 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1111 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1248 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1025 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1263 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1246 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1301 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1379 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1462 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1120 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1513 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1296 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1094 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1356 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1670 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1945 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1201 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2309 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1048 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1995 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1139 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1153 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1064 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1095 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1057 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1162 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1331 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1040 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1350 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1287 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1114 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1071 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1179 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1158 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1132 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1241 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1102 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2094 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1033 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1183 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1175 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1332 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1067 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1113 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1184 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1096 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1044 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1367 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1044 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1155 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1604 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1158 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1210 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1061 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1301 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1467 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1297 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1043 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1446 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1065 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1072 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1432 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1145 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1136 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1714 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1246 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1050 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1289 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1294 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1472 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1059 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1065 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1116 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1483 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1049 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1126 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1122 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1600 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2011 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1243 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1039 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1324 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1038 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1134 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1259 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1411 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1136 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1236 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1068 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1160 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1157 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1388 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1034 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1240 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1597 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1533 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1049 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1144 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1089 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1131 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1167 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1591 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1450 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1798 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1685 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1236 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1757 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1029 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1381 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1298 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1312 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1180 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1062 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1750 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1137 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1360 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1419 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1924 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1202 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1418 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1047 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1557 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1259 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1180 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1371 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1105 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1436 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1209 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1906 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2236 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1327 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1045 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1059 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1789 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1029 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1135 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1094 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1302 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1229 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1280 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1135 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1248 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1155 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1428 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1187 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1199 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1333 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1489 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2079 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1141 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1088 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1732 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2260 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1183 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2182 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1040 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1077 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1153 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1069 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1190 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1278 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1900 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1096 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1410 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1054 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1833 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1079 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1125 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1048 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1049 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1593 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1121 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1209 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1244 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1227 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1111 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1517 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1441 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1199 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1478 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1076 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1222 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1415 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1697 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1312 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1181 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1598 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1276 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1840 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1596 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2166 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1118 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1174 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1487 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2214 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1406 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1351 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1034 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1845 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1501 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2098 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1906 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1657 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1662 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1451 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1383 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1290 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1855 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1106 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1463 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1515 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1154 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2318 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1489 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1599 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1314 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1598 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1361 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1748 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1410 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1563 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1026 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1171 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1453 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1944 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1188 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1088 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1805 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1115 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1255 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1511 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1168 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1367 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1061 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1446 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1508 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1067 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1418 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1463 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1280 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1112 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1079 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1047 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1547 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1187 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1066 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2481 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1228 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1332 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1148 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1268 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1196 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1341 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1108 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1054 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2022 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1112 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1683 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1171 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3233 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1180 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1676 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1097 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1340 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1398 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1183 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1277 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1235 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1465 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1048 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1364 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1132 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2137 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1639 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1640 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1119 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1463 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1185 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1564 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1159 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1187 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1668 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1067 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1187 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1694 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2917 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1070 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1180 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1072 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1138 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1867 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1563 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1275 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1111 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1333 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1185 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1308 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1084 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1082 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1296 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1421 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1325 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1739 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1030 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1435 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1210 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2827 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1916 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1033 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1067 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1361 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1215 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1547 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1096 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory allocated: 12172.85546875 MiB\n",
      "Episode 1: Loss: 98.00357055664062\n",
      "memory allocated: 12872.00732421875 MiB\n",
      "Episode 2: Loss: 100.80538940429688\n",
      "memory allocated: 11563.5888671875 MiB\n",
      "Episode 3: Loss: 91.32156372070312\n",
      "memory allocated: 12851.9111328125 MiB\n",
      "Episode 4: Loss: 97.4994125366211\n",
      "memory allocated: 12848.3662109375 MiB\n",
      "Episode 5: Loss: 104.6044692993164\n",
      "memory allocated: 10717.36376953125 MiB\n",
      "Episode 6: Loss: 100.3438491821289\n",
      "memory allocated: 12266.951171875 MiB\n",
      "Episode 7: Loss: 75.62680053710938\n",
      "memory allocated: 13466.7998046875 MiB\n",
      "Episode 8: Loss: 77.94529724121094\n",
      "memory allocated: 14279.810546875 MiB\n",
      "Episode 9: Loss: 71.88252258300781\n",
      "memory allocated: 8320.7880859375 MiB\n",
      "Episode 10: Loss: 40.5863037109375\n",
      "memory allocated: 14278.22900390625 MiB\n",
      "Episode 11: Loss: 25.581228256225586\n",
      "memory allocated: 14290.01318359375 MiB\n",
      "Episode 12: Loss: 58.23491668701172\n",
      "memory allocated: 12218.890625 MiB\n",
      "Episode 13: Loss: 12.959354400634766\n",
      "memory allocated: 14286.60302734375 MiB\n",
      "Episode 14: Loss: 7.316319942474365\n",
      "memory allocated: 11999.11962890625 MiB\n",
      "Episode 15: Loss: 7.484495162963867\n",
      "memory allocated: 12998.0595703125 MiB\n",
      "Episode 16: Loss: 10.61409854888916\n",
      "memory allocated: 14282.17822265625 MiB\n",
      "Episode 17: Loss: 11.40890884399414\n",
      "memory allocated: 14288.71484375 MiB\n",
      "Episode 18: Loss: 4.240475654602051\n",
      "memory allocated: 14284.1181640625 MiB\n",
      "Episode 19: Loss: 4.25913143157959\n",
      "memory allocated: 13462.06494140625 MiB\n",
      "Episode 20: Loss: 2.358372926712036\n",
      "memory allocated: 13968.55908203125 MiB\n",
      "Episode 21: Loss: 2.369990825653076\n",
      "memory allocated: 14284.83203125 MiB\n",
      "Episode 22: Loss: 1.095417857170105\n",
      "memory allocated: 13344.43603515625 MiB\n",
      "Episode 23: Loss: 0.8564936518669128\n",
      "memory allocated: 14286.35205078125 MiB\n",
      "Episode 24: Loss: 0.2982368469238281\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-31b3c9394923>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# Update Optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_master_grads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# instead of loss.backward() for fp16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_master_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_master_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MAML-Pytorch/fp16.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, update_master_grads, retain_graph)\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0;31m# backward pass with retain_graph=False to tear down the graph.  Doing this would avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;31m# discarding the iteration,  but probably wouldn't improve overall efficiency.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mupdate_master_grads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_master_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MAML-Pytorch/loss_scaler.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, retain_graph)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mscaled_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;31m##############################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize 2 models\n",
    "net_cls = MODEL_CLASS\n",
    "net = net_cls.from_pretrained('gpt2').to(DEVICE)\n",
    "net.resize_token_embeddings(len(TOKENIZER))\n",
    "net = FP16_Module(net)\n",
    "\n",
    "net_pi = net_cls.from_pretrained('gpt2').to(DEVICE)\n",
    "net_pi.resize_token_embeddings(len(TOKENIZER))\n",
    "net_pi = FP16_Module(net_pi)\n",
    "\n",
    "# Training loss function\n",
    "train_loss_fct = CrossEntropyLoss(ignore_index=FILL_VAL, weight=TOKENS_WEIGHT)\n",
    "\n",
    "# Optimizer\n",
    "max_grad_norm=1\n",
    "param_optimizer = list(net_pi.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=6.25e-5, eps=1e-4)\n",
    "optimizer = FP16_Optimizer(optimizer, static_loss_scale=None, dynamic_loss_scale=True,\n",
    "                                   dynamic_loss_args={'scale_window': 100, 'min_scale': 1, 'delayed_shift': 2})\n",
    "\n",
    "\n",
    "# the optimizer is to update theta parameters, not theta_pi parameters.\n",
    "\n",
    "meta_param_optimizer = list(net.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "meta_optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in meta_param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in meta_param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "meta_optimizer = AdamW(meta_optimizer_grouped_parameters, lr=6.25e-5, eps=1e-4)\n",
    "meta_optimizer = FP16_Optimizer(meta_optimizer, static_loss_scale=None, dynamic_loss_scale=True,\n",
    "                                   dynamic_loss_args={'scale_window': 100, 'min_scale': 1, 'delayed_shift': 2})\n",
    "\n",
    "# Sequential Tasks\n",
    "for task_id in range(len(tasks)):\n",
    "    tic_TASK = time.time()\n",
    "    \n",
    "    \n",
    "    _tasks = [tasks[task_id]]\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##### Start training on task_id #####\n",
    "    gen_token = get_gen_token(_tasks[0])\n",
    "    TOKENIZER.add_tokens([gen_token])\n",
    "    # TOKENIZER.save_pretrained(model_dir)\n",
    "    SPECIAL_TOKENS[_tasks[0]] = gen_token\n",
    "    SPECIAL_TOKEN_IDS[_tasks[0]] = TOKENIZER.convert_tokens_to_ids(gen_token)\n",
    "    # logger.info('gen token = {} , gen token id = {}'.format(gen_token, SPECIAL_TOKEN_IDS[tasks[0]]))\n",
    "    # MODEL_CONFIG.vocab_size = len(TOKENIZER)\n",
    "    # MODEL_CONFIG.to_json_file(os.path.join(model_dir,CONFIG_NAME))\n",
    "\n",
    "    ##### Get Extra data and that particular dataset #####\n",
    "    train_extra_data = []\n",
    "    train_dataset = [TASK_DICT[t][\"train\"] for t in _tasks]\n",
    "    train_qadata = QADataset(train_dataset, \"train\", SPECIAL_TOKEN_IDS[_tasks[0]], train_extra_data)\n",
    "    max_train_batch_size = max(len(train_qadata) // min_n_steps, min_batch_size)\n",
    "    train_dataloader = create_dataloader(train_qadata, \"train\", max_train_batch_size)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Scheduler\n",
    "    n_train_optimization_steps = len(train_qadata) * n_train_epochs\n",
    "    scheduler = AnnealingLR(optimizer, start_lr=6.25e-5, warmup_iter=int(0.005*len(train_qadata)),\n",
    "            num_iters=int(n_train_optimization_steps), decay_style=\"linear\")\n",
    "\n",
    "\n",
    "    ##### Stream from that dataset's dataloader #####\n",
    "    iter_dataloader = iter(train_dataloader)\n",
    "    n_steps = 0\n",
    "    \n",
    "    episode_loss = []\n",
    "    while True:\n",
    "        tic_BATCH = time.time()\n",
    "        \n",
    "        # 1. Get the support data from the first batch \n",
    "        #    and the query data from the second batch\n",
    "        try:\n",
    "            _, _, support_x, _, support_y, support_gen_x, support_gen_y = next(iter_dataloader)\n",
    "            _, _, query_x, _, query_y, query_gen_x, query_gen_y = next(iter_dataloader)\n",
    "            \n",
    "            n_inputs = sum(_cqa.shape[0] for _cqa in support_x)\n",
    "            \n",
    "            # Since we only have 1 GPU, just use the first one, it will separate batches according to the device IDS\n",
    "            support_x = support_x[0]\n",
    "            support_y = support_y[0]\n",
    "            query_x = query_x[0]\n",
    "            query_y = query_y[0]\n",
    "            \n",
    "            support_x = support_x.to(DEVICE)\n",
    "            support_y = support_y.to(DEVICE)\n",
    "            query_x = query_x.to(DEVICE)\n",
    "            query_y = query_y.to(DEVICE)\n",
    "            \n",
    "        except StopIteration:\n",
    "            break\n",
    "        \n",
    "        \n",
    "#         print(support_x.shape)\n",
    "#         print(query_x.shape)\n",
    "        \n",
    "        \n",
    "        ### START Adaptation Phase ###\n",
    "        # 2. Reinitialize net_pi with parameters from net\n",
    "        for m_from, m_to in zip(net.modules(), net_pi.modules()):\n",
    "            m_to.load_state_dict(m_from.state_dict().copy())\n",
    "        \n",
    "        \n",
    "        # 3. Update the weights with the support set\n",
    "        # May update for several steps\n",
    "        for i in range(num_updates):\n",
    "            \n",
    "            qa_logits = net_pi(support_x)\n",
    "            # Somehow it also returns attentions in [1]?, this is selecting 0 of what WrapModel is doing \n",
    "            qa_logits = qa_logits[0]\n",
    "            qa_loss = train_loss_fct(qa_logits.transpose(1,2), support_y)\n",
    "            loss = qa_loss\n",
    "        \n",
    "            # Update Optimizer\n",
    "            optimizer.backward(loss, update_master_grads=False) # instead of loss.backward() for fp16\n",
    "            optimizer.update_master_grads()\n",
    "            optimizer.clip_master_grads(max_grad_norm)\n",
    "            optimizer.step()\n",
    "            if not optimizer.overflow:\n",
    "                for i in range(n_inputs):\n",
    "                    scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        ### END Adaptation Phase ###\n",
    "\n",
    "        ### START Meta-Learning Phase ###\n",
    "        # 4. After Adaptation, use the query set for learning\n",
    "        # Somehow it also returns attentions in [1]?, this is selecting 0 of what WrapModel is doing \n",
    "        qa_logits = net_pi(query_x)[0]\n",
    "        qa_loss = train_loss_fct(qa_logits.transpose(1,2), query_y)\n",
    "        loss = qa_loss\n",
    "        \n",
    "        # Add loss to episode loss\n",
    "        episode_loss.append(loss.item())\n",
    "        \n",
    "        # gradient for validation on theta_pi\n",
    "        # after call autorad.grad, you can not call backward again except for setting create_graph = True\n",
    "        # as we will use the loss as dummpy loss to conduct a dummy backprop to write our gradients to theta network,\n",
    "        # here we set create_graph to true to support second time backward.\n",
    "        grads_pi = torch.autograd.grad(loss, net_pi.parameters(), create_graph=True)\n",
    "        \n",
    "        # As we already have the grads to update\n",
    "        # We use a dummy forward / backward pass to get the correct grads into self.net\n",
    "        # the right grads will be updated by hook, ignoring backward.\n",
    "        # use hook mechnism to write sumed gradient into network.\n",
    "        # we need to update the theta/net network, we need a op from net network, so we call self.learner.net_forward\n",
    "        # to get the op from net network, since the loss from self.learner.forward will return loss from net_pi network.\n",
    "        \n",
    "        \n",
    "        # Somehow it also returns attentions in [1]?, this is selecting 0 of what WrapModel is doing \n",
    "        qa_logits = net(query_x)[0]\n",
    "        dummy_loss = train_loss_fct(qa_logits.transpose(1,2), query_y)\n",
    "        \n",
    "        \n",
    "        # Register a hook on each parameter in the net that replaces the current dummy grad\n",
    "        # with our grads accumulated across the meta-batch\n",
    "        hooks = []\n",
    "        for i, v in enumerate(net.parameters()):\n",
    "            def closure():\n",
    "                ii = i\n",
    "                return lambda grad: grads_pi[ii]\n",
    "            # if you write: hooks.append( v.register_hook(lambda grad : sum_grads_pi[i]) )\n",
    "            # it will pop an ERROR, i don't know why?\n",
    "            hooks.append(v.register_hook(closure()))\n",
    "        \n",
    "        # use our sumed gradients_pi to update the theta/net network,\n",
    "        # since our optimizer receive the self.net.parameters() only.\n",
    "        # Update Meta Optimizer\n",
    "        meta_optimizer.backward(dummy_loss, update_master_grads=False) # instead of loss.backward() for fp16\n",
    "        meta_optimizer.update_master_grads()\n",
    "        meta_optimizer.clip_master_grads(max_grad_norm)\n",
    "        meta_optimizer.step()\n",
    "        # DO I NEED SCHEDULER HERE???\n",
    "        meta_optimizer.zero_grad()\n",
    "\n",
    "        # if you do NOT remove the hook, the GPU memory will expode!!!\n",
    "        for h in hooks:\n",
    "            h.remove()\n",
    "        \n",
    "        \n",
    "        ### END Meta-Learning Phase ###\n",
    "        n_steps += 1\n",
    "        \n",
    "        toc_BATCH = time.time() - tic_BATCH\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "#         mem = float(torch.cuda.memory_allocated() / (1024 * 1024))\n",
    "#         print(\"memory allocated:\", mem, \"MiB\")\n",
    "        if n_steps%10 == 0:\n",
    "            logger.info(f'{RUN_ID} {_tasks[0]} Steps: {n_steps}/{len(train_qadata)//max_train_batch_size} Episode {n_steps}: Loss: {loss:.5f} lr {scheduler.get_lr():.1E} Batch: {n_inputs}')\n",
    "            logger.info(f'[TIME] BATCH {RUN_ID} {_tasks[0]} {toc_BATCH}')\n",
    "            \n",
    "    toc_TASK = time.time() - tic_TASK\n",
    "    MODEL_SAVE_LOC = os.path.join(model_dir, f'{_tasks[0]}.model')\n",
    "    LOSS_SAVE_LOC = os.path.join(model_dir, f'{_tasks[0]}_loss.pickle')\n",
    "    torch.save(net.state_dict(), MODEL_SAVE_LOC)\n",
    "    logger.info(f'{RUN_ID} {_tasks[0]} Done Saving Model at {MODEL_SAVE_LOC}')\n",
    "    logger.info(f'[TIME] TASK {RUN_ID} {_tasks[0]} {toc_TASK}')\n",
    "    pickle.dump( episode_loss, open( LOSS_SAVE_LOC, \"wb\" ), protocol=pickle.HIGHEST_PROTOCOL )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
