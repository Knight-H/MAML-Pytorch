{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, CONFIG_NAME \n",
    "from pytorch_transformers import AdamW\n",
    "from fp16 import FP16_Module, FP16_Optimizer\n",
    "from torch import nn\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "import json\n",
    "from multiprocessing import Pool\n",
    "from parallel import DataParallelModel, DataParallelCriterion\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from scheduler import AnnealingLR\n",
    "import warnings\n",
    "\n",
    "import GPUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILL_VAL = -1\n",
    "\n",
    "# MEMORY_FACTOR = {\n",
    "#     \"finetune\": 0.58,\n",
    "#     \"multitask\": 0.58,\n",
    "#     \"lll\": 0.35,\n",
    "#     \"ewc\": 0.30,\n",
    "#     \"mas\": 0.18,\n",
    "#     \"gem\": 0.50,\n",
    "# }\n",
    "n_gpus = 1\n",
    "\n",
    "#DRIVER MISMATCH??\n",
    "# device_ids = GPUtil.getAvailable(maxLoad=0.1, maxMemory=0.05, limit=n_gpus)\n",
    "# gpus = GPUtil.getGPUs()\n",
    "# gpu_names = [gpus[device_id].name for device_id in device_ids]\n",
    "\n",
    "# memory_sizes = [gpus[device_id].memoryTotal for device_id in device_ids]\n",
    "# memory_sizes[0] = args.memory_sizes[0] * (1 - 0.04 * (n_gpus-1))\n",
    "# for i in range(1, n_gpus):\n",
    "#     memory_sizes[i] = args.memory_sizes[i] * 1.04\n",
    "\n",
    "# train_batch_size = [int(memory_size * MEMORY_FACTOR['lll']) for memory_size in memory_sizes]\n",
    "# test_batch_size = [int(memory_size * MEMORY_FACTOR['lll']) for memory_size in memory_sizes]\n",
    "\n",
    "train_batch_size = 4\n",
    "test_batch_size = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_gpus = 1\n",
    "# device_ids = [0]\n",
    "# gpus = GPUtil.getGPUs()\n",
    "# gpu_names = [gpus[device_id].name for device_id in device_ids]\n",
    "# memory_sizes = [gpus[device_id].memoryTotal for device_id in device_ids]\n",
    "# memory_sizes[0] = memory_sizes[0] * (1 - 0.04 * (n_gpus-1))\n",
    "# for i in range(1, n_gpus):\n",
    "#     memory_sizes[i] = args.memory_sizes[i] * 1.04\n",
    "    \n",
    "# train_batch_size = [int(memory_size * MEMORY_FACTOR['lll']) for memory_size in memory_sizes]\n",
    "# test_batch_size = [int(memory_size * MEMORY_FACTOR['lll']) for memory_size in memory_sizes]\n",
    "# print(memory_sizes)\n",
    "# print(train_batch_size)\n",
    "# print(test_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CLASSES = {\n",
    "    'gpt2': (GPT2LMHeadModel, GPT2Tokenizer, GPT2Config),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In settings.py\n",
    "special_tokens = {\"ans_token\":'__ans__', \"pad_token\":'__pad__', \"unk_token\":'__unk__', \"eos_token\": '<|endoftext|>'}\n",
    "\n",
    "model_class, tokenizer_class, config_class = MODEL_CLASSES['gpt2']\n",
    "tokenizer = tokenizer_class.from_pretrained('gpt2')\n",
    "tokenizer.add_tokens(list(special_tokens.values()))\n",
    "special_token_ids = {k:tokenizer.convert_tokens_to_ids(v) for k,v in special_tokens.items()}\n",
    "\n",
    "\n",
    "model_config = config_class.from_pretrained('gpt2')\n",
    "model_config.vocab_size = len(tokenizer)\n",
    "\n",
    "tokens_weight = torch.ones([model_config.vocab_size], dtype=torch.float).cuda()\n",
    "tokens_weight[special_token_ids[\"ans_token\"]] = 5\n",
    "\n",
    "\n",
    "MODEL_CLASS = model_class\n",
    "TOKENIZER = tokenizer\n",
    "SPECIAL_TOKENS = special_tokens\n",
    "SPECIAL_TOKEN_IDS = special_token_ids\n",
    "TOKENS_WEIGHT = tokens_weight\n",
    "\n",
    "model_config = config_class.from_pretrained('gpt2')\n",
    "max_len = model_config.n_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FP16_Module(\n",
       "  (module): GPT2LMHeadModel(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(50260, 768)\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0): Block(\n",
       "          (ln_1): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Block(\n",
       "          (ln_1): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): Block(\n",
       "          (ln_1): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): Block(\n",
       "          (ln_1): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): Block(\n",
       "          (ln_1): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): Block(\n",
       "          (ln_1): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): Block(\n",
       "          (ln_1): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): Block(\n",
       "          (ln_1): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): Block(\n",
       "          (ln_1): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): Block(\n",
       "          (ln_1): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): Block(\n",
       "          (ln_1): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): Block(\n",
       "          (ln_1): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In Train.py\n",
    "model = MODEL_CLASS.from_pretrained('gpt2').cuda()\n",
    "model.resize_token_embeddings(len(TOKENIZER))\n",
    "model = FP16_Module(model)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrapModel(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(WrapModel, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        outputs = self.model(input_ids)\n",
    "        return outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WrapModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_model = DataParallelModel(WrapModel(model), [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaLearner(nn.Module):\n",
    "    \"\"\"\n",
    "    As we have mentioned in Learner class, the metalearner class will receive a series of loss on different tasks/episodes\n",
    "    on theta_pi network, and it will merage all loss and then sum over it. The summed loss will be backproped on theta\n",
    "    network to update theta parameters, which is the initialization point we want to find.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, net_cls, net_cls_args, n_way, k_shot, meta_batchsz, beta, num_updates):\n",
    "        \"\"\"\n",
    "        :param net_cls: class, not instance. the class of specific Network for learner\n",
    "        :param net_cls_args: tuple, args for net_cls, like (n_way, imgsz)\n",
    "        :param n_way:\n",
    "        :param k_shot:\n",
    "        :param meta_batchsz: number of tasks/episode\n",
    "        :param beta: learning rate for meta-learner\n",
    "        :param num_updates: number of updates for learner\n",
    "        \"\"\"\n",
    "        super(MetaLearner, self).__init__()\n",
    "\n",
    "        self.n_way = n_way\n",
    "        self.k_shot = k_shot\n",
    "        self.meta_batchsz = meta_batchsz\n",
    "        self.beta = beta\n",
    "        # self.alpha = alpha # set alpha in Learner.optimizer directly.\n",
    "        self.num_updates = num_updates\n",
    "\n",
    "        # it will contains a learner class to learn on episodes and gather the loss together.\n",
    "        self.learner = Learner(net_cls, *net_cls_args)\n",
    "        # the optimizer is to update theta parameters, not theta_pi parameters.\n",
    "        self.optimizer = optim.Adam(self.learner.parameters(), lr=beta)\n",
    "        \n",
    "    def write_grads(self, dummy_loss, sum_grads_pi):\n",
    "        \"\"\"\n",
    "        write loss into learner.net, gradients come from sum_grads_pi.\n",
    "        Since the gradients info is not calculated by general backward, we need this function to write the right gradients\n",
    "        into theta network and update theta parameters as wished.\n",
    "        :param dummy_loss: dummy loss, nothing but to write our gradients by hook\n",
    "        :param sum_grads_pi: the summed gradients\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # Register a hook on each parameter in the net that replaces the current dummy grad\n",
    "        # with our grads accumulated across the meta-batch\n",
    "        hooks = []\n",
    "\n",
    "        for i, v in enumerate(self.learner.parameters()):\n",
    "            def closure():\n",
    "                ii = i\n",
    "                return lambda grad: sum_grads_pi[ii]\n",
    "\n",
    "            # if you write: hooks.append( v.register_hook(lambda grad : sum_grads_pi[i]) )\n",
    "            # it will pop an ERROR, i don't know why?\n",
    "            hooks.append(v.register_hook(closure()))\n",
    "\n",
    "        # use our sumed gradients_pi to update the theta/net network,\n",
    "        # since our optimizer receive the self.net.parameters() only.\n",
    "        self.optimizer.zero_grad()\n",
    "        dummy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # if you do NOT remove the hook, the GPU memory will expode!!!\n",
    "        for h in hooks:\n",
    "            h.remove()\n",
    "            \n",
    "    def forward(self, support_x, support_y, query_x, query_y):\n",
    "        \"\"\"\n",
    "        Here we receive a series of episode, each episode will be learned by learner and get a loss on parameters theta.\n",
    "        we gather the loss and sum all the loss and then update theta network.\n",
    "        setsz = n_way * k_shotf\n",
    "        querysz = n_way * k_shot\n",
    "        :param support_x: [meta_batchsz, setsz, c_, h, w]\n",
    "        :param support_y: [meta_batchsz, setsz]\n",
    "        :param query_x:   [meta_batchsz, querysz, c_, h, w]\n",
    "        :param query_y:   [meta_batchsz, querysz]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        sum_grads_pi = None\n",
    "        meta_batchsz = support_y.size(0)\n",
    "\n",
    "        # support_x[i]: [setsz, c_, h, w]\n",
    "        # we do different learning task sequentially, not parallel.\n",
    "        accs = []\n",
    "        # for each task/episode.\n",
    "        for i in range(meta_batchsz):\n",
    "            _, grad_pi, episode_acc = self.learner(support_x[i], support_y[i], query_x[i], query_y[i], self.num_updates)\n",
    "            accs.append(episode_acc)\n",
    "            if sum_grads_pi is None:\n",
    "                sum_grads_pi = grad_pi\n",
    "            else:  # accumulate all gradients from different episode learner\n",
    "                sum_grads_pi = [torch.add(i, j) for i, j in zip(sum_grads_pi, grad_pi)]\n",
    "\n",
    "        # As we already have the grads to update\n",
    "        # We use a dummy forward / backward pass to get the correct grads into self.net\n",
    "        # the right grads will be updated by hook, ignoring backward.\n",
    "        # use hook mechnism to write sumed gradient into network.\n",
    "        # we need to update the theta/net network, we need a op from net network, so we call self.learner.net_forward\n",
    "        # to get the op from net network, since the loss from self.learner.forward will return loss from net_pi network.\n",
    "        dummy_loss, _ = self.learner.net_forward(support_x[0], support_y[0])\n",
    "        self.write_grads(dummy_loss, sum_grads_pi)\n",
    "\n",
    "        return accs\n",
    "    \n",
    "    def pred(self, support_x, support_y, query_x, query_y):\n",
    "        \"\"\"\n",
    "        predict for query_x\n",
    "        :param support_x:\n",
    "        :param support_y:\n",
    "        :param query_x:\n",
    "        :param query_y:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        meta_batchsz = support_y.size(0)\n",
    "\n",
    "        accs = []\n",
    "        # for each task/episode.\n",
    "        # the learner will copy parameters from current theta network and then fine-tune on support set.\n",
    "        for i in range(meta_batchsz):\n",
    "            _, _, episode_acc = self.learner(support_x[i], support_y[i], query_x[i], query_y[i], self.num_updates)\n",
    "            accs.append(episode_acc)\n",
    "\n",
    "        return np.array(accs).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = MetaLearner(Naive, (n_way, imgsz), n_way=n_way, k_shot=k_shot, meta_batchsz=meta_batchsz, beta=meta_lr,\n",
    "                   num_updates=num_updates).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = Learner(net_cls, *net_cls_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'fp16.FP16_Module'>\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-95d648291d45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# pls make sure net_cls is a class but NOT an instance of class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# pls make sure net_cls is a class but NOT an instance of class.\n",
    "print(model.__class__)\n",
    "assert model.__class__ == type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner(nn.Module):\n",
    "    \"\"\"\n",
    "    Learner class will be responsible for ADAPT on meta-train set (Support) and return loss on meta-test set (Query)\n",
    "    (1) Create 2 same network, theta and theta_pi.\n",
    "    (2) For each episode, theta_pi copy initial parameters from theta and ADAPT several steps by meta-train set (Support)\n",
    "    (3) and then calculate its loss on meta-test set (Query). \n",
    "    (4) All loss on meta-test will be summed and backprop on theta network (done on Metalearner class)\n",
    "    \"\"\"\n",
    "    def __init__(self, net_cls, token_size):\n",
    "        \"\"\"\n",
    "        It will receive a class: net_cls and its parameters: args for net_cls.\n",
    "        :param net_cls: class, not instance\n",
    "        :param args: the parameters for net_cls\n",
    "        \"\"\"\n",
    "        super(Learner, self).__init__()\n",
    "        # pls make sure net_cls is a class but NOT an instance of class.\n",
    "        assert net_cls.__class__ == type\n",
    "        \n",
    "        # we will create two class instance meanwhile and use one as theta network and the other as theta_pi network.\n",
    "        self.net = net_cls.from_pretrained('gpt2').cuda()\n",
    "        self.net.resize_token_embeddings(token_size)\n",
    "        self.net = FP16_Module(model)\n",
    "        \n",
    "        self.net_pi = net_cls.from_pretrained('gpt2').cuda()\n",
    "        self.net_pi.resize_token_embeddings(token_size)\n",
    "        self.net_pi = FP16_Module(model)\n",
    "        \n",
    "        \n",
    "        # From train.py\n",
    "        param_optimizer = list(self.net_pi.named_parameters())\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "        \n",
    "        # ADAPT theta_pi = theta_pi - lr * grad\n",
    "        self.optimizer = AdamW(optimizer_grouped_parameters, lr=6.25e-5, eps=1e-4)\n",
    "        \n",
    "        \n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Override this function to return only net parameters for MetaLearner's optimize\n",
    "        it will ignore theta_pi network parameters.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.net.parameters()\n",
    "    def update_pi(self):\n",
    "        \"\"\"\n",
    "        copy parameters from self.net -> self.net_pi\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        for m_from, m_to in zip(self.net.modules(), self.net_pi.modules()):\n",
    "#             if isinstance(m_to, nn.Linear) or isinstance(m_to, nn.Conv2d) or isinstance(m_to, nn.BatchNorm2d):\n",
    "#                 m_to.weight.data = m_from.weight.data.clone()\n",
    "#                 if m_to.bias is not None:\n",
    "#                     m_to.bias.data = m_from.bias.data.clone()\n",
    "\n",
    "            # NOT SURE IF THIS IS CORRECT??? USING STATEDICT loading/ via NAMED PARAMETERS??\n",
    "            m_to.load_state_dict(m_from.state_dict().copy())\n",
    "    def forward(self, support_x, support_y, query_x, query_y, num_updates):\n",
    "        \"\"\"\n",
    "        learn on current episode meta-train: support_x & support_y and then calculate loss on meta-test set: query_x&y\n",
    "        :param support_x: [setsz, c_, h, w]\n",
    "        :param support_y: [setsz]\n",
    "        :param query_x:   [querysz, c_, h, w]\n",
    "        :param query_y:   [querysz]\n",
    "        :param num_updates: 5\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        \n",
    "        # now try to fine-tune from current $theta$ parameters -> $theta_pi$\n",
    "        # after num_updates of fine-tune, we will get a good theta_pi parameters so that it will retain satisfying\n",
    "        # performance on specific task, that's, current episode.\n",
    "        # firstly, copy theta_pi from theta network\n",
    "        self.update_pi()\n",
    "        \n",
    "        # update for several steps\n",
    "        for i in range(num_updates):\n",
    "            # forward and backward to update net_pi grad.\n",
    "            loss, pred = self.net_pi(support_x, support_y)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        # Compute the meta gradient and return it, the gradient is from one episode\n",
    "        # in metalearner, it will merge all loss from different episode and sum over it.\n",
    "        loss, pred = self.net_pi(query_x, query_y)\n",
    "        # pred: [setsz, n_way], indices: [setsz]\n",
    "        _, indices = torch.max(pred, dim=1)\n",
    "#         correct = torch.eq(indices, query_y).sum().data[0]\n",
    "        correct = torch.eq(indices, query_y).sum().item()\n",
    "        acc = correct / query_y.size(0)\n",
    "        \n",
    "        # gradient for validation on theta_pi\n",
    "        # after call autorad.grad, you can not call backward again except for setting create_graph = True\n",
    "        # as we will use the loss as dummpy loss to conduct a dummy backprop to write our gradients to theta network,\n",
    "        # here we set create_graph to true to support second time backward.\n",
    "        grads_pi = autograd.grad(loss, self.net_pi.parameters(), create_graph=True)\n",
    "\n",
    "        return loss, grads_pi, acc\n",
    "    \n",
    "    def net_forward(self, support_x, support_y):\n",
    "        \"\"\"\n",
    "        This function is purely for updating net network. In metalearner, we need the get the loss op from net network\n",
    "        to write our merged gradients into net network, hence will call this function to get a dummy loss op.\n",
    "        :param support_x: [setsz, c, h, w]\n",
    "        :param support_y: [sessz, c, h, w]\n",
    "        :return: dummy loss and dummy pred\n",
    "        \"\"\"\n",
    "        loss, pred = self.net(support_x, support_y)\n",
    "        return loss, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_losses(parallel_model, cqa, Y, gen_X, gen_Y, loss_fct):\n",
    "    qa_logits = parallel_model(cqa)\n",
    "    lm_logits = parallel_model(gen_X)\n",
    "    qa_loss = loss_fct([torch.transpose(l, 1, 2) for l in qa_logits], Y)\n",
    "    lm_loss = loss_fct([torch.transpose(l, 1, 2) for l in lm_logits], gen_Y)\n",
    "    return torch.mean(qa_loss), args.lm_lambda * torch.mean(lm_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = Learner(MODEL_CLASS, token_size =len(TOKENIZER) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/root/LAMOL/lamol_data\"\n",
    "n_train_epochs  = 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_DICT = {\n",
    "    \"movie\": {\n",
    "               \"train\":os.path.join(data_dir,\"movie_train.json\"),\n",
    "               \"eval\":os.path.join(data_dir,\"movie_dev.json\"),\n",
    "               \"test\":os.path.join(data_dir,\"movie_test.json\"),\n",
    "               \"n_train_epochs\": n_train_epochs \n",
    "    },\n",
    "    \"boolq\": {\n",
    "               \"train\":os.path.join(data_dir,\"boolq_train.json\"),\n",
    "               \"eval\":os.path.join(data_dir,\"boolq_dev.json\"),\n",
    "               \"test\":os.path.join(data_dir,\"boolq_test.json\"),\n",
    "               \"n_train_epochs\": n_train_epochs \n",
    "    },\n",
    "    \"scifact\": {\n",
    "               \"train\":os.path.join(data_dir,\"scifact_train.json\"),\n",
    "               \"eval\":os.path.join(data_dir,\"scifact_dev.json\"),\n",
    "               \"test\":os.path.join(data_dir,\"scifact_test.json\"),\n",
    "               \"n_train_epochs\": n_train_epochs \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, data_paths, data_type, gen_token, extra_data=[]):\n",
    "        self.data_type = data_type\n",
    "        self.gen_token = gen_token\n",
    "        self.ans_token = SPECIAL_TOKEN_IDS[\"ans_token\"]\n",
    "        self.eos_token = SPECIAL_TOKEN_IDS[\"eos_token\"]\n",
    "        self.pad_token = SPECIAL_TOKEN_IDS[\"pad_token\"]\n",
    "\n",
    "        if not isinstance(data_paths, list):\n",
    "            data_paths = [data_paths]\n",
    "\n",
    "        data = []\n",
    "        for data_path in data_paths:\n",
    "            if not data_path:\n",
    "                continue\n",
    "            with open(data_path, \"r\") as f:\n",
    "                raw_ds = json.load(f)\n",
    "            raw_ds = map(lambda x: x[\"paragraphs\"], raw_ds[\"data\"])\n",
    "            d = []\n",
    "            for raw_d in raw_ds:\n",
    "                d.extend(raw_d)\n",
    "            data += d\n",
    "        \n",
    "        self.data = []\n",
    "        self.max_a_len = 0\n",
    "        if len(data) > 0:\n",
    "            self.data_tokenization(data)\n",
    "\n",
    "        if len(extra_data) > 0:\n",
    "            extra_data = map(lambda x: self.etl_single_extra_data(x), extra_data)\n",
    "            extra_data = list(filter(lambda x:x, extra_data))\n",
    "            if args.gen_lm_sample_percentage > 0. and len(extra_data) == 0:\n",
    "                logger.warning(\"No good extra data but sample percentage > 0!\")\n",
    "            self.data += extra_data\n",
    "\n",
    "\n",
    "    def etl_single_extra_data(self, data):\n",
    "        gen_token = data[0]\n",
    "        data = ' '.join([str(datum) for datum in data[1:]])\n",
    "        try:\n",
    "            context = \"\"\n",
    "            qa = data\n",
    "            question, answer = re.split(str(SPECIAL_TOKEN_IDS[\"ans_token\"]), qa)\n",
    "            context = [int(c) for c in context.strip().split()]\n",
    "            question = [int(q) for q in question.strip().split()]\n",
    "            answer = [int(a) for a in re.sub(str(SPECIAL_TOKEN_IDS[\"eos_token\"]), \"\", answer).strip().split()]\n",
    "            uid = uuid.uuid1().hex\n",
    "            data = self.parse_example(gen_token, context, question, answer, uid)\n",
    "        except ValueError:\n",
    "            return\n",
    "        return data\n",
    "\n",
    "    def concat_example(self, gen_token, c, sep_token, q, ans_token, a, eos_token):\n",
    "        example = sep_token + q + ans_token + a\n",
    "        if len(example) + 1 > max_len:\n",
    "            logger.warning('an example with len {} is too long!'.format(len(example) + 1))\n",
    "            return\n",
    "        example = gen_token + c[:max_len-len(example)-1] + example + eos_token\n",
    "        return example\n",
    "\n",
    "    def parse_example(self, gen_token, context, question, answer, idx):\n",
    "        cq_example = self.concat_example([], context, [], question, [self.ans_token], [], [])\n",
    "        cqa_example = self.concat_example([], context, [], question, [self.ans_token], answer, [])\n",
    "        Y_example = self.concat_example([], [], [], [], [], answer, [self.eos_token])\n",
    "        Y_example = [FILL_VAL] * (len(cqa_example) - len(Y_example)) + Y_example\n",
    "        gen_X_example = self.concat_example([gen_token], context, [], question, [self.ans_token], answer, [])\n",
    "        gen_Y_example = self.concat_example([], context, [], question, [self.ans_token], answer, [self.eos_token])\n",
    "        return cq_example, len(cq_example), cqa_example, len(cqa_example), Y_example, gen_X_example, gen_Y_example, idx\n",
    "\n",
    "    def parallel_tokenization(self, d):\n",
    "        examples = []\n",
    "        context = TOKENIZER.encode(d[\"context\"])\n",
    "        max_a_len = 0\n",
    "        for qa in d[\"qas\"]:\n",
    "            question = TOKENIZER.encode(qa[\"question\"])\n",
    "\n",
    "            raw_answers = qa[\"answers\"]\n",
    "            if len(raw_answers) == 0:\n",
    "                assert qa[\"is_impossible\"]\n",
    "                raw_answers.append({\"text\": \"\"})\n",
    "\n",
    "            answer = []\n",
    "            for i, raw_answer in enumerate(raw_answers):\n",
    "                answer.extend(TOKENIZER.encode(raw_answer[\"text\"]))\n",
    "                if i != len(raw_answers) - 1:\n",
    "                    answer.append(self.pad_token)\n",
    "            max_a_len = max(max_a_len, len(answer))\n",
    "\n",
    "            examples.append(self.parse_example(self.gen_token, context, question, answer, qa.get(\"id\", 0)))\n",
    "        return examples, max_a_len\n",
    "\n",
    "    def data_tokenization(self, data):\n",
    "        with Pool(4) as pool:\n",
    "            data = pool.map(self.parallel_tokenization, data)\n",
    "        for datum, max_a_len in data:\n",
    "            self.data.extend(datum)\n",
    "            self.max_a_len = max(self.max_a_len, max_a_len)\n",
    "\n",
    "    def sort(self):\n",
    "        self.data.sort(key=lambda x: len(x[0]))\n",
    "        return self\n",
    "\n",
    "    def sort_by_index(self):\n",
    "        self.data.sort(key=lambda x: x[-1])\n",
    "\n",
    "    def get_indices(self):\n",
    "        return [d[-1] for d in self.data]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicBatchSampler(Sampler):\n",
    "    def __init__(self, dataset, data_type, max_batch_size):\n",
    "        self.dataset = dataset\n",
    "        self.data_type = data_type\n",
    "        if data_type == \"train\":\n",
    "            self.batch_size = train_batch_size\n",
    "        else:\n",
    "            self.batch_size = test_batch_size\n",
    "        self.n_samples = len(dataset)\n",
    "        self.max_batch_size = max_batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.data_type == \"test\":\n",
    "            indices = range(self.n_samples)\n",
    "        else:\n",
    "            indices = np.random.permutation(self.n_samples)\n",
    "        max_len, cnt, st = 0, 0, 0\n",
    "        batch = []\n",
    "        for ed, idx in enumerate(indices):\n",
    "            ln = len(self.dataset[idx][2])\n",
    "            if max(max_len, ln)**LEN_FACTOR * (ed - st + 1) > self.batch_size[cnt]:\n",
    "                st = ed\n",
    "                cnt += 1\n",
    "                max_len = 0\n",
    "                if cnt == args.n_gpus:\n",
    "                    yield batch\n",
    "                    cnt = 0\n",
    "                    batch = []\n",
    "            max_len = max(max_len, ln)\n",
    "            batch.append(idx)\n",
    "            if len(batch) == self.max_batch_size and self.data_type == \"train\":\n",
    "                yield batch\n",
    "                cnt, max_len, st = 0, 0, ed\n",
    "                batch = []\n",
    "        if len(batch) > 0:\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dynamic_collate_fn(data, batch_size):\n",
    "\n",
    "    def local_collate():\n",
    "        null_counter = 0\n",
    "        _cqs, _len_cqs, _cqas, _len_cqas, _Ys, _gen_Xs, _gen_Ys = [], [], [], [], [], [], []\n",
    "        Y_max_len = max(len(data[j][4]) for j in range(st, ed))\n",
    "        cq_max_len = max(len(data[j][0]) for j in range(st, ed))\n",
    "        for j in range(st, ed):\n",
    "            if None in data[j] or [] in data[j]:\n",
    "                null_counter+=1\n",
    "                logger.warning('null example in collate_fn, count: {}'.format(null_counter))\n",
    "                continue\n",
    "\n",
    "            pad_len = cqa_max_len - len(data[j][2])\n",
    "\n",
    "            _cqs.append(pad_to_max_len(data[j][0], cq_max_len-len(data[j][0]), SPECIAL_TOKEN_IDS[\"pad_token\"]))\n",
    "            _len_cqs.append(data[j][1])\n",
    "            _cqas.append(pad_to_max_len(data[j][2], pad_len, SPECIAL_TOKEN_IDS[\"pad_token\"]))\n",
    "            _len_cqas.append(data[j][3])\n",
    "            _Ys.append(pad_to_max_len(data[j][4], Y_max_len - len(data[j][4]), FILL_VAL))\n",
    "            _gen_Xs.append(pad_to_max_len(data[j][5], pad_len, SPECIAL_TOKEN_IDS[\"pad_token\"]))\n",
    "            _gen_Ys.append(pad_to_max_len(data[j][6], pad_len, FILL_VAL))\n",
    "\n",
    "        cqs.append(torch.tensor(_cqs))\n",
    "        len_cqs.append(torch.tensor(_len_cqs))\n",
    "        cqas.append(torch.tensor(_cqas))\n",
    "        len_cqas.append(torch.tensor(_len_cqas))\n",
    "        Ys.append(torch.tensor(_Ys))\n",
    "        gen_Xs.append(torch.tensor(_gen_Xs))\n",
    "        gen_Ys.append(torch.tensor(_gen_Ys))\n",
    "\n",
    "    cqs, len_cqs, cqas, len_cqas, Ys, gen_Xs, gen_Ys = [], [], [], [], [], [], []\n",
    "    cqa_max_len, cnt, st = 0, 0, 0\n",
    "    for ed, datum in enumerate(data):\n",
    "        ln = len(datum[2]) # use cqas to calibrate\n",
    "        if max(cqa_max_len, ln)**LEN_FACTOR * (ed - st + 1) > batch_size[cnt]:\n",
    "            local_collate()\n",
    "            cnt += 1\n",
    "            cqa_max_len = 0\n",
    "            st = ed\n",
    "        cqa_max_len = max(cqa_max_len, ln)\n",
    "    ed += 1  # otherwise ed will be len(data)-1\n",
    "    local_collate()\n",
    "\n",
    "    return cqs, len_cqs, cqas, len_cqas, Ys, gen_Xs, gen_Ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def varlen_collate_fn(data):\n",
    "    batch_size = (len(data) + n_gpus - 1) // n_gpus\n",
    "    cqs = torch.tensor(pad_all_to_max_len([datum[0] for datum in data], SPECIAL_TOKEN_IDS[\"pad_token\"])).split(batch_size)\n",
    "    len_cqs = torch.tensor([datum[1] for datum in data]).split(batch_size)\n",
    "    cqas = torch.tensor(pad_all_to_max_len([datum[2] for datum in data], SPECIAL_TOKEN_IDS[\"pad_token\"])).split(batch_size)\n",
    "    len_cqas = torch.tensor([datum[3] for datum in data]).split(batch_size)\n",
    "    Ys = torch.tensor(pad_all_to_max_len([datum[4] for datum in data], FILL_VAL)).split(batch_size)\n",
    "    gen_Xs = torch.tensor(pad_all_to_max_len([datum[5] for datum in data], SPECIAL_TOKEN_IDS[\"pad_token\"])).split(batch_size)\n",
    "    gen_Ys = torch.tensor(pad_all_to_max_len([datum[6] for datum in data], FILL_VAL)).split(batch_size)\n",
    "    return list(cqs), list(len_cqs), list(cqas), list(len_cqas), list(Ys), list(gen_Xs), list(gen_Ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_max_len(l, pad_len, val):\n",
    "    return l + [val] * pad_len\n",
    "def pad_all_to_max_len(ls, val):\n",
    "    max_len = max(len(l) for l in ls)\n",
    "    return [pad_to_max_len(l, max_len-len(l), val) for l in ls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_dataloader(dataset, data_type, max_batch_size=1000000000):\n",
    "    if data_type == \"train\":\n",
    "        batch_size = train_batch_size\n",
    "    else:\n",
    "        batch_size = test_batch_size\n",
    "\n",
    "    if isinstance(batch_size, list):\n",
    "        collate_fn=lambda x,bs=batch_size: dynamic_collate_fn(x, bs)\n",
    "        shuffle = False\n",
    "        batch_size = 1\n",
    "        batch_sampler = DynamicBatchSampler(dataset, data_type, max_batch_size)\n",
    "    else:\n",
    "        collate_fn=lambda x: varlen_collate_fn(x)\n",
    "#         shuffle = not (data_type != \"train\" or args.debug)\n",
    "        shuffle = False\n",
    "        batch_sampler = None\n",
    "\n",
    "    dataloader =  DataLoader(dataset, num_workers=4,\n",
    "                             collate_fn=collate_fn,\n",
    "                             shuffle=shuffle,\n",
    "                             batch_size=batch_size,\n",
    "                             batch_sampler=batch_sampler)\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gen_token(task):\n",
    "    return '__' + task + '__'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_n_steps = 1500\n",
    "min_batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1185 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1808 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2222 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1044 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1099 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1141 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1073 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1113 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2420 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1091 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1216 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1268 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1165 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1085 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1400 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1373 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1735 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1127 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1168 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1642 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1182 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1523 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1100 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1244 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1638 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1031 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1480 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1037 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2057 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1749 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1219 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1381 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1181 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1100 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1027 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1448 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1091 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1659 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1309 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1509 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1183 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1050 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1218 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1043 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1213 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1329 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1324 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1107 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1198 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1077 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1164 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1263 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1672 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1275 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1181 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1150 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1055 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1614 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1049 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1445 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1105 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1057 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1101 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1051 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1065 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1218 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1397 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1100 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1262 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1072 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1281 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1289 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1324 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1187 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1608 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1528 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1556 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1248 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1156 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1263 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1111 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1075 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1025 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1246 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1379 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1513 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1301 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1296 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1462 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1094 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1945 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1356 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1120 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1201 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1048 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1995 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1670 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2309 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1139 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1153 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1064 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1057 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1162 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1095 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1040 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1114 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1331 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1071 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1287 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1350 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1179 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1241 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1132 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1102 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1158 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1183 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1033 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1332 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2094 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1067 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1175 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1367 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1184 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1113 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1604 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1044 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1044 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1155 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1158 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1061 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1096 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1301 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1297 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1145 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1446 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1210 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1072 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1065 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1467 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1289 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1043 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1136 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1059 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1294 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1246 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1050 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1432 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1049 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1483 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1714 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1065 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1472 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1039 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1600 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1122 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1116 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2011 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1126 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1243 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1324 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1134 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1038 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1136 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1259 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1411 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1068 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1160 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1388 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1236 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1533 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1157 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1240 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1034 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1049 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1597 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1089 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1131 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1144 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1591 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1450 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1685 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1029 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1167 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1236 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1298 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1381 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1062 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1180 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1798 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1312 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1750 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1360 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1757 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1419 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1924 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1557 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1047 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1137 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1418 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1371 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1202 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1259 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1906 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1180 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1789 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1105 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1045 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1436 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1302 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1209 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1229 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1327 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1135 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2236 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1059 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1155 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1187 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1029 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1094 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1135 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1199 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1280 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1141 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1333 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1428 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1248 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1489 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1088 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2182 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2260 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2079 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1153 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1040 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1732 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1096 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1069 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1183 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1900 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1054 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1077 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1278 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1410 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1190 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1049 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1048 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1833 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1121 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1125 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1209 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1079 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1111 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1227 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1593 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1441 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1244 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1517 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1199 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1222 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1415 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1478 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1076 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1697 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1181 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2166 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1598 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1118 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1276 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1312 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1406 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1174 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1840 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1351 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1845 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1487 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1596 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2098 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2214 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1662 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1657 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1383 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1034 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1906 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1451 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1463 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1501 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1290 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2318 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1106 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1515 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1489 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1154 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1855 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1748 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1599 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1171 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1598 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1361 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1314 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1563 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1026 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1188 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1410 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1453 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1944 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1805 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1255 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1115 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1088 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1511 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1367 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1168 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1061 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1446 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1067 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1508 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1463 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1418 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1280 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1112 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1047 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1079 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1187 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1547 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1228 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1066 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2481 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1148 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1341 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1332 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1268 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1683 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1196 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1108 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1054 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1112 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3233 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2022 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1180 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1676 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1171 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1340 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1277 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1398 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1465 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1235 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1364 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1097 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1048 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2137 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1183 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1640 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1132 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1159 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1639 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1463 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2917 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1185 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1187 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1119 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1067 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1187 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1564 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1072 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1668 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1563 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1180 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1333 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1138 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1694 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1111 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1070 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1084 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1308 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1867 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1082 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1421 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1275 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1185 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1435 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1325 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1210 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1296 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1033 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1739 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1361 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2827 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1030 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1215 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1096 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1916 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1067 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1547 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# First Epoch\n",
    "tasks = ['movie']\n",
    "\n",
    "\n",
    "gen_token = get_gen_token(tasks[0])\n",
    "TOKENIZER.add_tokens([gen_token])\n",
    "# TOKENIZER.save_pretrained(model_dir)\n",
    "SPECIAL_TOKENS[tasks[0]] = gen_token\n",
    "SPECIAL_TOKEN_IDS[tasks[0]] = TOKENIZER.convert_tokens_to_ids(gen_token)\n",
    "# logger.info('gen token = {} , gen token id = {}'.format(gen_token, SPECIAL_TOKEN_IDS[tasks[0]]))\n",
    "# MODEL_CONFIG.vocab_size = len(TOKENIZER)\n",
    "# MODEL_CONFIG.to_json_file(os.path.join(model_dir,CONFIG_NAME))\n",
    "\n",
    "\n",
    "train_extra_data = []\n",
    "train_dataset = [TASK_DICT[t][\"train\"] for t in tasks]\n",
    "train_qadata = QADataset(train_dataset, \"train\", SPECIAL_TOKEN_IDS[tasks[0]], train_extra_data)\n",
    "max_train_batch_size = max(len(train_qadata) // min_n_steps, min_batch_size)\n",
    "train_dataloader = create_dataloader(train_qadata, \"train\", max_train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_qadata.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7110, 25, 734, 6036, 11886, 467, 284, 257, 4928, 2151]\n",
      "901\n",
      "901\n",
      "[7110, 25, 734, 6036, 11886, 467, 284, 257, 4928, 2151]\n",
      "903\n",
      "[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "[50260, 7110, 25, 734, 6036, 11886, 467, 284, 257, 4928]\n",
      "[7110, 25, 734, 6036, 11886, 467, 284, 257, 4928, 2151]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# return cq_example, len(cq_example), cqa_example, len(cqa_example), Y_example, gen_X_example, gen_Y_example, idx\n",
    "#           0                1               2           3                   4       5              6          7\n",
    "print(train_qadata.__getitem__(0)[0][:10])\n",
    "print(len(train_qadata.__getitem__(0)[0])) # Checking \n",
    "print(train_qadata.__getitem__(0)[1])\n",
    "print(train_qadata.__getitem__(0)[2][:10])\n",
    "print(train_qadata.__getitem__(0)[3])\n",
    "print(train_qadata.__getitem__(0)[4][:10])\n",
    "print(train_qadata.__getitem__(0)[5][:10])\n",
    "print(train_qadata.__getitem__(0)[6][:10])\n",
    "print(train_qadata.__getitem__(0)[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAMOL Dataset\n",
    "\n",
    "`train_qadata` is a pytorch Dataset.  \n",
    "A single datapoint of a returns a list of length 8.  \n",
    "```python\n",
    "return cq_example, len(cq_example), cqa_example, len(cqa_example), Y_example, gen_X_example, gen_Y_example, idx\n",
    "           0                1               2           3                   4       5              6          7\n",
    "# 0 cq_example is context+question+__ans__. ie. [7110, 25, 734, 6036, 11886, 467, 284, 257, 4928, 2151]\n",
    "# 1 len(cq_example) is the length ie. 901\n",
    "# 2 cqa_example is context+question+__ans__+answer ie. [7110, 25, 734, 6036, 11886, 467, 284, 257, 4928, 2151]\n",
    "# 3 len(cqa_example) is the length ie. 903\n",
    "# 4 Y_example is FILL_VALUE+answer only. ie. [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
    "# 5 gen_X_example is __gen__+context+question+__ans__+answer ie. [50260, 7110, 25, 734, 6036, 11886, 467, 284, 257, 4928]\n",
    "# 6 gen_Y_example is context+question+__ans__+answer ie. [7110, 25, 734, 6036, 11886, 467, 284, 257, 4928, 2151]\n",
    "# 7 idx is id (supposed to be uuid? but i don't see it) ie. 0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_fct = CrossEntropyLoss(ignore_index=FILL_VAL, weight=TOKENS_WEIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_grad_norm=1\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=6.25e-5, eps=1e-4)\n",
    "optimizer = FP16_Optimizer(optimizer, static_loss_scale=None, dynamic_loss_scale=True,\n",
    "                                   dynamic_loss_args={'scale_window': 100, 'min_scale': 1, 'delayed_shift': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_epochs = 3\n",
    "n_train_optimization_steps = len(train_qadata) * n_train_epochs\n",
    "scheduler = AnnealingLR(optimizer, start_lr=6.25e-5, warmup_iter=int(0.005*len(train_qadata)),\n",
    "            num_iters=int(n_train_optimization_steps), decay_style=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[tensor([[ 7110,    25,   734,  ..., 50257,   399,  7156],\n",
      "        [  262,  3772, 31030,  ..., 50258, 50258, 50258],\n",
      "        [  340,   318,  6918,  ..., 50258, 50258, 50258],\n",
      "        [  366,  1235,   329,  ..., 50258, 50258, 50258]])]\n",
      "torch.Size([4, 903])\n",
      "4\n",
      "2\n",
      "torch.Size([4, 903, 50260])\n",
      "12\n",
      "torch.Size([4, 903, 50260])\n",
      "torch.Size([4, 903])\n",
      "tensor(102.7731, device='cuda:0', grad_fn=<NllLoss2DBackward>)\n",
      "411.0924987792969\n"
     ]
    }
   ],
   "source": [
    "device_ids = [0]\n",
    "\n",
    "# Initialize 2 models\n",
    "net_cls = MODEL_CLASS\n",
    "net = net_cls.from_pretrained('gpt2').cuda()\n",
    "net.resize_token_embeddings(token_size)\n",
    "net = FP16_Module(model)\n",
    "\n",
    "net_pi = net_cls.from_pretrained('gpt2').cuda()\n",
    "net_pi.resize_token_embeddings(token_size)\n",
    "net_pi = FP16_Module(model)\n",
    "\n",
    "\n",
    "# Sequential Tasks\n",
    "tasks = ['movies', 'boolq', 'scifact']\n",
    "for task_id in range(len(tasks)):\n",
    "    \n",
    "    _tasks = [tasks[task_id] for task_id in task_ids]\n",
    "    \n",
    "    ##### Start training on task_id #####\n",
    "    gen_token = get_gen_token(tasks[task_id])\n",
    "    TOKENIZER.add_tokens([gen_token])\n",
    "    # TOKENIZER.save_pretrained(model_dir)\n",
    "    SPECIAL_TOKENS[tasks[0]] = gen_token\n",
    "    SPECIAL_TOKEN_IDS[tasks[0]] = TOKENIZER.convert_tokens_to_ids(gen_token)\n",
    "    # logger.info('gen token = {} , gen token id = {}'.format(gen_token, SPECIAL_TOKEN_IDS[tasks[0]]))\n",
    "    # MODEL_CONFIG.vocab_size = len(TOKENIZER)\n",
    "    # MODEL_CONFIG.to_json_file(os.path.join(model_dir,CONFIG_NAME))\n",
    "\n",
    "    ##### Get Extra data and that particular dataset #####\n",
    "    train_extra_data = []\n",
    "    train_dataset = [TASK_DICT[t][\"train\"] for t in _tasks]\n",
    "    train_qadata = QADataset(train_dataset, \"train\", SPECIAL_TOKEN_IDS[_tasks[0]], train_extra_data)\n",
    "    max_train_batch_size = max(len(train_qadata) // min_n_steps, min_batch_size)\n",
    "    train_dataloader = create_dataloader(train_qadata, \"train\", max_train_batch_size)\n",
    "\n",
    "\n",
    "    ##### Stream from that dataset's dataloader #####\n",
    "    cum_loss, cum_qa_loss, cum_lm_loss, cur_n_inputs = 0, 0, 0, 0\n",
    "    for n_steps, (_, _, cqa, _, Y, gen_X, gen_Y) in enumerate(train_dataloader):\n",
    "        print(len(cqa))\n",
    "        print(cqa)\n",
    "        print(cqa[0].shape)\n",
    "\n",
    "        n_inputs = sum(_cqa.shape[0] for _cqa in cqa)\n",
    "        print(n_inputs)\n",
    "\n",
    "        support_x = cqa[0][:10,...]\n",
    "        support_y = Y[0][:10]\n",
    "        query_x = cqa[0][10:20,...]\n",
    "        query_y = Y[0][10:20]\n",
    "\n",
    "        support_x = support_x.to('cuda:0')\n",
    "        support_y = support_y.to('cuda:0')\n",
    "\n",
    "\n",
    "    #     for i in range(len(cqa)):\n",
    "    #         cqa[i] = (cqa[i].to(device_ids[i]),)\n",
    "    #         Y[i] = Y[i].to(device_ids[i])\n",
    "    #         gen_X[i] = (gen_X[i].to(device_ids[i]),)\n",
    "    #         gen_Y[i] = gen_Y[i].to(device_ids[i])\n",
    "\n",
    "\n",
    "    #     losses = get_losses(parallel_model, cqa, Y, gen_X, gen_Y, train_loss_fct)\n",
    "        # Supposed to be parallel!!\n",
    "    #     qa_logits = parallel_model([cqa[0][:10,...]])\n",
    "        qa_logits = model(support_x)\n",
    "\n",
    "    #     lm_logits = parallel_model(gen_X)\n",
    "    #     print(qa_logits)\n",
    "        print(len(qa_logits))\n",
    "        print(qa_logits[0].shape)\n",
    "        print(len(qa_logits[1]))\n",
    "    #      Somehow it also returns attentions in [1]?, this is selecting 0 of what WrapModel is doing \n",
    "\n",
    "\n",
    "\n",
    "        qa_logits = qa_logits[0]\n",
    "\n",
    "        print(qa_logits.shape)\n",
    "        print(support_y.shape)\n",
    "        qa_loss = train_loss_fct(qa_logits.transpose(1,2), support_y)\n",
    "    #     qa_loss = train_loss_fct([torch.transpose(l, 1, 2) for l in qa_logits], support_y)\n",
    "        print(qa_loss)\n",
    "    #     lm_loss = loss_fct([torch.transpose(l, 1, 2) for l in lm_logits], gen_Y)\n",
    "    #     loss = sum([torch.mean(qa_loss), args.lm_lambda * torch.mean(lm_loss)])\n",
    "        loss = qa_loss\n",
    "\n",
    "        # Update Optimizer\n",
    "        optimizer.backward(loss, update_master_grads=False) # instead of loss.backward() for fp16\n",
    "        optimizer.update_master_grads()\n",
    "        optimizer.clip_master_grads(max_grad_norm)\n",
    "        optimizer.step()\n",
    "        if not optimizer.overflow:\n",
    "            for i in range(n_inputs):\n",
    "                scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        qa_loss = qa_loss.item() * n_inputs # since each cross entropy loss only gives us the mean as reduciton\n",
    "    #     lm_loss = losses[1].item() * n_inputs\n",
    "    #     cum_loss += (qa_loss + lm_loss)\n",
    "        cum_qa_loss += qa_loss\n",
    "    #     cum_lm_loss += lm_loss\n",
    "        cur_n_inputs += n_inputs\n",
    "        print(qa_loss)\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FP16_Module(\n",
       "  (module): GPT2LMHeadModel(\n",
       "    (transformer): GPT2Model(\n",
       "      (wte): Embedding(50260, 768)\n",
       "      (wpe): Embedding(1024, 768)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "      (h): ModuleList(\n",
       "        (0): Block(\n",
       "          (ln_1): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): Block(\n",
       "          (ln_1): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): Block(\n",
       "          (ln_1): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): Block(\n",
       "          (ln_1): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): Block(\n",
       "          (ln_1): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): Block(\n",
       "          (ln_1): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): Block(\n",
       "          (ln_1): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): Block(\n",
       "          (ln_1): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): Block(\n",
       "          (ln_1): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): Block(\n",
       "          (ln_1): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): Block(\n",
       "          (ln_1): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): Block(\n",
       "          (ln_1): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (c_attn): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (ln_2): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Conv1D()\n",
       "            (c_proj): Conv1D()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_f): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SPECIAL_TOKENS_ATTRIBUTES',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_additional_special_tokens',\n",
       " '_bos_token',\n",
       " '_cls_token',\n",
       " '_convert_id_to_token',\n",
       " '_convert_token_to_id',\n",
       " '_convert_token_to_id_with_added_voc',\n",
       " '_eos_token',\n",
       " '_from_pretrained',\n",
       " '_mask_token',\n",
       " '_pad_token',\n",
       " '_sep_token',\n",
       " '_tokenize',\n",
       " '_unk_token',\n",
       " 'add_special_tokens',\n",
       " 'add_special_tokens_sentences_pair',\n",
       " 'add_special_tokens_single_sentence',\n",
       " 'add_tokens',\n",
       " 'added_tokens_decoder',\n",
       " 'added_tokens_encoder',\n",
       " 'additional_special_tokens',\n",
       " 'additional_special_tokens_ids',\n",
       " 'all_special_ids',\n",
       " 'all_special_tokens',\n",
       " 'bos_token',\n",
       " 'bos_token_id',\n",
       " 'bpe',\n",
       " 'bpe_ranks',\n",
       " 'byte_decoder',\n",
       " 'byte_encoder',\n",
       " 'cache',\n",
       " 'clean_up_tokenization',\n",
       " 'cls_token',\n",
       " 'cls_token_id',\n",
       " 'convert_ids_to_tokens',\n",
       " 'convert_tokens_to_ids',\n",
       " 'convert_tokens_to_string',\n",
       " 'decode',\n",
       " 'decoder',\n",
       " 'encode',\n",
       " 'encoder',\n",
       " 'eos_token',\n",
       " 'eos_token_id',\n",
       " 'errors',\n",
       " 'from_pretrained',\n",
       " 'init_inputs',\n",
       " 'init_kwargs',\n",
       " 'mask_token',\n",
       " 'mask_token_id',\n",
       " 'max_len',\n",
       " 'max_len_sentences_pair',\n",
       " 'max_len_single_sentence',\n",
       " 'max_model_input_sizes',\n",
       " 'pad_token',\n",
       " 'pad_token_id',\n",
       " 'pat',\n",
       " 'pretrained_init_configuration',\n",
       " 'pretrained_vocab_files_map',\n",
       " 'save_pretrained',\n",
       " 'save_vocabulary',\n",
       " 'sep_token',\n",
       " 'sep_token_id',\n",
       " 'special_tokens_map',\n",
       " 'tokenize',\n",
       " 'unk_token',\n",
       " 'unk_token_is',\n",
       " 'vocab_files_names',\n",
       " 'vocab_size']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pytorch_transformers.tokenization_gpt2.GPT2Tokenizer at 0x7f4c7d589550>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_fct = DataParallelCriterion(CrossEntropyLoss(ignore_index=FILL_VAL, weight=TOKENS_WEIGHT), args.device_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
