{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "\n",
    "from reptile_omniglotNShot import OmniglotNShot\n",
    "from reptile_naive import Naive\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner(nn.Module):\n",
    "    \"\"\"\n",
    "    This is a learner class, which will accept a specific network module, such as OmniNet that define the network forward\n",
    "    process. Learner class will create two same network, one as theta network and the other acts as theta_pi network.\n",
    "    for each episode, the theta_pi network will copy its initial parameters from theta network and update several steps\n",
    "    by meta-train set and then calculate its loss on meta-test set. All loss on meta-test set will be sumed together and\n",
    "    then backprop on theta network, which should be done on metalaerner class.\n",
    "    For learner class, it will be responsible for update for several steps on meta-train set and return with the loss on\n",
    "    meta-test set.\n",
    "    \"\"\"\n",
    "    def __init__(self, net_cls, *args):\n",
    "        \"\"\"\n",
    "        It will receive a class: net_cls and its parameters: args for net_cls.\n",
    "        :param net_cls: class, not instance\n",
    "        :param args: the parameters for net_cls\n",
    "        \"\"\"\n",
    "        super(Learner, self).__init__()\n",
    "        # pls make sure net_cls is a class but NOT an instance of class.\n",
    "        assert net_cls.__class__ == type\n",
    "        \n",
    "        # we will create two class instance meanwhile and use one as theta network and the other as theta_pi network.\n",
    "        self.net = net_cls(*args)\n",
    "        self.net_pi = net_cls(*args)\n",
    "        \n",
    "        # update theta_pi = theta_pi - lr * grad\n",
    "        # according to the paper, here we use naive version of SGD to update theta_pi\n",
    "        # 0.1 here means the learner_lr\n",
    "        self.optimizer = optim.SGD(self.net_pi.parameters(), 0.1)\n",
    "        \n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Override this function to return only net parameters for MetaLearner's optimize\n",
    "        it will ignore theta_pi network parameters.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return self.net.parameters()\n",
    "    \n",
    "    def update_pi(self):\n",
    "        \"\"\"\n",
    "        copy parameters from self.net -> self.net_pi\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        for m_from, m_to in zip(self.net.modules(), self.net_pi.modules()):\n",
    "            if isinstance(m_to, nn.Linear) or isinstance(m_to, nn.Conv2d) or isinstance(m_to, nn.BatchNorm2d):\n",
    "                m_to.weight.data = m_from.weight.data.clone()\n",
    "                if m_to.bias is not None:\n",
    "                    m_to.bias.data = m_from.bias.data.clone()\n",
    "                    \n",
    "    def forward(self, support_x, support_y, query_x, query_y, num_updates):\n",
    "        \"\"\"\n",
    "        learn on current episode meta-train: support_x & support_y and then calculate loss on meta-test set: query_x&y\n",
    "        :param support_x: [setsz, c_, h, w]\n",
    "        :param support_y: [setsz]\n",
    "        :param query_x:   [querysz, c_, h, w]\n",
    "        :param query_y:   [querysz]\n",
    "        :param num_updates: 5\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        \n",
    "        # now try to fine-tune from current $theta$ parameters -> $theta_pi$\n",
    "        # after num_updates of fine-tune, we will get a good theta_pi parameters so that it will retain satisfying\n",
    "        # performance on specific task, that's, current episode.\n",
    "        # firstly, copy theta_pi from theta network\n",
    "        self.update_pi()\n",
    "        \n",
    "        # update for several steps\n",
    "        for i in range(num_updates):\n",
    "            # forward and backward to update net_pi grad.\n",
    "            loss, pred = self.net_pi(support_x, support_y)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        # Compute the meta gradient and return it, the gradient is from one episode\n",
    "        # in metalearner, it will merge all loss from different episode and sum over it.\n",
    "        loss, pred = self.net_pi(query_x, query_y)\n",
    "        # pred: [setsz, n_way], indices: [setsz]\n",
    "        _, indices = torch.max(pred, dim=1)\n",
    "#         correct = torch.eq(indices, query_y).sum().data[0]\n",
    "        correct = torch.eq(indices, query_y).sum().item()\n",
    "        acc = correct / query_y.size(0)\n",
    "        \n",
    "        # gradient for validation on theta_pi\n",
    "        # after call autorad.grad, you can not call backward again except for setting create_graph = True\n",
    "        # as we will use the loss as dummpy loss to conduct a dummy backprop to write our gradients to theta network,\n",
    "        # here we set create_graph to true to support second time backward.\n",
    "        grads_pi = autograd.grad(loss, self.net_pi.parameters(), create_graph=True)\n",
    "\n",
    "        return loss, grads_pi, acc\n",
    "    \n",
    "    def net_forward(self, support_x, support_y):\n",
    "        \"\"\"\n",
    "        This function is purely for updating net network. In metalearner, we need the get the loss op from net network\n",
    "        to write our merged gradients into net network, hence will call this function to get a dummy loss op.\n",
    "        :param support_x: [setsz, c, h, w]\n",
    "        :param support_y: [sessz, c, h, w]\n",
    "        :return: dummy loss and dummy pred\n",
    "        \"\"\"\n",
    "        loss, pred = self.net(support_x, support_y)\n",
    "        return loss, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaLearner(nn.Module):\n",
    "    \"\"\"\n",
    "    As we have mentioned in Learner class, the metalearner class will receive a series of loss on different tasks/episodes\n",
    "    on theta_pi network, and it will merage all loss and then sum over it. The summed loss will be backproped on theta\n",
    "    network to update theta parameters, which is the initialization point we want to find.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, net_cls, net_cls_args, n_way, k_shot, meta_batchsz, beta, num_updates):\n",
    "        \"\"\"\n",
    "        :param net_cls: class, not instance. the class of specific Network for learner\n",
    "        :param net_cls_args: tuple, args for net_cls, like (n_way, imgsz)\n",
    "        :param n_way:\n",
    "        :param k_shot:\n",
    "        :param meta_batchsz: number of tasks/episode\n",
    "        :param beta: learning rate for meta-learner\n",
    "        :param num_updates: number of updates for learner\n",
    "        \"\"\"\n",
    "        super(MetaLearner, self).__init__()\n",
    "\n",
    "        self.n_way = n_way\n",
    "        self.k_shot = k_shot\n",
    "        self.meta_batchsz = meta_batchsz\n",
    "        self.beta = beta\n",
    "        # self.alpha = alpha # set alpha in Learner.optimizer directly.\n",
    "        self.num_updates = num_updates\n",
    "\n",
    "        # it will contains a learner class to learn on episodes and gather the loss together.\n",
    "        self.learner = Learner(net_cls, *net_cls_args)\n",
    "        # the optimizer is to update theta parameters, not theta_pi parameters.\n",
    "        self.optimizer = optim.Adam(self.learner.parameters(), lr=beta)\n",
    "        \n",
    "    def write_grads(self, dummy_loss, sum_grads_pi):\n",
    "        \"\"\"\n",
    "        write loss into learner.net, gradients come from sum_grads_pi.\n",
    "        Since the gradients info is not calculated by general backward, we need this function to write the right gradients\n",
    "        into theta network and update theta parameters as wished.\n",
    "        :param dummy_loss: dummy loss, nothing but to write our gradients by hook\n",
    "        :param sum_grads_pi: the summed gradients\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # Register a hook on each parameter in the net that replaces the current dummy grad\n",
    "        # with our grads accumulated across the meta-batch\n",
    "        hooks = []\n",
    "\n",
    "        for i, v in enumerate(self.learner.parameters()):\n",
    "            def closure():\n",
    "                ii = i\n",
    "                return lambda grad: sum_grads_pi[ii]\n",
    "\n",
    "            # if you write: hooks.append( v.register_hook(lambda grad : sum_grads_pi[i]) )\n",
    "            # it will pop an ERROR, i don't know why?\n",
    "            hooks.append(v.register_hook(closure()))\n",
    "\n",
    "        # use our sumed gradients_pi to update the theta/net network,\n",
    "        # since our optimizer receive the self.net.parameters() only.\n",
    "        self.optimizer.zero_grad()\n",
    "        dummy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # if you do NOT remove the hook, the GPU memory will expode!!!\n",
    "        for h in hooks:\n",
    "            h.remove()\n",
    "            \n",
    "    def forward(self, support_x, support_y, query_x, query_y):\n",
    "        \"\"\"\n",
    "        Here we receive a series of episode, each episode will be learned by learner and get a loss on parameters theta.\n",
    "        we gather the loss and sum all the loss and then update theta network.\n",
    "        setsz = n_way * k_shotf\n",
    "        querysz = n_way * k_shot\n",
    "        :param support_x: [meta_batchsz, setsz, c_, h, w]\n",
    "        :param support_y: [meta_batchsz, setsz]\n",
    "        :param query_x:   [meta_batchsz, querysz, c_, h, w]\n",
    "        :param query_y:   [meta_batchsz, querysz]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        sum_grads_pi = None\n",
    "        meta_batchsz = support_y.size(0)\n",
    "\n",
    "        # support_x[i]: [setsz, c_, h, w]\n",
    "        # we do different learning task sequentially, not parallel.\n",
    "        accs = []\n",
    "        # for each task/episode.\n",
    "        for i in range(meta_batchsz):\n",
    "            _, grad_pi, episode_acc = self.learner(support_x[i], support_y[i], query_x[i], query_y[i], self.num_updates)\n",
    "            accs.append(episode_acc)\n",
    "            if sum_grads_pi is None:\n",
    "                sum_grads_pi = grad_pi\n",
    "            else:  # accumulate all gradients from different episode learner\n",
    "                sum_grads_pi = [torch.add(i, j) for i, j in zip(sum_grads_pi, grad_pi)]\n",
    "\n",
    "        # As we already have the grads to update\n",
    "        # We use a dummy forward / backward pass to get the correct grads into self.net\n",
    "        # the right grads will be updated by hook, ignoring backward.\n",
    "        # use hook mechnism to write sumed gradient into network.\n",
    "        # we need to update the theta/net network, we need a op from net network, so we call self.learner.net_forward\n",
    "        # to get the op from net network, since the loss from self.learner.forward will return loss from net_pi network.\n",
    "        dummy_loss, _ = self.learner.net_forward(support_x[0], support_y[0])\n",
    "        self.write_grads(dummy_loss, sum_grads_pi)\n",
    "\n",
    "        return accs\n",
    "    \n",
    "    def pred(self, support_x, support_y, query_x, query_y):\n",
    "        \"\"\"\n",
    "        predict for query_x\n",
    "        :param support_x:\n",
    "        :param support_y:\n",
    "        :param query_x:\n",
    "        :param query_y:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        meta_batchsz = support_y.size(0)\n",
    "\n",
    "        accs = []\n",
    "        # for each task/episode.\n",
    "        # the learner will copy parameters from current theta network and then fine-tune on support set.\n",
    "        for i in range(meta_batchsz):\n",
    "            _, _, episode_acc = self.learner(support_x[i], support_y[i], query_x[i], query_y[i], self.num_updates)\n",
    "            accs.append(episode_acc)\n",
    "\n",
    "        return np.array(accs).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before norm: mean 0.9187823248049223 max 1.0 min 0.0 std 0.21980729156739412\n",
      "after norm: mean -1.4055967274460421e-15 max 0.36949490899930376 min -4.179944706353012 std 0.9999999999999992\n",
      "train_shape (1200, 20, 28, 28, 1) test_shape (423, 20, 28, 28, 1)\n",
      "Naive(\n",
      "  (net): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU(inplace=True)\n",
      "    (11): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): ReLU(inplace=True)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=64, out_features=5, bias=True)\n",
      "  )\n",
      "  (criteon): CrossEntropyLoss()\n",
      ")\n",
      "Naive repnet sz: torch.Size([2, 64, 1, 1])\n",
      "Naive(\n",
      "  (net): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "    (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU(inplace=True)\n",
      "    (11): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): ReLU(inplace=True)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=64, out_features=5, bias=True)\n",
      "  )\n",
      "  (criteon): CrossEntropyLoss()\n",
      ")\n",
      "Naive repnet sz: torch.Size([2, 64, 1, 1])\n",
      "episode: 0 \tfinetune acc:0.606250 \t\ttest acc:0.629167\n",
      "episode: 30 \tfinetune acc:0.700000 \t\ttest acc:0.758333\n",
      "episode: 60 \tfinetune acc:0.737500 \t\ttest acc:0.783333\n",
      "episode: 90 \tfinetune acc:0.837500 \t\ttest acc:0.825000\n",
      "episode: 120 \tfinetune acc:0.843750 \t\ttest acc:0.808333\n",
      "episode: 150 \tfinetune acc:0.831250 \t\ttest acc:0.843750\n",
      "episode: 180 \tfinetune acc:0.837500 \t\ttest acc:0.850000\n",
      "episode: 210 \tfinetune acc:0.893750 \t\ttest acc:0.875000\n",
      "episode: 240 \tfinetune acc:0.843750 \t\ttest acc:0.847917\n",
      "episode: 270 \tfinetune acc:0.862500 \t\ttest acc:0.889583\n",
      "episode: 300 \tfinetune acc:0.893750 \t\ttest acc:0.887500\n",
      "episode: 330 \tfinetune acc:0.912500 \t\ttest acc:0.893750\n",
      "episode: 360 \tfinetune acc:0.900000 \t\ttest acc:0.860417\n",
      "episode: 390 \tfinetune acc:0.831250 \t\ttest acc:0.893750\n",
      "episode: 420 \tfinetune acc:0.975000 \t\ttest acc:0.870833\n",
      "episode: 450 \tfinetune acc:0.881250 \t\ttest acc:0.904167\n",
      "episode: 480 \tfinetune acc:0.862500 \t\ttest acc:0.864583\n",
      "episode: 510 \tfinetune acc:0.918750 \t\ttest acc:0.885417\n",
      "episode: 540 \tfinetune acc:0.918750 \t\ttest acc:0.916667\n",
      "episode: 570 \tfinetune acc:0.818750 \t\ttest acc:0.900000\n",
      "episode: 600 \tfinetune acc:0.875000 \t\ttest acc:0.893750\n",
      "episode: 630 \tfinetune acc:0.918750 \t\ttest acc:0.877083\n",
      "episode: 660 \tfinetune acc:0.881250 \t\ttest acc:0.900000\n",
      "episode: 690 \tfinetune acc:0.918750 \t\ttest acc:0.904167\n",
      "episode: 720 \tfinetune acc:0.868750 \t\ttest acc:0.893750\n",
      "episode: 750 \tfinetune acc:0.893750 \t\ttest acc:0.881250\n",
      "episode: 780 \tfinetune acc:0.906250 \t\ttest acc:0.897917\n",
      "episode: 810 \tfinetune acc:0.881250 \t\ttest acc:0.895833\n",
      "episode: 840 \tfinetune acc:0.937500 \t\ttest acc:0.910417\n",
      "episode: 870 \tfinetune acc:0.843750 \t\ttest acc:0.902083\n",
      "episode: 900 \tfinetune acc:0.925000 \t\ttest acc:0.927083\n",
      "episode: 930 \tfinetune acc:0.912500 \t\ttest acc:0.922917\n",
      "episode: 960 \tfinetune acc:0.931250 \t\ttest acc:0.925000\n",
      "episode: 990 \tfinetune acc:0.931250 \t\ttest acc:0.910417\n",
      "episode: 1020 \tfinetune acc:0.943750 \t\ttest acc:0.927083\n",
      "episode: 1050 \tfinetune acc:0.912500 \t\ttest acc:0.908333\n",
      "episode: 1080 \tfinetune acc:0.962500 \t\ttest acc:0.906250\n",
      "episode: 1110 \tfinetune acc:0.900000 \t\ttest acc:0.897917\n",
      "episode: 1140 \tfinetune acc:0.925000 \t\ttest acc:0.912500\n",
      "episode: 1170 \tfinetune acc:0.900000 \t\ttest acc:0.918750\n",
      "episode: 1200 \tfinetune acc:0.943750 \t\ttest acc:0.916667\n",
      "episode: 1230 \tfinetune acc:0.968750 \t\ttest acc:0.937500\n",
      "episode: 1260 \tfinetune acc:0.856250 \t\ttest acc:0.914583\n",
      "episode: 1290 \tfinetune acc:0.900000 \t\ttest acc:0.918750\n",
      "episode: 1320 \tfinetune acc:0.912500 \t\ttest acc:0.922917\n",
      "episode: 1350 \tfinetune acc:0.900000 \t\ttest acc:0.902083\n",
      "episode: 1380 \tfinetune acc:0.912500 \t\ttest acc:0.933333\n",
      "episode: 1410 \tfinetune acc:0.962500 \t\ttest acc:0.947917\n",
      "episode: 1440 \tfinetune acc:0.925000 \t\ttest acc:0.908333\n",
      "episode: 1470 \tfinetune acc:0.912500 \t\ttest acc:0.931250\n",
      "episode: 1500 \tfinetune acc:0.912500 \t\ttest acc:0.943750\n",
      "episode: 1530 \tfinetune acc:0.950000 \t\ttest acc:0.927083\n",
      "episode: 1560 \tfinetune acc:0.981250 \t\ttest acc:0.916667\n",
      "episode: 1590 \tfinetune acc:0.918750 \t\ttest acc:0.929167\n",
      "episode: 1620 \tfinetune acc:0.887500 \t\ttest acc:0.945833\n",
      "episode: 1650 \tfinetune acc:0.893750 \t\ttest acc:0.933333\n",
      "episode: 1680 \tfinetune acc:0.962500 \t\ttest acc:0.918750\n",
      "episode: 1710 \tfinetune acc:0.931250 \t\ttest acc:0.929167\n",
      "episode: 1740 \tfinetune acc:0.937500 \t\ttest acc:0.939583\n",
      "episode: 1770 \tfinetune acc:0.906250 \t\ttest acc:0.931250\n",
      "episode: 1800 \tfinetune acc:0.918750 \t\ttest acc:0.937500\n",
      "episode: 1830 \tfinetune acc:0.956250 \t\ttest acc:0.922917\n",
      "episode: 1860 \tfinetune acc:0.968750 \t\ttest acc:0.927083\n",
      "episode: 1890 \tfinetune acc:0.937500 \t\ttest acc:0.912500\n",
      "episode: 1920 \tfinetune acc:0.956250 \t\ttest acc:0.941667\n",
      "episode: 1950 \tfinetune acc:0.912500 \t\ttest acc:0.931250\n",
      "episode: 1980 \tfinetune acc:0.925000 \t\ttest acc:0.929167\n",
      "episode: 2010 \tfinetune acc:0.950000 \t\ttest acc:0.931250\n",
      "episode: 2040 \tfinetune acc:0.912500 \t\ttest acc:0.922917\n",
      "episode: 2070 \tfinetune acc:0.925000 \t\ttest acc:0.939583\n",
      "episode: 2100 \tfinetune acc:0.956250 \t\ttest acc:0.937500\n",
      "episode: 2130 \tfinetune acc:0.943750 \t\ttest acc:0.927083\n",
      "episode: 2160 \tfinetune acc:0.981250 \t\ttest acc:0.954167\n",
      "episode: 2190 \tfinetune acc:0.931250 \t\ttest acc:0.927083\n",
      "episode: 2220 \tfinetune acc:0.937500 \t\ttest acc:0.950000\n",
      "episode: 2250 \tfinetune acc:0.950000 \t\ttest acc:0.920833\n",
      "episode: 2280 \tfinetune acc:0.950000 \t\ttest acc:0.927083\n",
      "episode: 2310 \tfinetune acc:0.937500 \t\ttest acc:0.947917\n",
      "episode: 2340 \tfinetune acc:0.925000 \t\ttest acc:0.941667\n",
      "episode: 2370 \tfinetune acc:0.937500 \t\ttest acc:0.920833\n",
      "episode: 2400 \tfinetune acc:0.893750 \t\ttest acc:0.922917\n",
      "episode: 2430 \tfinetune acc:0.981250 \t\ttest acc:0.945833\n",
      "episode: 2460 \tfinetune acc:0.943750 \t\ttest acc:0.922917\n",
      "episode: 2490 \tfinetune acc:0.937500 \t\ttest acc:0.943750\n",
      "episode: 2520 \tfinetune acc:0.962500 \t\ttest acc:0.954167\n",
      "episode: 2550 \tfinetune acc:0.943750 \t\ttest acc:0.939583\n",
      "episode: 2580 \tfinetune acc:0.937500 \t\ttest acc:0.931250\n",
      "episode: 2610 \tfinetune acc:0.956250 \t\ttest acc:0.964583\n",
      "episode: 2640 \tfinetune acc:0.956250 \t\ttest acc:0.931250\n",
      "episode: 2670 \tfinetune acc:0.925000 \t\ttest acc:0.950000\n",
      "episode: 2700 \tfinetune acc:0.925000 \t\ttest acc:0.952083\n",
      "episode: 2730 \tfinetune acc:0.968750 \t\ttest acc:0.952083\n",
      "episode: 2760 \tfinetune acc:0.943750 \t\ttest acc:0.943750\n",
      "episode: 2790 \tfinetune acc:0.968750 \t\ttest acc:0.945833\n",
      "episode: 2820 \tfinetune acc:0.962500 \t\ttest acc:0.945833\n",
      "episode: 2850 \tfinetune acc:0.968750 \t\ttest acc:0.933333\n",
      "episode: 2880 \tfinetune acc:0.925000 \t\ttest acc:0.956250\n",
      "episode: 2910 \tfinetune acc:0.962500 \t\ttest acc:0.956250\n",
      "episode: 2940 \tfinetune acc:0.956250 \t\ttest acc:0.947917\n",
      "episode: 2970 \tfinetune acc:0.975000 \t\ttest acc:0.958333\n",
      "episode: 3000 \tfinetune acc:0.956250 \t\ttest acc:0.941667\n",
      "episode: 3030 \tfinetune acc:0.968750 \t\ttest acc:0.952083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3060 \tfinetune acc:0.987500 \t\ttest acc:0.943750\n",
      "episode: 3090 \tfinetune acc:0.950000 \t\ttest acc:0.918750\n",
      "episode: 3120 \tfinetune acc:0.950000 \t\ttest acc:0.945833\n",
      "episode: 3150 \tfinetune acc:0.956250 \t\ttest acc:0.954167\n",
      "episode: 3180 \tfinetune acc:0.937500 \t\ttest acc:0.962500\n",
      "episode: 3210 \tfinetune acc:0.968750 \t\ttest acc:0.958333\n",
      "episode: 3240 \tfinetune acc:0.968750 \t\ttest acc:0.947917\n",
      "episode: 3270 \tfinetune acc:0.937500 \t\ttest acc:0.947917\n",
      "episode: 3300 \tfinetune acc:0.943750 \t\ttest acc:0.958333\n",
      "episode: 3330 \tfinetune acc:0.931250 \t\ttest acc:0.943750\n",
      "episode: 3360 \tfinetune acc:0.925000 \t\ttest acc:0.947917\n",
      "episode: 3390 \tfinetune acc:0.937500 \t\ttest acc:0.958333\n",
      "episode: 3420 \tfinetune acc:0.950000 \t\ttest acc:0.945833\n",
      "episode: 3450 \tfinetune acc:0.962500 \t\ttest acc:0.941667\n",
      "episode: 3480 \tfinetune acc:0.956250 \t\ttest acc:0.939583\n",
      "episode: 3510 \tfinetune acc:0.931250 \t\ttest acc:0.952083\n",
      "episode: 3540 \tfinetune acc:0.943750 \t\ttest acc:0.958333\n",
      "episode: 3570 \tfinetune acc:0.975000 \t\ttest acc:0.943750\n",
      "episode: 3600 \tfinetune acc:0.925000 \t\ttest acc:0.954167\n",
      "episode: 3630 \tfinetune acc:0.968750 \t\ttest acc:0.966667\n",
      "episode: 3660 \tfinetune acc:0.956250 \t\ttest acc:0.956250\n",
      "episode: 3690 \tfinetune acc:0.931250 \t\ttest acc:0.947917\n",
      "episode: 3720 \tfinetune acc:0.925000 \t\ttest acc:0.958333\n",
      "episode: 3750 \tfinetune acc:0.906250 \t\ttest acc:0.952083\n",
      "episode: 3780 \tfinetune acc:0.937500 \t\ttest acc:0.931250\n",
      "episode: 3810 \tfinetune acc:0.943750 \t\ttest acc:0.950000\n",
      "episode: 3840 \tfinetune acc:0.975000 \t\ttest acc:0.943750\n",
      "episode: 3870 \tfinetune acc:0.931250 \t\ttest acc:0.929167\n",
      "episode: 3900 \tfinetune acc:0.931250 \t\ttest acc:0.950000\n",
      "episode: 3930 \tfinetune acc:0.962500 \t\ttest acc:0.939583\n",
      "episode: 3960 \tfinetune acc:0.943750 \t\ttest acc:0.935417\n",
      "episode: 3990 \tfinetune acc:0.975000 \t\ttest acc:0.920833\n",
      "episode: 4020 \tfinetune acc:0.956250 \t\ttest acc:0.933333\n",
      "episode: 4050 \tfinetune acc:0.943750 \t\ttest acc:0.954167\n",
      "episode: 4080 \tfinetune acc:0.956250 \t\ttest acc:0.947917\n",
      "episode: 4110 \tfinetune acc:0.900000 \t\ttest acc:0.956250\n",
      "episode: 4140 \tfinetune acc:0.918750 \t\ttest acc:0.931250\n",
      "episode: 4170 \tfinetune acc:0.937500 \t\ttest acc:0.962500\n",
      "episode: 4200 \tfinetune acc:0.906250 \t\ttest acc:0.935417\n",
      "episode: 4230 \tfinetune acc:0.943750 \t\ttest acc:0.954167\n",
      "episode: 4260 \tfinetune acc:0.912500 \t\ttest acc:0.945833\n",
      "episode: 4290 \tfinetune acc:0.950000 \t\ttest acc:0.945833\n",
      "episode: 4320 \tfinetune acc:0.975000 \t\ttest acc:0.916667\n",
      "episode: 4350 \tfinetune acc:0.912500 \t\ttest acc:0.943750\n",
      "episode: 4380 \tfinetune acc:0.925000 \t\ttest acc:0.937500\n",
      "episode: 4410 \tfinetune acc:0.925000 \t\ttest acc:0.945833\n",
      "episode: 4440 \tfinetune acc:0.956250 \t\ttest acc:0.958333\n",
      "episode: 4470 \tfinetune acc:0.906250 \t\ttest acc:0.956250\n",
      "episode: 4500 \tfinetune acc:0.925000 \t\ttest acc:0.954167\n",
      "episode: 4530 \tfinetune acc:0.937500 \t\ttest acc:0.956250\n",
      "episode: 4560 \tfinetune acc:0.962500 \t\ttest acc:0.956250\n",
      "episode: 4590 \tfinetune acc:0.950000 \t\ttest acc:0.943750\n",
      "episode: 4620 \tfinetune acc:0.931250 \t\ttest acc:0.966667\n",
      "episode: 4650 \tfinetune acc:0.968750 \t\ttest acc:0.962500\n",
      "episode: 4680 \tfinetune acc:0.962500 \t\ttest acc:0.945833\n",
      "episode: 4710 \tfinetune acc:0.956250 \t\ttest acc:0.933333\n",
      "episode: 4740 \tfinetune acc:0.912500 \t\ttest acc:0.972917\n",
      "episode: 4770 \tfinetune acc:0.937500 \t\ttest acc:0.952083\n",
      "episode: 4800 \tfinetune acc:0.943750 \t\ttest acc:0.941667\n",
      "episode: 4830 \tfinetune acc:0.956250 \t\ttest acc:0.966667\n",
      "episode: 4860 \tfinetune acc:0.918750 \t\ttest acc:0.958333\n",
      "episode: 4890 \tfinetune acc:0.987500 \t\ttest acc:0.950000\n",
      "episode: 4920 \tfinetune acc:0.962500 \t\ttest acc:0.941667\n",
      "episode: 4950 \tfinetune acc:0.956250 \t\ttest acc:0.956250\n",
      "episode: 4980 \tfinetune acc:0.950000 \t\ttest acc:0.966667\n",
      "episode: 5010 \tfinetune acc:0.943750 \t\ttest acc:0.967188\n",
      "episode: 5040 \tfinetune acc:0.981250 \t\ttest acc:0.959375\n",
      "episode: 5070 \tfinetune acc:0.975000 \t\ttest acc:0.951563\n",
      "episode: 5100 \tfinetune acc:0.975000 \t\ttest acc:0.959375\n",
      "episode: 5130 \tfinetune acc:0.968750 \t\ttest acc:0.973437\n",
      "episode: 5160 \tfinetune acc:0.962500 \t\ttest acc:0.960938\n",
      "episode: 5190 \tfinetune acc:0.968750 \t\ttest acc:0.957812\n",
      "episode: 5220 \tfinetune acc:0.987500 \t\ttest acc:0.965625\n",
      "episode: 5250 \tfinetune acc:0.968750 \t\ttest acc:0.946875\n",
      "episode: 5280 \tfinetune acc:0.950000 \t\ttest acc:0.971875\n",
      "episode: 5310 \tfinetune acc:0.950000 \t\ttest acc:0.965625\n",
      "episode: 5340 \tfinetune acc:0.968750 \t\ttest acc:0.953125\n",
      "episode: 5370 \tfinetune acc:0.962500 \t\ttest acc:0.971875\n",
      "episode: 5400 \tfinetune acc:0.975000 \t\ttest acc:0.962500\n",
      "episode: 5430 \tfinetune acc:0.968750 \t\ttest acc:0.953125\n",
      "episode: 5460 \tfinetune acc:0.975000 \t\ttest acc:0.951562\n",
      "episode: 5490 \tfinetune acc:0.956250 \t\ttest acc:0.973437\n",
      "episode: 5520 \tfinetune acc:0.968750 \t\ttest acc:0.953125\n",
      "episode: 5550 \tfinetune acc:0.950000 \t\ttest acc:0.967187\n",
      "episode: 5580 \tfinetune acc:0.968750 \t\ttest acc:0.965625\n",
      "episode: 5610 \tfinetune acc:0.993750 \t\ttest acc:0.967187\n",
      "episode: 5640 \tfinetune acc:0.968750 \t\ttest acc:0.968750\n",
      "episode: 5670 \tfinetune acc:0.950000 \t\ttest acc:0.965625\n",
      "episode: 5700 \tfinetune acc:0.981250 \t\ttest acc:0.956250\n",
      "episode: 5730 \tfinetune acc:0.968750 \t\ttest acc:0.975000\n",
      "episode: 5760 \tfinetune acc:0.981250 \t\ttest acc:0.971875\n",
      "episode: 5790 \tfinetune acc:0.956250 \t\ttest acc:0.970313\n",
      "episode: 5820 \tfinetune acc:0.981250 \t\ttest acc:0.967187\n",
      "episode: 5850 \tfinetune acc:0.962500 \t\ttest acc:0.959375\n",
      "episode: 5880 \tfinetune acc:0.943750 \t\ttest acc:0.957812\n",
      "episode: 5910 \tfinetune acc:0.993750 \t\ttest acc:0.967187\n",
      "episode: 5940 \tfinetune acc:0.981250 \t\ttest acc:0.957813\n",
      "episode: 5970 \tfinetune acc:0.962500 \t\ttest acc:0.959375\n",
      "episode: 6000 \tfinetune acc:0.943750 \t\ttest acc:0.968750\n",
      "episode: 6030 \tfinetune acc:0.968750 \t\ttest acc:0.965625\n",
      "episode: 6060 \tfinetune acc:0.975000 \t\ttest acc:0.975000\n",
      "episode: 6090 \tfinetune acc:0.962500 \t\ttest acc:0.957812\n",
      "episode: 6120 \tfinetune acc:0.950000 \t\ttest acc:0.975000\n",
      "episode: 6150 \tfinetune acc:0.975000 \t\ttest acc:0.973437\n",
      "episode: 6180 \tfinetune acc:0.981250 \t\ttest acc:0.957812\n",
      "episode: 6210 \tfinetune acc:0.981250 \t\ttest acc:0.970312\n",
      "episode: 6240 \tfinetune acc:0.981250 \t\ttest acc:0.970312\n",
      "episode: 6270 \tfinetune acc:0.937500 \t\ttest acc:0.957812\n",
      "episode: 6300 \tfinetune acc:0.981250 \t\ttest acc:0.950000\n",
      "episode: 6330 \tfinetune acc:0.962500 \t\ttest acc:0.973437\n",
      "episode: 6360 \tfinetune acc:0.950000 \t\ttest acc:0.973437\n",
      "episode: 6390 \tfinetune acc:0.956250 \t\ttest acc:0.964062\n",
      "episode: 6420 \tfinetune acc:0.925000 \t\ttest acc:0.962500\n",
      "episode: 6450 \tfinetune acc:0.993750 \t\ttest acc:0.956250\n",
      "episode: 6480 \tfinetune acc:0.962500 \t\ttest acc:0.962500\n",
      "episode: 6510 \tfinetune acc:0.962500 \t\ttest acc:0.965625\n",
      "episode: 6540 \tfinetune acc:0.962500 \t\ttest acc:0.971875\n",
      "episode: 6570 \tfinetune acc:0.937500 \t\ttest acc:0.960938\n",
      "episode: 6600 \tfinetune acc:0.962500 \t\ttest acc:0.964062\n",
      "episode: 6630 \tfinetune acc:0.950000 \t\ttest acc:0.979688\n",
      "episode: 6660 \tfinetune acc:0.956250 \t\ttest acc:0.971875\n",
      "episode: 6690 \tfinetune acc:0.962500 \t\ttest acc:0.957812\n",
      "episode: 6720 \tfinetune acc:0.987500 \t\ttest acc:0.959375\n",
      "episode: 6750 \tfinetune acc:0.950000 \t\ttest acc:0.970312\n",
      "episode: 6780 \tfinetune acc:0.968750 \t\ttest acc:0.976562\n",
      "episode: 6810 \tfinetune acc:0.962500 \t\ttest acc:0.960938\n",
      "episode: 6840 \tfinetune acc:0.956250 \t\ttest acc:0.960937\n",
      "episode: 6870 \tfinetune acc:0.968750 \t\ttest acc:0.970312\n",
      "episode: 6900 \tfinetune acc:0.987500 \t\ttest acc:0.956250\n",
      "episode: 6930 \tfinetune acc:0.962500 \t\ttest acc:0.962500\n",
      "episode: 6960 \tfinetune acc:0.937500 \t\ttest acc:0.967188\n",
      "episode: 6990 \tfinetune acc:0.981250 \t\ttest acc:0.967187\n",
      "episode: 7020 \tfinetune acc:0.987500 \t\ttest acc:0.973438\n",
      "episode: 7050 \tfinetune acc:0.987500 \t\ttest acc:0.971875\n",
      "episode: 7080 \tfinetune acc:0.975000 \t\ttest acc:0.978125\n",
      "episode: 7110 \tfinetune acc:0.956250 \t\ttest acc:0.976562\n",
      "episode: 7140 \tfinetune acc:0.962500 \t\ttest acc:0.945312\n",
      "episode: 7170 \tfinetune acc:0.987500 \t\ttest acc:0.968750\n",
      "episode: 7200 \tfinetune acc:0.968750 \t\ttest acc:0.970313\n",
      "episode: 7230 \tfinetune acc:0.987500 \t\ttest acc:0.964063\n",
      "episode: 7260 \tfinetune acc:0.950000 \t\ttest acc:0.965625\n",
      "episode: 7290 \tfinetune acc:0.975000 \t\ttest acc:0.964062\n",
      "episode: 7320 \tfinetune acc:0.956250 \t\ttest acc:0.957812\n",
      "episode: 7350 \tfinetune acc:0.925000 \t\ttest acc:0.968750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 7380 \tfinetune acc:0.981250 \t\ttest acc:0.976563\n",
      "episode: 7410 \tfinetune acc:0.968750 \t\ttest acc:0.971875\n",
      "episode: 7440 \tfinetune acc:0.968750 \t\ttest acc:0.951562\n",
      "episode: 7470 \tfinetune acc:0.962500 \t\ttest acc:0.973437\n",
      "episode: 7500 \tfinetune acc:0.975000 \t\ttest acc:0.965625\n",
      "episode: 7530 \tfinetune acc:0.975000 \t\ttest acc:0.971875\n",
      "episode: 7560 \tfinetune acc:0.943750 \t\ttest acc:0.978125\n",
      "episode: 7590 \tfinetune acc:0.962500 \t\ttest acc:0.978125\n",
      "episode: 7620 \tfinetune acc:0.931250 \t\ttest acc:0.975000\n",
      "episode: 7650 \tfinetune acc:0.981250 \t\ttest acc:0.957812\n",
      "episode: 7680 \tfinetune acc:0.975000 \t\ttest acc:0.967187\n",
      "episode: 7710 \tfinetune acc:0.937500 \t\ttest acc:0.971875\n",
      "episode: 7740 \tfinetune acc:0.968750 \t\ttest acc:0.970312\n",
      "episode: 7770 \tfinetune acc:0.962500 \t\ttest acc:0.965625\n",
      "episode: 7800 \tfinetune acc:0.962500 \t\ttest acc:0.956250\n",
      "episode: 7830 \tfinetune acc:0.956250 \t\ttest acc:0.948437\n",
      "episode: 7860 \tfinetune acc:0.950000 \t\ttest acc:0.957812\n",
      "episode: 7890 \tfinetune acc:0.918750 \t\ttest acc:0.975000\n",
      "episode: 7920 \tfinetune acc:0.975000 \t\ttest acc:0.981250\n",
      "episode: 7950 \tfinetune acc:0.987500 \t\ttest acc:0.962500\n",
      "episode: 7980 \tfinetune acc:0.975000 \t\ttest acc:0.971875\n",
      "episode: 8010 \tfinetune acc:0.993750 \t\ttest acc:0.979687\n",
      "episode: 8040 \tfinetune acc:0.950000 \t\ttest acc:0.981250\n",
      "episode: 8070 \tfinetune acc:0.962500 \t\ttest acc:0.965625\n",
      "episode: 8100 \tfinetune acc:0.943750 \t\ttest acc:0.957812\n",
      "episode: 8130 \tfinetune acc:0.962500 \t\ttest acc:0.970313\n",
      "episode: 8160 \tfinetune acc:0.981250 \t\ttest acc:0.965625\n",
      "episode: 8190 \tfinetune acc:0.937500 \t\ttest acc:0.965625\n",
      "episode: 8220 \tfinetune acc:0.968750 \t\ttest acc:0.967187\n",
      "episode: 8250 \tfinetune acc:0.981250 \t\ttest acc:0.968750\n",
      "episode: 8280 \tfinetune acc:0.987500 \t\ttest acc:0.967188\n",
      "episode: 8310 \tfinetune acc:0.962500 \t\ttest acc:0.957812\n",
      "episode: 8340 \tfinetune acc:0.937500 \t\ttest acc:0.965625\n",
      "episode: 8370 \tfinetune acc:0.968750 \t\ttest acc:0.970312\n",
      "episode: 8400 \tfinetune acc:0.950000 \t\ttest acc:0.982812\n",
      "episode: 8430 \tfinetune acc:0.925000 \t\ttest acc:0.970313\n",
      "episode: 8460 \tfinetune acc:0.950000 \t\ttest acc:0.978125\n",
      "episode: 8490 \tfinetune acc:0.937500 \t\ttest acc:0.959375\n",
      "episode: 8520 \tfinetune acc:0.968750 \t\ttest acc:0.962500\n",
      "episode: 8550 \tfinetune acc:0.962500 \t\ttest acc:0.978125\n",
      "episode: 8580 \tfinetune acc:0.950000 \t\ttest acc:0.967188\n",
      "episode: 8610 \tfinetune acc:0.981250 \t\ttest acc:0.964062\n",
      "episode: 8640 \tfinetune acc:0.968750 \t\ttest acc:0.967187\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c3e7c5e4f048>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# backprop has been embeded in forward func.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0maccs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupport_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-4888f21810c2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, support_x, support_y, query_x, query_y)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# for each task/episode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_batchsz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_pi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupport_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_updates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0maccs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msum_grads_pi\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-567059df9cf9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, support_x, support_y, query_x, query_y, num_updates)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_updates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# forward and backward to update net_pi grad.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet_pi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupport_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MAML-Pytorch/reptile_naive.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, target)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;31m# TODO: if statement only here to tell the jit to skip emitting this when it is None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmomentum\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# use cumulative moving average\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                     \u001b[0mexponential_average_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m    591\u001b[0m                     \u001b[0;32mdel\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_parameters'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "meta_batchsz = 32\n",
    "n_way = 5\n",
    "k_shot = 1\n",
    "k_query = k_shot\n",
    "meta_lr = 1e-3\n",
    "num_updates = 5\n",
    "dataset = 'omniglot'\n",
    "\n",
    "imgsz = 28\n",
    "db = OmniglotNShot('dataset', batchsz=meta_batchsz, n_way=n_way, k_shot=k_shot, k_query=k_query, imgsz=imgsz)\n",
    "meta = MetaLearner(Naive, (n_way, imgsz), n_way=n_way, k_shot=k_shot, meta_batchsz=meta_batchsz, beta=meta_lr,\n",
    "                   num_updates=num_updates).cuda()\n",
    "\n",
    "tb = SummaryWriter('runs')\n",
    "\n",
    "# main loop\n",
    "for episode_num in range(200000):\n",
    "    # 1. train\n",
    "    support_x, support_y, query_x, query_y = db.get_batch('test')\n",
    "    support_x = Variable( torch.from_numpy(support_x).float().transpose(2, 4).transpose(3, 4).repeat(1, 1, 3, 1, 1)).cuda()\n",
    "    query_x = Variable( torch.from_numpy(query_x).float().transpose(2, 4).transpose(3, 4).repeat(1, 1, 3, 1, 1)).cuda()\n",
    "    support_y = Variable(torch.from_numpy(support_y).long()).cuda()\n",
    "    query_y = Variable(torch.from_numpy(query_y).long()).cuda()\n",
    "\n",
    "    # backprop has been embeded in forward func.\n",
    "    accs = meta(support_x, support_y, query_x, query_y)\n",
    "    train_acc = np.array(accs).mean()\n",
    "    \n",
    "    # 2. test\n",
    "    if episode_num % 30 == 0:\n",
    "        test_accs = []\n",
    "        for i in range(min(episode_num // 5000 + 3, 10)): # get average acc.\n",
    "            support_x, support_y, query_x, query_y = db.get_batch('test')\n",
    "            support_x = Variable( torch.from_numpy(support_x).float().transpose(2, 4).transpose(3, 4).repeat(1, 1, 3, 1, 1)).cuda()\n",
    "            query_x = Variable( torch.from_numpy(query_x).float().transpose(2, 4).transpose(3, 4).repeat(1, 1, 3, 1, 1)).cuda()\n",
    "            support_y = Variable(torch.from_numpy(support_y).long()).cuda()\n",
    "            query_y = Variable(torch.from_numpy(query_y).long()).cuda()\n",
    "            \n",
    "            # get accuracy\n",
    "            test_acc = meta.pred(support_x, support_y, query_x, query_y)\n",
    "            test_accs.append(test_acc)\n",
    "            \n",
    "        test_acc = np.array(test_accs).mean()\n",
    "        print('episode:', episode_num, '\\tfinetune acc:%.6f' % train_acc, '\\t\\ttest acc:%.6f' % test_acc)\n",
    "        tb.add_scalar('test-acc', test_acc)\n",
    "        tb.add_scalar('finetune-acc', train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('omniglot_train.txt','r') as f:\n",
    "    data = f.read().splitlines()\n",
    "    data = [line.split('\\t') for line in data]\n",
    "episode = [int(line[0].split(':')[1]) for line in data]\n",
    "train_acc = [float(line[1].split(':')[1]) for line in data]\n",
    "test_acc = [float(line[3].split(':')[1]) for line in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f715a3d3310>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOydd5hdVb3+P2uf3qe3TNokkx4CJJQQukSKIAqCHRGvXfF3LdcuylWxe69e61X0ghSlg/QWIKGkkV4nkzK9z5xe9/r9sfbe58xkMpmYiSF43ueZ58zZde199n7Xd73fsoSUkiKKKKKIIt680I53A4oooogiiji2KBJ9EUUUUcSbHEWiL6KIIop4k6NI9EUUUUQRb3IUib6IIooo4k0O+/FuwEhUVFTIadOmHe9mFFFEEUWcUFi3bl2vlLJytHVvOKKfNm0aa9euPd7NKKKIIoo4oSCE2H+odUXppogiiijiTY4i0RdRRBFFvMlxWKIXQtwqhOgWQmw5xHohhPiFEKJJCLFJCHFqwboPCSF2G38fmsiGF1FEEUUUMT6Mx6L/M3DJGOsvBRqNv48BvwEQQpQBNwFnAKcDNwkhSo+msUUUUUQRRRw5Dkv0UsoXgf4xNrkSuE0qvAqUCCFqgYuBp6WU/VLKAeBpxu4wiiiiiCKKOAaYCI1+EtBS8L3VWHao5QdBCPExIcRaIcTanp6eCWhSEUUUUUQRJiaC6MUoy+QYyw9eKOXvpZRLpJRLKitHDQMtoogiiijiH8REEH0rMLngez3QPsbyIooo4p+E3miKxzZ3jGvblbt7ae6JTsh5X97TS1P3xBzrjYYXd/Wwrzd2vJtxRJgIon8YuM6IvjkTGJJSdgBPAm8VQpQaTti3GsuKKKKIfxL+uqaFT92xnqFE5rDbfuCPr3HhT1+YkPN+5b7N/PK53RNyrDcaPn3nev73pebj3YwjwmEzY4UQdwHnAxVCiFZUJI0DQEr5W+Ax4DKgCYgDHzbW9Qsh/hNYYxzqZinlWE7dIoooYoLRE0kB0BdNEfI4/mnnjadzDMQP37mcaAgnM0SSWaKp7PFuyhHhsEQvpXzvYdZL4NOHWHcrcOs/1rQi3syQUrKlLczC+tDxbspxQ9tgArddo9zvGtf2m1uHWDApiBCjub9GR38sbX02jOH+KpxpLpPTcdiObrCfyuYOO4rI6ZLtHWEWTDr4GWjqjjCpxIvHaQPgQF+cbR1h5tcFmVzmBSCeztI+mGRauZddXVHm1QWPqs2Hw66uCIl0DoBYKjfmttmczo7OyKjXdjxQzIwt4rhgVVMfV/zPSnZ0ho93U44blv3gORZ/95lxbbu9I8wV/7OSFbuOLCrNJPo+4/NQSOd06//9fUevP6eyOuHDEP1TWzu5/JcraRtMDFuezOR42y9Wctsr+wDVCV3/p9V84i/r+Nzdr1vb/WnVPq745UruW9/K5b98yRq9HAtEkhku/8VKbnl8OwCxw1j0X7p3E5f/ciXdkeQxa9ORoEj0RRwX7DXI5Fi+nG8mmOS7/widgL1RU7oZm+iT6TzR7+46OieqlJJ0Vj+sRd86kDDaNvwZ6AonSWV1qwPY3hGhuTdG0G1nR2cEXZfW/olMjtf29qNLGIiPfY1Hg6buKOmczuq9Sn2Op8cm+gdebwOgO/zGeL6LRH+CIJnJMdETuaezuvXSHArmSzvR6DBe4mjyxNI6jzWklCQzB8sCHUPKMuwIH5mFmJduFOGYxy78lFISz+R/h91jRMtIKekYStDSH7c6kZHtNUcHQ4nMmM+sOcoY+Qy0DyaHrX9scweagI+e00A8naN9KDHsml4/MAhAZMRxxvPO6Pro93skmjt6AIn5uiRSacjlz5fTJRnjug/0xa3l5v0n1ge/PQe6to16/GxOt/Y/FigS/QmAgViaU25+mpVNvRN2TCkl5/7oee5cfWDM7e5Z18pZP3iO7AQ/hJ0GcY18Of/V8YMndjDnm0+Qyg4nH4voB8dP9FJKy8rti6VZubuXOd98gkc2tnPSt5/ixV09nPH9Z3l4YzvxdP58YxH9wxvbWXrLc5zzo+dZ8t1neGVPHyd95yleLng2U4ZhkNMlsfQIEk2GoVvJHyZRh0c8A51hg8iNUchzO7o5fXoZZ84oV+0zRhwmie41RjmFDtL+WJo533yCP67cO+Y9+vqDm5nzzSfGNniGWrn68dN4j+15AJZpm3kq8k545iZrkx89uYN3/+4VAJ7Z3jWsHQC0robOTbDn2VFPcfYPn+fCn64Ys61HgyLRnwBoG1RD1Jb+xOE3HicSmRyd4SQH+uNjbtc6kKA3mprwKAOTuCInWPTCsUDhiOnvG1XM+8rdwzt1836ZHeR4EE5myeQUgfXH0qzep2SHnz61k3ROZ9WeXoYSGba2hy0nI+RHW6PhgdfbqAu5ufHCmQCsPzBAOquztT3va0ll8tdzkHyz6r/g9+dDKmrJSSOfLfNaCx3Jk0u9NFb5WSCayW55EKQ8SI6KJPPnMju436zYc/BF7HkOnv4W7Hmee9e1ArB2/8Ahr5ntfwfgAm0DdrL82vHfavmufLT41rYw+w1LfkdnGIdNOcwt30jPDvXZvQNevwMiRmew8wno3k5nODmh7/dIFIn+BIDp1BrPEHO8MIfLh9MazeHkRFvenYYEUZRuhmvLiyarKI1HRyQ5dRpyRUd4/GRQqH33RdOUGOGV+wxCMi3jjqEkCePZqgq4LKIdiaF4hlVNvVyxqI5LF9YCeR9L4T6Fjt2hkSGWPTshm4QDr1gkmAl3wwOfhHi/ca2mdKOOHUtn8bnslHid/NT1R5Zv+RI88ZWDHMyFz1I6leSH9t9TFh8R767r8OCnYNV/w53XctVkdQ/GTCrb/ggAwhXgJNFMSMQJSy+k8p1bx1CCmPEu7eqKsnhqKTZN5H+Dnp3qc9cT8NCnYMNfQEp44GPw/Pfy13CMDJ8i0QMt/XEu+a8XLc3xjQbTKkpNoFZuWtLxkUPrEcgY55yIB/CFXT286zcvk8nptJsaferoYq2buiNc+t8vMXgYR9y3H97Kr1c0DVv26KYOrrt19bjPdeX/rORXzzcdfsNxoFAqKLRMTWv4gdfbuPAnzxONKP250KIfKTN8/q8b+PWKJr5y3yZueWQTZNVzbFrELrtGXyxNLJWllj6ecX6R08QOdndHjGMmLIu+odJHVzhJzjjHK3fdwtqfXAk7H0f+7hwW6Lu4bGEtPqeKzO4x3pnIQCf07wUpSRUYJAdZ9IPGJEjNK6z2TW55CDbeCXtfBPIa/UA8g65L4ukcQUcWenYxm72kcSA333PQsbd1hLn45y/SFU5ib32Vd9tX8EX739TKdJxP3vYa133nlxDpgItvAaef6wZ+DcCTWztH/6Fifcj9qwCY7Y1wtm0LuhT8NXc+RLsgHTf8FkmSGZ1sTqepO8rs6gBlPmdeujHkKuLGSG2oDRIDkBxCdmyyTnessomLRI+Kj93RGXnDpjUPHUOL/nDHNC36iSD6l5t6Wbt/gB0dEavTOtrjvn5gkO0d+WHzofDcjm5e2dM3bNlre/t4cVfPsCH/oSClZGPrED9+cueRNzKThNveAe0brEWFVm9/gWUaSWWpDbm5ZnE9U/tX4fn5LPShDrrCSQIuO5mcZKjpFYvMs53bWLl5F398aS93r2lh6ms3wZ3XAnnZYGaVn/5YiqFEhkXaHmZq7dzjupmeftWJtA8mrQ5/eoWfrC7pi6aQUhLYdT8nRV4iu+0RSoa28xfn95lXLvC6VHx7TyRFkBjfa74WfnEyrPnDMINkKJGBcDv0NysLduBgop/ebYSYDuwD8hp9Tpd0R1LkdMm1O/4dfnMWAE+LsyAxyMjSWauaetnZFWFT6xCelpcAWK6tI9q5G75fy5VN3+CC3CrSOODUD8KCq5mW3qVOPTSEfmCUTr9vNwJJRHoozfZwXfU+On2z2aw3qPWD+wkns9b9a+6NEU1lmVkdoNznVL+BrkPvLnAG8scNt1udnhjcTxDFPbu6Ige3YQJQJHoga1gvpp75RsOxsOij47To08Y9mQiJpd2wSl9vyeuhRysJmffmcNcxlMgcFPts7jse3ftwxx8T4TZofh6aV1iLCnVsU6IAdZ/n14X4wVUncbZ7LzY9RXjfWjI5yclTSigjTOmdl8K9N6gdbruSz3OHReqztRZk11Z1XGOk0Fjlpz+WZjCRoVrk7/39zm9TTT9d4aR1b2ZU+gD1W23a302j3oxT5Mi0bgTAJ1I4ujZR8vIP+Hf7vQyFI9SJPpwYneXeF4b5HMKJjGrr3R9QFmwqDL5K6NoCqTC19DEpasxpNLAPpOTMgUeY6lKWbctAnHrRTf3QOgAOlJzOhvQkhMzhI8kiVwfX2FYAkhYjXLNzKEGwYxVNeh2akIRX3wXAJdpqLrGtYb1zMbgCUDIZn4wRIM437X9Bu3U5PPI51SGZGFQFeNfqs/AlO6gY2Ei4Zikt5hzcA/sY3LWKCzQV37+hRXWes6r8eYt+qAUycZh9acEz0Wp1bABzhQqKKFr0xxA5i+iPXXjTSPxtTQv3rG05/IYcmUW/qqmXnz51eKszkhyndGNq9OOwvH/57G5e3NXD/728j3vXtfLA6638aVU+6sHUmdcbji+7JoYRvZSSr96/mXf/7hVe2j2+xCDTf9EXS/H5v24YNS5f1yXhZOagazXv66E06UL0HybhaCT6oin+392vMxBLQ8qw0qL5aIzCqJpC6SaayhJw29E0welBdZ8G928G4JQppVQJRSTs+Ds/fmQ99ng3p2n537uSQUSsh98+t92KarnIto4faL+mt6+XWtFPStr5SOaLTBWd/MTxW7K6bsW0z6j0A2DbcDvuhz6KS6jfx923lX57lTrJy7/A/vLP+Jz9fmZG11IuhgCQoXpoWTNMuskOtMKBV6BnO3QboYUzlwMwTXRylqY6pZgWIN3bTLprO9/Qf8cnAkouaemPc5n2mtrv06+x+szfEEZ1Rhdp6/ib+Bo/dvyehbb9Vgcz0NdFoH8Lj+SWEpYe9m1aabWnVvTztFiqvoRUzcUF2l6utr1I0lsL6/5MdndBEtuQIuC1+mxsehr0DJGqxRyQxr0Y2M/UB67kT84fEyTKhpZBrtBe5uQ1X6LBOUh/LE1yi9L4wws+CNUL1V+4nRWvrrFOM0/bh584ezoHORY4bAmEfwUcD6L/j/uULnfNksmH2fLILPr3/0G9FJ9fPmvMVHlTrhi3dDMOy/v3LzWzfG419xvJIiY+vGw6kCfUV5uV021mlX+YdJPM6NxlhHvO3NLJOY2HL1lt3psnX9/D4t2/4OeZT/H9D1wwbJtIKouUWM6ykfuOx6I/Iv9NpJPYQ9/lqS0XcFJ9CTfUG1ZaxHD4rfoFMrTQ2rw/llYOvxU/4Ja4jSdcSjdu0JRuvGfrWmARy6e7WCvyDsAdr/wdnDBD6+Dr51exsk2nyogp11/8Ge/0bqLKXc/lW58BG+S6fFR4EkSpRJtxKd/f2c/3HLdyee5Vmnsn8VvHz1nUejnQyMJ13xh2SQLJZsdJnJd9xtLSAeqzB7CjJo6LTb8Y/4Y/KmvVQE37U+ofqcOOx9T/jRfBxjtpEJ3M1/YRly6ez8zjnK49JHa+TA0w39WDQOf0FR/k7fYNDJYsoKR8BtX9PQxJRfTX2F7AIXRy0sZ7fetoDZcxQBBn1yYEktVyDtfYVzM9m/erZIWDJ9In802AkikAfMFxH26R4RPiP/iO/A7as/9F5SzVGTHYQtZVSnO61jpGqmYJfewhZ/diK7DKP2Z/lNm7H2S58zHYDl+2P88l2Z/Ca79ljT6LnnQjl31yJbz4Y3juu/RFNtGrBQl6XZyZ285F3j3U9cVBfx60ibXBixY9hUT/BpdujkCjT2bG7hTGK93ko25G6Ni6Di/8SCWCoKzxeDp3yFR7XZd0GZE2neEk5T4n08p9wzqQQufaeKotQj4Ge25qE9fbn6K+67mDtzHlndToFr2ZgDMWCh2bh8Xme5jSdDtftt/F41s68hZ9pEvJAs/ejH/tr6zN+2JpeOXX0LWFZWykTvSAlHgj+wAoT+zlipoBFtxxCpfa1lr7fdCWtzw/2jDA/zunGrdQ1/QB+QiT4ju4lmfonH0d/5ddzrXyCU7Wt1FeO52ZVX7uzF1Ilyzlrba1dHd1coltDSXND1NlH91XtUNOJqoFVNRMzUK6KaNRa6PCsOjbqy8EwNOlZBYHWRZ03g/+anUAI3qFhguQCKaLDubb9rNDTmafrMGX7CDT/DIAU2nnPG0T9eH1PKafQfNp3wKgNuRmyLDoZ2gdyGAttoZzuTb9IK+7P8GX7XcRHFSjhG36VCrrplJL3jezt2QpHUmHcmgbFv0SsYOdej1dvtncln0rlV0r876EoRaSvjo6ZZn6XjoNZ6gaECT9U9RoxcBn7A+xPP4Yt9veAe++g0C2n+uz9+COtnBr9lIrAIGgmn/pNLGDFlnFzrp3crFtLcsyLzN96TsnnOShSPRAoUZ/bCz6e9a2DLMax8rW642muOO1/cOWjWbRP7Glk52dwx03hUlNhyNKk2ATh9Pos4ZGn8oST2f5w0vNqmPs3aXCwnY+ZrXNdJ4VQgjY3RXhTy/vG9aRzqzy43fb8xa9lIjVv6OO3jHb39Qd4f71eYvR3C4UVWF0NbEdB+1jbmNa9FvahnhmW5fVARxk0eu6It50nvDMDsznOvQguCucVCOSnDrutbYX2L6/naFBo2hrpENp1HoGd/traKjfqz+SgM5N5KaeDcDs2HqIdiEyMdKam0bRxgfKtiNkjrPtSv7I2Vws1QqyLFtXU2vPW/tBkWCtnMvN1f9N+q0/5PacslDLcj0QmkS5z4lEY5v7ZM7StlLSpzRmrXMTF/tUeeGbHJ/nlobbiWnKidicLmPQZUwSVzmXA7YpNIpWKkSYlLTT5DkJvOXMWv89ThJ7+Iz9QaqS+7iz+otk7T4YOkDcOwm8ZcQ9tUzXOpgvDrBNn8oBWYWdHJUtjwMQiB/gBtvj9GvlfCHzSXKTTlO/b8hD2LDoa0U/mr8aTvkAAsmq3Hw+aX+ES6IPEHXXMoQfEchb4j+p/Qmvzr8JXcKLu3tY2SFISRVyul5vpCeS4nn9ZAAee+xBJa8NthD31NIuVbIW9afhNQqtbbHNhQ7lYP9i5uN8Jv1Zrs5+jzsCH4Hp5yIRfND2NFnNxXP6KfnnLFgHwBSthwOyim8NXMZmfRrJ2tNg6WcO+XwdDYpETz7U7VgQfSyV5Uv3buL+1/PkdFC2YAEe3dTB1x/YYlm/MHoc/Tcf2sIfRtTE3l+Q/HRYojcINjFO6SaSzPLcjm6+++h2Xj+gwsJUo9SnFXVgTFxhWr42Ifi329byn39XpKQZalJjtR+/y54fKfTuonrVt7jJcRt2TRyyINbvX2zm83/baL005nVWJJQvYEbBMN1E/v6pzug3L+zhaw9sPrRG37EBnvzqsISYfovobYe8V/eua+Wr928m2q8kGo9Ic7JoYtteQ8qKdkFMdWS2dJi5Yj9Om0a4bSeko8TnvItuWcL08BroU9eRnHIBPpFicb/qUKfKdnQ0essW4zKs9yFfA7SuoYKhYe1Zm5tBvOpUqkMu9sg6FfsNEKyj3O9Ul1p+JhUizFXSGB3oGd4tnySHxiPJReTKZhLzKLLcmSwh6q1X21XOps0xhZmijTp7mD6C7BtMw/WPIoGv2u/iI46neJozuGlHPdmset6+HLmGnC7ptNdzrrYZPzG2ymn02GoAcOtxBgmgJfo517aZ+8RyMtgtcvW77GSd+SqVwl8JC9/F52f8neszX2ZIeimTg3T7ZgGghVTbdQRTT3kLrpAaXXzjwS1897EdtBkEvl420jmUZKecTBw33dtX8vTWThhqIeauo4cS4rVnwIKrrc7+1o6pVjs22Bfxd30p67LTCXoc4A4SDc7ELTI0uReQwpkvXxGsz++nz+D19jhXpW8m+f6HwXZs1PQi0ZO36LPHQLoxrchkAbn3j1Fgyty+UCoZzaJPpnMMjiDD3QWhWYcj+nwc/fgSpqKprOU07BhKHkT0ZtSG2cafXXsyX7p4NlldWnNKBonyvnJlLc6qDhAwLHopJexT4XAX29ZyQahTtb9tPfQZmY19e6BtPbuMJJ/Ht3RAOs78sNpvUmYfAHPEASKxGOx+WkV5jLgX8XSWoXiG7kjKGmF0jJRuEoYFHs8P+U2itxX6PXp2wf6Xra/m/Yn1t5MTipgWhyI0tRkx2pm4ijU3cKa2nSsW1VEZVTHW4dIFrNQXUNv/mpVgEzz3k2B3Yx9UnbqGzhABOpzK7yE95YTmXgCt67BHh8eC75W11IY8uOw2yv1uNugzjB9iEmU+VRq5t0o5Jpfb1tGPItAF6Y3sZir9GQflfhe5oJI4WvQK0gGla1M5h07XNHwixSKtmbBWQlNXFKrmsrf+7Sy1bcMvoyx/3xe4Zslkvq59jodLPsgjmdPZ3xdjQ6yUMqGe1236VDLVp7DKuYzncifzVNn7rGu4K6EseTNuHyBTQPT4lB/H5fGTwc7T+hIA9tlnIATYgoroNW8515zeoEgYlfHdMZSkTVYA8Lo+k6wuWTi5HM/UJZxm38MLG3dCJk7YVaM613c9CLMvxWd0Oq/o89ARxPDgq5hiNcms+5+oXgzAaywAOMiiB3jQoSJxMtgJeD0cKxSJHsjpipzSx8CiN6WRZAFJF4bTHWr7yCjadaFFnxqlOmBh1cHOcJL1B0ZP6169t9/qbJKZsQubFTpjTfmis4Do97S0EUtlD3J0lvmc+Jw2qunnKu0FThW7+Ij9CW6OfpsQUSXduOzoUo0G+rY8R8JZTko6uMq+Ul3b366Dpwyn4ONfRt7zISv87LHNHfDqr7g58X0aRDvTZStdsgSXyNK18Sm4412w4gfD7h+ocxV+dzs0OoaSZHM6r+7pgeYXjBhtrExNyDtjh3W2j34Z+ee3wZb7IN5PQ/sjLNM2kwt30e6ZTRaNZRVxBgYK5tvpUhE0urBxprady0+qZZFtLxnhYsDXwKrcAtzpflh7qyKx6efCBV9T+9oMcpYB9mrKKhShOph8OqQjsHeF+q2kG4C9eg21IfV/TcjNetmojhOso9ynLHpnaT33+N8PwAr72ZZ+/P20isUv9zlxVs9iSHrpI0i2rBEQULOAXreKJZ+mt5BylbOrO8Km1kE2BJQzXHcGoOF8yn1O7k+czB8damqLv61tZV1KtX9b+XI2yQZKSkvZvOyX3JD5D4YmvwWAffbpNOuKqL0FI6m9EY2cNDpcv4p+8bsUua4LKD/BumwDbrsNYRC96SconHxlKJGhWdaScJSwR9ZZ68Xk05nDPrp2K1lm0KWO4XZoRltUpxPGz0Y5k+1yKjOq8zHy5jn0KaoTfSQ2DygoLeH0sum0H3JO6uecPNXoqOwaNm388wwcKcZF9EKIS4QQO4UQTUKIr4yyfqoQ4lkhxCYhxAohRH3BupwQYoPx9/BENn6icCw1elMaKXSkjhWqZxK9Ka1IKS2Ho0kyUkrSuYPrfRdKN79ZsYdrfvvKQQlJrQNxrv3dKzyxtZMG0Y6HJMnsoeUbK44+laUvkmS+2Ev7UILevm4ANjTt5/7X2w6aiKHc78TrsvMlx9+4MfJzbnfewmKxEw3JLFc/82qD+N3qhXl2exf6vpU8k57HdjmFOXozjmSfij/u3a0cmO3rEYMHkKkIFX4na/cPoG95AIBl2hZ8IsUDuXMA8G38k2rEtodAH94hRlPZYd9n1wRJJRP87C8P8MqtX4bb3m75HYj3QecWyKat36yQ6OOtW9Ro5L6Pwm/P4QMd3+d2xw8IhJsYsJXTLSpodA7gJz9ikJ3KUbgyO4/Tte1U+uyc6etgN5MJpySr9Plqw64t0HC+cnIs/Szc8JQVh90nA7wWNcL7gpOgXlm97HiUrHDQ5VSSwl5ZS41J9EEPL+ZOQhd2qJpHfakHr9PG7OoAK+s/yvLUj7jDfz18+HHuOf8ZXtQXAarD9l/0Zd6VuRkQxGdfBR9bAaXT6AnMJWlo3NJXya6uKG//n1V89VWN7fpkcvOvBrtL+QMkNBud9G2v7ONBLiB6/TOsWHALEo3akJvLT6rF7dCY0TgPvBW8GsrHnRda9FedOsUKsTQtevNZSkw+j3ekbubh+DxFzKZGHziY6AF+lr2G55fdgTSoMORxwOTTsZHjPSh/Qbt3NgBuw5L3OPKdzidSn+MzqU9bYamF53Cd/G7envpP1qbUiKjLSP4C2FH1NlpkNUumKSfvRObIjIbDEr0Qwgb8CrgUmAe8Vwgxb8RmPwFuk1KeBNwM3FKwLiGlPNn4e/sEtXtCkTuG0o2pXRf+kGPVBo8bHYLpLI2mslb7TIvePNZIiz6SzFgW3M7OMDldHhQtE06o42roPOz8Bh+2PTFm5I1ZAiGSyjK5+3kedX0dW/dWooNKaw4Spy+aQu9r5gyxHS9qeFruc+Jz2q2IB59IscymdPo731VDSbKVgFu9EK+ufpVKMcTKzBw2y+lMSu5iHoZkM7BPZRAaMspM0cZZMyqYRgdatyLNizUVj/xM7hQ6ZSm1Xca8p5EOaF0z3KJPDbfoF08p5dP2h/iPvTdwo/1+45yGM7yvCX5/Hqz+nUX01qgqGaYs283ttqtg8hmQGOAu97vRhCSQG6CfED22aryJNnzkfQCZdhVW+5h+BiERJxDexTTRxY5sDS/v6aOTclIhQ2JpOF99ahpMOQNCyn7qkwEe6zDki+AkKGsATxkkBrD5ypjeMJM4HnoIUVeinoe6Ejfr5SxeuPp1KJ9BidfJhm+9lfNnV1Ib8rBb1iOdfiidSklVXoYo8ztx+UvJlavRQFnAC3XKYelye1mjKxJ0haoLEqUEV6a/i7z0x8YxXNYzBOqdWNpYg3/aadYzUBvyUF/qZcO33soF8+vh37eyofY9VjsKyfXH7zqJkjIj9NYg+oBhZc+oCrBBzqQrnFb7BEZY9N7hRD+EH1k2w5JjQh4HTD0LKWxcYlvDoK2CPk2dw21X2xRa3l2U0Uk5k8u8OI1ZuYLGNYV8braKmda2OV1aI0PzGVwytZR/BsZj0Z8ONEkpm6WUaeBu4MoR2/51EyAAACAASURBVMwDzPqbz4+y/g0Nk0iPqXRTYNGbEoh/lAgOS7oxXopCUjIJ/lBEHzXS54XAqpttWtomUVlRKkTxiySTRB/JyCBDkShb24cOyh7NSzcZaiOKpDyDu0lGlCwUFHFqOp5l8SMX81fXf/J1+x0IASVeJ16XjQoxxGv6HEDFYgM4Vv8GfnEq1cl9avl+ldDyqj6XJttMnLkYV9heVQ3QM7DjUas9jVobZzSUcaGRiZiSDpZq29CloNc/i9XSsEGC9SBs0PQMyegALgz9PBYhm8z7MpZMK6VeqNGJTRg3rd9wcndsAD0LzSusztnqsHtV6vwGOQM+9Aj8+xbuzF1kHbdHhuh31OAIt+ATCWIuZYHbB5qISRcv5JTFHGp9AW+igxZZbVVSTE1RIxOmnzfstzBllT4ZIoyPZ6s/DIveY1j9n1L3UnOgLbmBh8uuBwQ1IaX7mpZ9wJ+3PJ12DSGEZRx4DavZ/A5QYWj5jVVqP9OJC0pO2SSVfFPqHf4sZ4QDh0MRnikTFeIyoyiaaYmb53Q7bCr/w+G2Ogif04ZWQK6aJhCeEvXFlG6M40w3MnvTOR23wwaBGkCMKt1Y1+G0WfuHPA5whxCTFqMhWZ1tYDCRxaYJqyLlaKgLuS15KeSxW+0sNToWu9H+9sEEA7E0A/E0moCT6ksOecyJxHiIfhJQmMLZaiwrxEbgauP/dwIBIYQRj4RbCLFWCPGqEOIdR9XaY4Rj6YxNjGLRmxmLoz04iVQWJxnLojct8JDHUWDRq894OjdMboomswQ9Dsu6Udtk2dgyyOLvPk1Td9Qi+lKhhtElIkLFfe/k+V9/lrf9YiWfu3MtZPMjjsKom8lJRW6B+AEyMaVj12hh3tn8bYZCs3glN49l2hZKvU5smsDntFMhhmjSJ9HpnpG/yJbXAEn9oEruOlPbRocsY7+s5oBbRUtcbXsJy4275X4QNjLCyUnODhqrAswSrSScZeyQk7EJSbOsRXMH2OpQji+mLlUvebiNb225hAed3wSg7qWvcqvjR1ZT5tYGEWJEJM1IZ+yBVwnHlCxmTTBhlJ3dlpkENjvSU8rOuJ+IVMTaFPcSdtYgop2UESHqqICSqWgyR78M0kE5+/Rq/Dv+ikDiqm60Knpmz/p3uPqPUDIimc5w4vWj9ODd8z6r9HmAc78E1z0E774NGi+iqeE6Kvwuy5hoqFAEWB10MxI1BSRb+B2URQ+woC6E26FR6s2Tts9ptzqskprpw55nl9GJgJJ/TFT4nbjsGhfNU8RbFVDnmma0rxDmft7RQlrdBkEaFn1VQHUKc2ryjlqXwwZ2F7z7djj9Y0abbQdp4R6nzbpPVkfQcD4Aa7IzeWFXD+6C6xkNNSG3JS8VjhrMa2gwOqC9vTHO/uFz/HHlXoIehzUn7mid4URiPEQ/2tWNZMQvAucJIV4HzgPaANM0nCKlXAK8D/gvIcSMEfsihPiY0Rms7ek5sjkxJwLHMrwynjm0RT9ax3Je/1/Z5f4Q6agiG1Njr/A7rWOkRxaNMhBJZgm4HcMetFgqR3NvVMncgwlL1y9FWbVlRHEO7MYfU3LFxw98EX7UYO2ftoqPZZiZU3JKRboV3XBY1tOJSybZVv8entYXM03rYo5nCBKD+O05SonSS4iWktPA5oTS6dax6wbXccdHTuetvj0cCJwCCPq9DeRsiizD1QaJta2Fqrm02upZ4Oyg3O9kltZKh3Mq+6UijM1yOn6XnSbfqWqfSUuUFRduB2Cu1oKLNO7B3Zwidlu1WUq9Dqa5DlNIKh1lfm671YGmsjp0bycpHezOlKPrkmgqSzon6XBOA2Bf0k/UU4dA0qi1kdB8luTRZ0S3vKbPxT6g7unM2SdZp/NWTIaF7zq4HYZFf9XZJ/OH65Zw3dKpw9c3nA+TVKTHjRc1ct8nl1qr3jqvhkdvPNuaWLsQeYtekU6Z14nTpuG0a5ak8W/nNPD3z56DsyBhzOuysVrO5X/n/R/usz7BI589m7OMyUGcBZOLF5LY1y6by2OfO8ci1LNmlPPojWczt/bgib3N0YPZhmHwDCf6C2ZX8diN5zCzym9t7zGcp8y9AkLq3gkhDrLqPQ6bJSFZ6+ZchtScrNQX0jqQsAjZxMovX8Dab1yETRMIoTpQb6H8Y8AkelPD394RJpbOkcrq1narvnIhT39+xOhtgjEeom8FCk2LeqC9cAMpZbuU8iop5SnA141lQ+Y647MZWAGcMvIEUsrfSymXSCmXVFYePu19onEsnbHJUS369LDzFuKy8F8B0I14a1Njr/C7yGYz8POFODffZW0/jOhTWfwu+7AHLZ7Oh0VGknlHpBnaNlnrRsulCRJlTk2A0+RmFcFh6NQXZl7gRefnqM60WRX2poousnEl3diMpJ8BWwWv6XMBuDP6EfjhVBoefz+akPTIEJtmfBxueAIq51htE/tfZllZGFeyh54yRVB+r4f9b7uDL2Y+zvolectbnvx+tmXraMztodyttPomOZm9UsVfb9Gn43XaSAWn87WSH8HiDyl91ijwBXC+thF3qgenyDFHU4PUgNtBjTbIc7mTeW/668Q9+QQb1Uj1itzt/C4fc6m4+mQmh961jT2yDh2NVFbPh1aGlCbbI0MkfIpcKsUQceGBWoPopSK1V437BXDa4iXYDXngkNm3VXNg8plMOfktXDSv2pJaRkPQ7WBqed5K1jTB/LrQqNvWGvKOSVSaJqgJuSn3OS0r1uO0MbPKP2w/04KNlM4DzcacmiCVhmXtKtDUSwuIfkqZd5jjUohDt6vckI1GvU53CWh2y7LXNMG8OnVfzeff7RilgzDWFxrnXqeNgCHdmOGX1J3C4Oea2S6nksjkcNmHH6u+1EuF30VVwEWl34XDplkjj8L3r9yQn8wRS+HsXeZ2k0o8w0Y9xwLjIfo1QKMQYroQwgm8BxgWPSOEqBBCmMf6KnCrsbxUCOEytwGWAaNPmngccSxKIHQOJZn2lUd5bofSf5OjRN1k9YM7lpCuwhYzyRj0N7P04XOZJjqoCLgozfbC0AGcLfnY7WERJUlVEKvwQYsVlCWIpjLW9iWGdFMvVIdSQpRLFxSQ3LaHAJitNzFF6+EcTenz+5yNTBOd2NL5LEyAHq2CHeSdeNQuwt2upJkeGcLpL1PWplFfhGnnKGlkr3KcauXK0g95HDinL+Xe3Hl8+L58zZyuWe/lnvRSAtl+gmt/SUAk2JiqZq8RfrdZn47PaafM52RVZhY4PCrSItZtHeMS22p8GTVSOj/QRonXgU0TlOr9tMtyXtHnk3SWDbuuTip44dT/Jiy9zLG18WfHD/E++iloXc3ruiL1eDofemqvUVEz3ZQQD+ZHRjE8ZKqVzNEvlfRidoy4SwiVV3PWzAqCbsehJQJXAD7yJNQsGH39P4hyn1NZ7wUSSV2J2yLtQ8HsGAotbivipKCzctg0a/mREJq57ahJaoveCxd+c9RyASZZew5B9KVeB9MKOkG3YxTpBggG/AXbjE6VdSUeaktUR+kbxaI3RzPlPidBt31YCPRo/oJjhcOmYUkps0KIzwBPAjbgVinlViHEzcBaKeXDwPnALUIICbwIfNrYfS7wOyGEjupUfiClfMMR/bGw6FfsVATzhDGhQaFFb+rvmZxESjnqi60nwrD/ZbyJThaJPZT6l1qk7BzIVys0iTub00lkcpZFf4H2Ojfb/8wr8cetmHnToi/xOvjIvCBsyZ+vRMS4dH4luZeEckpufwSW3UipHAQBizQlMUTqz2Na8x/y+rmBLspwO3rZevHdBEJlTBl4DTpUadteGbKcXRbRz3+nSpIyJl3wl1YBKUIex7AX4IvBH/OTa09ld3+WFfrJRMsX4n9BxcevjdWwWczkL9V+1uyfzeUuRfRWQlpB+nvCUcJZcqs1Armmro8lS0+BbApPZpA5s2bBNrVdIQ7kSvlb9CRqZSnlWpxTbRthu7quV3Xl+I2nc1bnLU79AF/cFKVVVqn0fHcJJAeJ4ea1ZD1nA1lvBYShS6uEkqngVXLHd94+/7BTOx4LaJrg9x9cPMxi/8bb5h32fTA7hsIOwvztnCNGJeU+J0OJjGWljwemdDOqRT/lDPU3Cg5n0d90xXyyuuTq37xsHH90ordpgoDLTiSVPUi6yR9rnlXV2LwPwVGkm5BHJZ/tLZjzYiLnlzgcxpVvK6V8DHhsxLJvFfx/L3DvKPu9DCwcufyNBl1OPNF3hYcnRRX+qIXJRboEy4dVYOHLdAR6VFnbetGL2+ekXij/hWuwCYGORGPSuh9BzyRip6oaGSbR32i/m8laD6/3b6cvppx4mdgAH95xIxnHlcwJDY/YKRFRKrxxNDPyxEjBL5cDIGCe2E9COnHUnwrNUCHCZDUXdj3FkPQxlFNx8/OXGpEnu/MWfy8hK6GFRe9RDjIzosSIXikprwH2E/Q4hkUj/X1gMj+qW8yul/cBguwlP4E7LlankJOoqShhe83bkfsP4HPaqPA7iaSypLI5XGYxLUBMP5fqXWogmpUaNeFNTG6sULH6wPzZiuhjdiPcLVAHkXY6ZRltAwkG8VPJ8FGMaZEnMznLwV5aVsG64HLojeF3O6ByNrS8Rlj3sHJXkkflZ2iY/xZ4JaGs3rf/AjR1b6ZX+Jg+ilPyn4HzZ1cN+75g0uhySiHyRD+aRT+cGMt8Tg70xwl6xp/ib5LkaNFpY8FqwyGs8EWTVWfudmgkMzpepz0fdTMi/DLocRBJZa3QypEojJoZy6IPeRyU+ZzDiH5kXahjiWJmLHmn6D8i3azZ189X7990UKGyloHhltmwRJtUjuttT/Ah25PDO5dI3vUhUhE1kTAwWeuhpIDobdkEk0QvIKnedRexl36Nvu5PfM52H363naDHQauR2h0Y2GqRkG9gOw3JLfwg9T0orLkNOMmiDaj0/GZHIyT6+eHfN1Ju1E+ZpbXRLUsQhkMRIOJWFnO7LGMwnhnuNKucbf3bK0P5l9VXAad/VH2CRfSVVepYSj/NjxaSGVUrvak7QqnXQWjmGfBvz3FX8Ab6CTKrKmBJCF6n3Urt74+lLYs+ix339Lxj8iF9GY6+HarcrjFJs93IoIzZ1Yu7M6N8RR2yjLbBBEPST1k2X08+HmigB7VtPJ2j1xhFlPuc1BiRLX63Q8W4A0O6i6e2dZGcdw02Q6Zy2TXlQJ22jBMReadnnoiDo0g3oKzz0gLNfzxw2W0EXPZDWtOHQugw0s3I7Vx27WBn7MhjjaMNXpcdt0Mb1smZGr1J9JCPwz8WUX6HQrEePfkSCP+IRf/Czh7uWt3CzVcuGBZetnvETDFm2WApJbF0lvc4V5CR2nCHrFnXBRDpmFXvZKqtl1a7ZhE9QKNQ+nWIKKSiuF/8Nh+021jj+h5XLpqErbkaeqB0aDv98TMBcMcK6sR3552UFnpUzZUNuWk0sJuHV77OR12K6G3oiGAtMxrnIr2ViHgPvqrpsG8fnbKM9qHk8MqOwXpw+EhmMsRwW84uC+6QsmRjPWD3UFNRxmcvnMkl85Vz9dtXzCOczPKzp3exqyvC7q4ojdUBRRT1i1lVI6C7g8ZqvyUi+Vw262Xqi6apNbIho85KSsrziSu/zr2Tq0qaEC/+GM74OAD2UC02rZ2wpizZ9dEyZtuhQ5bTE0kxaPcRTCkZLlx1OjumvheMnyORyXGgL065z4nbYaPWSFIKuO1WpEwiHmMwkeHUqaUWCY60ek80LJgU4sPLprF0Rrm17FDSzQfPnMZ5s45clvrSJbOHhUyOB4eTbgq3CyeyaJrg8pNqsQlxUITPoUYoo+HqU+uZWTncYb1sZgXXnzWNRZNLqDCkqLoSN9csnsxFc6tHO8wxQZHogdxRSDdmiYNMTsdhhJRJKWkaMfejGfuezOjoEqrEIGlsw0oL633N1hDLn+mFhJqEo1700uuwUS56SZU24hrYzSzRipt8vLstE6NCQImWZF5dLYRy0ANV0e30GRNn++Lt6Ai6nFOoTe9H2j2IbEFBL2MC4zWpKVzlgDrRR5nId1hTpkwHmwa1C2HPczjLp8I+ZfV2DCaGObjQNKicxUBHGyAOJnohlDYd7QRvOUIIvvDW/Cjg+mXTGUpkFNF3R9jVFeGKRfliUOaQuLE6QJsxO5LPZbd03f5YmmRlFW4g7q6mpDwf1Rt11yJOeje88j8w522qOYFa3PZOhgyi3yBnUuVx8kxKhWsOELASvvbN+Qg7PWcCqrNMpHPs7o7QWK1ecjNcMeCyW2F9gYyKya8ocEaOJMMTDW6HjZuumD9s2WjOWICzGyv+oXNct3TaEe9zJBZ9n1O9G7OqA8wqqFdz0LHGYdEvnlrK4hGZriGPg2+/Xd0j0wgp87m48S2Nhz3eROLEftImCEcTdZO0iF7tG05m+Or9mw8qRWzOERpLZ3GQpYww5YTJ5gqmXRvMW9yzssqa73JMokb24LJBveghUTafpKucBtHBQm0vGWljgLzFU5Y2yiEblRurEs1UpffzGdsDlCf30UMpbX4VtSEKyA+wiH6bruKz52gHhq8PKGubGsPtYkxQ3SnL6Y6khhWeAmDxh3nSdQlwCJ3VlG+8o6eBhzwOaoJuXm7qI5zMWtmZkB8SN1b5rXhpn9NmdQB9sRRhLURWaqS8NRCaTA6NPhnA5/VCRaPKet37Itjd4KvA7bAxIBTR98og35KfoNWYMm5Q5juxmK3koEJpu7ujNFYpojCzUf1uOyy8hm3Bc/hVTiWLl/mclkwwrklMTjAciQV8zNrgNS36se9vyOMYl9UP4J6A38qUFY91ctRoePM9af8A8hr90Vn0AK/s6ePuNS1MK/cOc6ylc6pKZDyVs+qGO0SOXCxfYTIbHyAsvcRx0ShVHPsm5yk4yBLK9FAr+kn46oh4pzBN6+SCYDu7ZD2365fQUasq/oXiBjknBugSlQh0nnV9iS867uG0+Iu06hX0B40yAQ5FSAmXQbjd28l5ymkzJj6eL/YNv1iT6M/8lCqkdc4XGKo/nxd1lezjGxkdsfhDPFqiKhb6R1r0UED05QevM7CwPsSqPSraqNDiWjaznPNnVzKj0m9FZXiddqMsr8bGliGGkjnuzZ1L3+TlYHcScdfSr5XzlrlVlnbOnuehYhZoNtwOG3scs9jnW8RmvcHKVAVVE8VEzBYcRvT7+mJEklnLol82o5wzG8rU7+8K8OCcH1sdRrnfad2LQzkLT2QczhH6z4BZa+ZwJH7R3GouW1gz5jb5TuPoO65yy6IvEv1xwdHMGWta6tnYAGy4k85BpUPe+4ml/FvgVdzkPeuprE4snaVa5Mldj+QdfDI+wJD0kRA+aoWK994slJxR0bsGh8gR9Uxm0DOZaaKT2foeqD2Zn6XfwfPzvgtAqP1F2Pk4JAZY6zqDb2o3sseINbeh0yIriJUZRJ8MAwJPrRHPnehHC9YQd5SQlRrzNNXZWLHlfuOlCNTAvz0DFY30XnkHG6TSv72jDG+9TjtOmza6hec1iN5TdvA6A5cuqLHC12ZW58l28dQy/vzh03HaNWtY7XPZ8DhtnDerkse3dDAQz/CV7MeINSprumTxNTSefTVff9s8KDNGM9kEVKnrdzs0emWI71b9lG5KrecCIOXIj5oimiJ60yezsUVlCZsWfUOln7s/ttSy3AvvS5kvX5bgRNfoR4PljLUdP2oZr0b/ntOnqGdhHMc6UofwaCgriKn/Z6NI9OQ1+n/EC25KN/Z1f4AHP0ndrtt5h/1VysI7eH/HLVxuFucCXmnuZW9vjKoCohexvINVJgYZwkfaptLUpbCxUVeWZ0WrmmR50N/AoLueajGIlhwgUqZkmP1RQacsxbP1brjrPZAcJOUIcmf8dN6S/imv2pTW3CoryVTOVxmf534Jpp8Dsy/Lt6esgRlVQXooYZ5QRB8OqvozlkVfgMIohdGm2fO5bKNb8zAui/6iedU4bRolXgeV/tFjsE0t1rTs33ZSLV3hFM8buQxWG5d/B95iRAX7q8BpdBxGhJDHaSOZ0Q+ecQpIO5Skk5UaL7dm2NwWtuq0bGpVI7TGav9B+0F+pKMJKPE4LH+F8ziS4bGCWUvmuFr04yT6IzrWBEg3pv+osDDcPwtFZyz5hKl/pHqlKd2s3NXFO4Dl+3/GcjuwUZFF4cTEN/x5LdMrfCwTg/kDxLoh2gPhNkgOMiR9lNizkIWsp5zt6SpSmodgi5r0ut/bgO7MTyGYqlgASNoGEtQUdCBInYwzH+N7IHAqZw6up1VW0hgMwU3Gtiddoz6f/Kr6nHM5ZwXK6e4poVZTo4pIyRyqel8dlehLPA4qAy56IimrcFMhZlcHrcJsB8G06L2HtuiDbgdvO6mWZCZ3yNC86ZU+Ai675Qy+cI6SSZ7coqJkRs1AFALKpkPnZqg0LHq7jUQ6d/AcskDWVQIZGMTPPetVGOxlC2toG0zQGU4S8jioOERHZPouynxONE3kLfo3oXQjhGDhpNCwMgf/bEwt9xLyOJgxyvN4pLBGBxNg0U8u81Lucx6y5MOxRJHogdxRaPSWRZ8aHL5ijyLmWtGHTROWDHCgP847tQJCjvXAfR+B7m1o9hBDlGHzSkhCzFFGJKLTFZrNlOgGOmQZMeEl6lCRHFLYoHoBsJm2wQR/YznX8rR16KxTPVBOu0Z0yoWkB/7EVn0aH60a4yWcfSlfXRQiuscPfdAlS4hUngItD1j10Atht2ms/PIFpLK6pY0W4nMXjRFd4DMs+TEseoCfXbtozPjrGZV+Nn/nYut7wO2g3Odkb59KTjlkqnlZg0H0eYu+P5a2yhkUIucqgSgMGOULPnL2dL5+2Vzmbn+CVFanruTQ08CZFr1VjdFpQxNvTmcswIOfPr55ARV+FxtveuuEHCvvjJ2A0YHbwbpvLj/q4/wjeHM+aUeIkdLNrq4Ia/f1W8TfG00RT2cZjKcJj5jII2Fo9NW2CHv1amYn/0xauKzM0lrRT0kB0ZTog0wV3cSdFWSkDX/z46reS6wHR7ybIenD7VME3U8JiUyOvqCyOHfrk0hmdLqdRiW+qrn4A4p42gYS/ML1Cfjs+vx1GQWfGip8ZCvmsDD1RzbLBqaOUsEQZ0Blg7rVuZN1Kvb+4+nPMzDlUvjSblVrZRS47LZRSf6w8B5eugGOKMnGRG2J29L2g4eSjmpPVtUPS6cB6jr29ykfy8gS0tLwI5glghdMCqJpwtJuC2u4j4RVFdIgeiGUVe98E2r0bzaMV+9/o6No0TPcGdsXTXHxf71IUEb5yjvP4L1nTON9//sq5zZWsqs7SsBt51fvO9Xa15wi0J8bop8gKZwMuuupSqjkp1rRT4nXQV8sjYbOQ65vUi96SWhV9BGkpvM161iOTJghfNg8ihBa0sryDpcugHbYLeshkyOiu2mlivopZ1oPYnckxZyagCoDbPdANoF0K3KaVR3A73KQwkmJ14F9NG34C9tVNUADkbO/xlvWLGEIPw67Udd7olFmlCwumTr2dv8AaoIetrSF8bvso18vwFmfhSUfBs3I8HTarLLQMyr97OiM4HXaiKdzODwB0OwM5tRvYjpevQ4bg2TGJHrTd1FY56WuxHNIn0MRbxzUhtzYNGHVuz9RUbToGa7RD8QzuGWSla7PUb33QQA6hpLqbzBB1wj91pRuvNlB+o3ys4lAnrhqRZ81WcNiscsqTBYPzqRXGlpdw/nW9mHpQzMs550xJQfEqk5BItiqTyWcyJDK6nzU/n1YfvMwWWJ6hU8lKhkEmrCr9syo9FsO0ZpRJp4AlLXuyMsPPo/HCikca2ado0LNQrjxdZh82oQf2iTeMSsE2hzgycfwFzrc5tSo32CSIckE3A4ITqJdqtGHqUGbo40jsegBbv/IGXzhrbPGfT1FHB9UB92s+OL5lt/nREWR6MlPPJLNSZKZHOUiTEAkKIvsQEpJPJ0jksoSTWUt56sJ83sgN0CvQfS5EkW0EkFIxKl2q20us71GUjq4LH0Lzef8lA5ZTsZVBhd83TreED5sbkUyXTl1PFHWgPj4CzzrOI9wUhF91FkOTt8wIms048yNGPH9cWWFTCr1YDMIaSwtuRCFs/o4jqWWbMazTzDMWZKCYxH9CBQOz2cbafeTSo3kJ5cdrnuIn2fVRGqmZGNO5GImSY0G0/laGG1RGXCNGqVUxBsPk8u8w6YyPBHxL0v0j2xs56xbniWZyVl14TM5nWQmR8iYYMOf6CCV1a1JtiPJrDU1oAlVw0YS1MOWfuuqVg7IbKWK0Z3pVuF3y23reEFfRItjBiJQw7cz17Hhojv50/b8zzAkfdi9Znam+gx5HFC7CL/HzVAiQzqrWzHYhWn0VuZoRSNodtyBUmu5aZWfMnl8c1R6C0jvRAwDNCfFDh1BtURPQfVB00KfVOJRNet9TiibTpjhjmxzbt+xLHqzszHDMYso4p+Nf1mTYkPLIO1DSZp7YsM0+kQmR0goog8k263JssOJDFFjBicTUkouyr3El113YydHWAvx2w+cSp0vCC+BY/rZ0LOVK6ZJ/rAxSb3o5Q79IrweFWvcRiXRwAye29jM+6UNp8gh3SGlBwNXnHUyZ1Yt5LTpSmsPehyWBTka+Vpx3Es/Aw0X8KX6BZwzp5ZFk0uQUvLbD5zK8nljZwKa0DSR16dPQKKvCSoL+0gmdzClm9qQ29qv3O/i9htOt6a6e/gzy0Y95lhEXx10c+v1SzizYWyncxFFHCv8yxK9GSu9uztSMPGIJJnRLYs+mOqg17Dgu4368oXSTSqrs0zbYunuMVsplyyoBb0a3v5LmHwGrP4dFQPrmSpUhMleWYPPabfIM5PT2dUdpZtS6umlpKwKjAiWi05bCNX5WZtCHjtDiQxuh23UGGyr5IKvAhrOwwtcOEdVyBNCqLYdAbxOu0H0J96wdVwa/QiYsdI1IbdlhQdcds6amS/IVVh/vBA1YxA95H+HIoo4HviXJfqO/O0vwAAAIABJREFUIVXxcHdXdNjk4MlMjqBQIXaeXIRERCU8mUN0U7q5f30rz+/s4f1avoRBxKhljqbBqdeBlDD7MkpW/4wb7OcCsF9W43XZsBvkORBP0xVO0eMsoV70UllVDXOWwqU/slLzTYQ8Dvb2xhBi9HlFJzql3uey0Rs9Mass1vwjRG/Ph0qa+x0yq3cExpq/tYgijjfG9QYLIS4RQuwUQjQJIb4yyvqpQohnhRCbhBArhBD1Bes+JITYbfx9aCIbfzQw09x3dSmLPkiUX9l+ih7pVjXeDWT78xUcP2u7n/fyONmczoMb2nlkYztTRZ7oo7YR1p4QcM2f0X2VXKW9BMA+qSZ1thtzXW7vUOWMu6TS02tralUs+xkfhxHx4yGPgyEj6qaQ1P/vhtP53+uWHO0tOQgmeZ2IGr3bYeM/r5zPtUsmH35jA/mYeA8NFT6+sHwWy+eNbYnf/6mz+Nm1i46qrUUUcaxxWDNECGEDfgUsB1qBNUKIh0fM/foT4DYp5f8JIS4EbgE+KIQoA24ClgASWGfsO8BxRDanW9N4NXVHcdo1Fmp7udi2lqd6NlgaPYAc2A+EmCHa+H/2+9gvq0lkcuzuiuAmZRUfA4jbR0lttruQU5Zh2/4gXbKEpHDjc9qwG178bR1qerohezlImFp3aHnFJPpSrxNnQVzvebMqj+Z2HBLmJAwnokYP8MEjrGVulrWtCbnRNMFnx1Ez/NQppZw6ZfQyy0UU8UbBeN7g04EmKWWzlDIN3A1cOWKbecCzxv/PF6y/GHhaStlvkPvTwCVH3+yjQ280zf9v7/6j6yrrfI+/v+dHcvKrSdqkpW1aG6AgoNhipqjID2UopRcpjC5uYVyD96p1ruJSFMd2VAS83stc73W4rIUwDNOrVweRKaN0tCzQEcfrANJUq9BCaSlo0/IjTemPJE2Tc873/rH3SXbS/DhJ0ybd+bzWyurZz372ybN3T7558t3Pfp5c3qmrLOWVtg46u3NMI0jX5A4fpJoOegiCXOJA0KP/ZOoRkuacmniNvXtbefVAV29v/u7sVTyQfT8HU4MHXGu8EIBX/BRqy4O1VQupm+dfPUgmneC1ecu5J/sBzpg99KiY6rI0XT15DnVlT8jj84Uhlsd1eOUkUpgcbbgbqyIno2J+gucCuyLbLWFZ1O+AD4avrwGqzGxGkcdiZqvMrNnMmltbWwfuHnd7wvz8efNryDu8cairNy+f7zpItXXwms1kf6qOaa9vBOD8xAu0huPkdz//a86zF7k9/W0AHs0t4a+zHyOVHvwPpEQY6Oef/jb+/Pz5LDvnlN5e8qGuLHNqylj03uX8YfFfDTtXdSFv3Np+5IRMcVtZWujRn3w3Y8di0bxa/vSsWUPecBU5WRUT6Af7KR84n+/NwMVm9lvgYmA3kC3yWNz9Pndvcvem+vrjk4aIKoy4WRCOUunqyTMtHGnjR9qpsU7arYrfVlzIrDf+HxUcpp4D/DIf5GI7/rCJL6Yf5PxEsHj3HzwYsjjkHNx1Z8Bbr2T2kg/y+aVn8oF3zOldIBiCB2ouPqOeOz547rDtLowE6c7mT8jMh4UcfToxNXr0p1RnuP+GplHdwBU5GRTzE9wCRO9oNQB7ohXcfY+7/5m7Lwa+FJYdKObYE2Xba4fY9Ifg1sCrB7qYzkEu7P4VGY5wdeJXvXl5626nxjroSFSysfwiUvkjXJV8klLr4YX8PPb4dMr2PkspwXj2J3Nnc4hgkrB0aoierxms/Ed4a9+879HgOdiCHYOJBqATcYO0cC/hZH8qUGSqKyZabAQWmlmjmZUAK4H10QpmVmdmhfdaA6wNXz8GLDWzWjOrBZaGZSfc/3p8G1/64bNAkKp5pPQrXLj5Zv4s+SvuLPkWf5J4EYBkTzvV1kFnopLn02fTlaziykSweEir17Alv4A5h3cwL9HG97Pv4/qeL/d+j9EE31QkHXLUEnxDiAb6+hMwydLbG2p0o1EkBkaMMO6eNbMbCQJ0Eljr7lvM7Hag2d3XA5cA/93MHPgl8Knw2H1m9jWCXxYAt7v7vqO+yQnQ0Z2lozsL7a1U793MPAvuBbzFgsUpTrfgD41kTzvT6KAzWUVP3thXOpdzs8FCH3upZrs3cAm/I02OV61/mmk0o1Oigb68yDlPooF+pGF/4+FD72zgQ+88eg56ETm5FBVh3H0DsGFA2S2R1+uAdUMcu5a+Hv6E6erJBw87/ettfPKl7/aWN4QBf4aFC3ZnO6iig85EFT25PPtSs5hjQS5+r1fzYr6BdCp4aGp/yWyITE8/mgeLUpHUTcUYUjcLh1s8REQkYso8zne4OxcE+iMH+5UXpi8oqM21kSRPVyoI9G8k+3rO7alatmf7pkDoKJ8LHUEK3n10qZtkwnqPK/apykKgX7FozpgW4xCRqWlqDKcAurI5OntyeD7HH5Lz+cbMvwH6evQFp3iwoHRXsops3nktTM/k3EhX1fGSzyHvQZDtqQhGihYmOhvtVAGFh6YKwxhHrJ9M8JuvXMY3r100qu8jIlPb1An03TncIX/kEIc8Q1dlkHueYYf61ZsVLrB9KF1HT87ZTbDgwH6bxoyqcrooZZfXk7MUNu0UzPoC/WifIC1MplZsjh6CxSuSGgUjIqMwdQJ9Nphz3rvaOZjPkK4YfjRJR8kMsrk8f8wHMxceTNYyp6YMM3je38LhyvmcUhMs/FEI8KPt0RfWNC02Ry8iMhZTJ9CH0wv7kXYO5EoprRz+6ceOknp6cnleyQZzwc+aPY81y99KeTrJrT1/wc7338tfXnwa3/vo+b1Pjo51ThjNfCgix9OUCPTu3juPfP5IOx2UUVVehpdU9KvX4cHY9BxJukpq6ck5e3tK6UhWUz59LrOryygrSfIaMyifew61FSW8bW71mHv0BRVF5uhFRMZiSnQlu3N5Pp74F2bbPqy7nQ4vDUawlFZDd99Mla9bHaeym/b0dNKpFNl8MCRz3Wm3csNFwXzyhalsZ0TmpOkN9GOcE0Y9ehE5nqZEj76rO8/7Er/j0sRvSGY76KAsCPRlQfrmoAfLzu1LBUMpO0vqSCWtd8WpV+veA3WnA8EMh8mE9RvTXkjdqEcvIpPR1Aj02RzT7SCzbD9Jz9LuwQpClgnmj3/ZgzngOzJhoC+tJ5VIcKQnR3cu3zt9LUBZSYra8pJ+87+kwh69cvQiMhlNiQhzuDsI9KUWPMbaQRnV5elgJSdgQ+58UvOXsMtPh/ZH6SqtJ500OsJlAzORmSLL0ol+aRvoe1BqrD36ylEMrxQRGa0pEWG6enqYR994+U7CHH0Y6Hd5Pc1nf5TEK/8GQEdpfb/eeVlk+ON//JN5dHb3PR0LjMOoG6VuROT4mRKBvvtQG0nrmwa/3cv6BfqDVJBIGKXlwcIie62mNx0DfYtGA1yz+OhJvgp1x7rqU4V69CJyHE2JHH2+vf98Nl2JsiDvXgj0Xk4qYfiMhfw6/1a2pN5OOpKDz4zQ4y45xhz9iVgWUESmrikRYfId/eezSZRWBZOC9fboy0kmjEvecTof8dtYetF7+62TmhkhEPeOuhljoNcEZSJyPE2JQE9H/x59RVUQ4Jn9DnYn5vC615I0Y9a0DM9/bRnvmFfTO+EYQCY9fI++d9SNeuYiMglNiciU6Owf6KumhfPcNF7ETTPXcphMv4VAgCFvxg6m74GpKXE5ReQkU1RkMrNlZrbNzHaY2epB9s83syfM7Ldm9nszWx6WLzCzw2a2Ofy6d7xPoBiJzjYAujx4yKm6pm9Cs8Ii2wNnhIwG/ujN2MGU9D4wNboUzGn1FSNXEhE5RiMO9zCzJHA3cBnBYt8bzWy9u2+NVPsy8JC732NmZxOsRrUg3PeSu0/oBOqprn0c8HIOUsE8Wqmtnd67r9ALT9pwPfrhfx+menv0oxsm+dhnL8JHriYickyK6dEvAXa4+0537wYeBFYMqOPAtPB1NbBn/Jp47NJH9tHm03jTK+nxJLNqp/XuG6pHn4706EtH6NGne3P0o+vRp5KJMY/UEREpVjFRZi6wK7LdEpZF3Qp82MxaCHrzn47sawxTOv9mZhceS2PHqvLwHtqo5k2vooMMs2vKevcVgvjAHH10TdeRcvQlxzjqRkTkeComMg3WTR2YcbgO+La7NwDLge+aWQJ4FZjv7ouBzwEPmNm0AcdiZqvMrNnMmltbWwfuPjaHXuOU9i08zbm87rXs8ypOmZbp3V0Yw54YJnVT9KgbBXoRmYSKeSSzBZgX2W7g6NTMR4FlAO7+lJllgDp3fwM4EpZvMrOXgDOA5ujB7n4fcB9AU1PT+Katt67HcJ5Ivodd3SXUWjs/jgT6wvw00R489E/djDyO/tiejBUROZ6KiUwbgYVm1mhmJcBKYP2AOn8ELgUws7OADNBqZvXhzVzM7FRgIbBzvBo/ko99ZyN/fOqfeK10Aa+WvIVWannR5/WbfKwQnI8edVPopVu/6RAGU5j0bKRcvojIRBixR+/uWTO7EXgMSAJr3X2Lmd0ONLv7euDzwN+b2U0EaZ2PuLub2UXA7WaWBXLAX7r7vuN2NhEv7+3gZ8+/wW0Vu9ldtpAykvzscxfz+sGufvWGytEXpkAYaWglwIpFc6mvKg1mxBQRmWSKmk3L3TcQ3GSNlt0Seb0VuGCQ4x4GHj7GNo7JhmdfBaA02067lVOaSnL6zEpOn1nZr17JUDn6sHykeW4ApleUcOW5c8aj2SIi4y62SeVHnwsCfSWdvNZV0m9O+ajS3hz9wFE3YY9+iONERE4WsY1iLW8e5pyZGTLWw67ONKfXVw5ab6gcfeEGa9kII25ERCa72Ab6bM5ZPCsI0ocoZ/m5swetVxoG8qEC/UhDK0VEJrvYBvqeXJ6G8m4AsulKLjitbtB6hYecjkrdJAupGwV6ETm5xXZpo55cnor8EQAuOOfUIddzfce8Gs5vnM6cyNOyAOmEevQiEg+xDPT5vJN3KPdOAJY3nTlk3dNnVvKDT7z7qPLCvDVluhkrIie5WAb6nnyeRbaDuq6OoCBz1KwLI0qpRy8iMRHPQJ9zflR6C+wIC0pHH+gLUyAU88CUiMhkFsu8RDaX718Qrg07GoVpD0aauVJEZLKLZaDv6T7Sv6C0atTvUejRlypHLyInuVhGsezhg30b6XJIjn4OmnQiQVUmxcyqzMiVRUQmsVjm6PNd7X0bY8jPAyQSxmOfvYjpFSXj1CoRkYkRz0B/5FDfxhhG3BQMHFsvInIyimXqJtcVCfRj7NGLiMRFLAM9R6Kpm9HfiBURiZNYBvp8NNCPYWiliEicxDJHX+jR73znlzn1vEsnuDEiIhOrqB69mS0zs21mtsPMVg+yf76ZPWFmvzWz35vZ8si+NeFx28zs8vFs/JC6gxz9vtNWwNzzTsi3FBGZrEbs0YeLe98NXAa0ABvNbH24fGDBl4GH3P0eMzubYNnBBeHrlcA5wBzgZ2Z2hrvnxvtE+ukO5rhJKD8vIlJUj34JsMPdd7p7N/AgsGJAHQcKw1uqgT3h6xXAg+5+xN1fJph9ZsmxN3t41t1O1hOkSvSwk4hIMYF+LrArst0SlkXdCnzYzFoIevOfHsWxmNkqM2s2s+bW1tYimz60RE87HWRIJTVPjYhIMYHeBinzAdvXAd929wZgOfBdM0sUeSzufp+7N7l7U319fRFNGqHB3Z20U9Y7X42IyFRWzKibFmBeZLuBvtRMwUeBZQDu/pSZZYC6Io8dd8lsO52e6V33VURkKismEm4EFppZo5mVENxcXT+gzh+BSwHM7CwgA7SG9VaaWamZNQILgWfGq/FDSfR0hKkb9ehFREbs0bt71sxuBB4DksBad99iZrcDze6+Hvg88PdmdhNBauYj7u7AFjN7CNgKZIFPHfcRN0Cqp4N2zzBbPXoRkeIemHL3DQQ3WaNlt0RebwUuGOLYrwNfP4Y2jloy20knVUrdiIgQ0ykQUtkO2ilT6kZEhJgG+nSugw7PkE7E8vREREYlfpHQnXS2kw4yGl4pIkIcA313BynvZp9XkUwo0IuIxC/QdwRP1h6waswU6EVE4hfoO9uAINCLiEgcA33HXgAOJhXoRUQgjoG+Mwz0iZoJboiIyOQQv0Af5ugPqUcvIgLEMtDvpdtKyaXKJ7olIiKTQvwCfWcbh5I1mv5ARCQUv2jYsZdDyRpSGkMvIgLEMtC3cihRrR69iEgoftGws40DiWpNfyAiEopfoO/Yy36rJqUevYgIELdAnz0C2cMcskr16EVEQkUFejNbZmbbzGyHma0eZP/fmtnm8OtFM9sf2ZeL7Bu4BOH4ymcBOOJJ5ehFREIjrjBlZkngbuAygsW+N5rZ+nBVKQDc/aZI/U8DiyNvcdjdF41fk4eR6wGgJ5/QqBsRkVAx3d4lwA533+nu3cCDwIph6l8HfH88Gjdq+WA52m5PKEcvIhIqJhrOBXZFtlvCsqOY2VuARuDnkeKMmTWb2dNmdvWYW1qMMHXTnTdKFOhFRIDiFgcfLAfiQ9RdCaxz91ykbL677zGzU4Gfm9mz7v5Sv29gtgpYBTB//vwimjSEMND3eFLrxYqIhIrp9rYA8yLbDcCeIequZEDaxt33hP/uBH5B//x9oc597t7k7k319fVFNGkIhR69J0hpvVgREaC4QL8RWGhmjWZWQhDMjxo9Y2ZnArXAU5GyWjMrDV/XARcAWwceO24Ko27yCUpS6tGLiEARqRt3z5rZjcBjQBJY6+5bzOx2oNndC0H/OuBBd4+mdc4C/s7M8gS/VO6IjtYZd4WbsXmjRD16ERGguBw97r4B2DCg7JYB27cOctyTwNuPoX2j03szNkG5cvQiIkDcnoyNpG70wJSISCBe0TAfPDB1OGeUpZMT3BgRkckhZoE+yNHnSFCVKSorJSISezEL9EHqJkuSylIFehERiGmgz3mSSvXoRUSAmAb6LAn16EVEQjEL9EGOPktSOXoRkVDMAn1fjr4qk57gxoiITA6xDPQ53YwVEekVy0CfJaGbsSIioXgF+lxfj76iRIFeRATiFujDHn1JOk1SSwmKiAAxDfSlpSUT3BARkckjloG+rLR0ghsiIjJ5xDLQZxToRUR6xSzQBw9MlSl1IyLSK2aBPuzRZzIT3BARkcmjqEBvZsvMbJuZ7TCz1YPs/1sz2xx+vWhm+yP7bjCz7eHXDePZ+KMoRy8icpQRB5ubWRK4G7gMaAE2mtn66Nqv7n5TpP6ngcXh6+nAV4EmwIFN4bFvjutZFISBvrxMgV5EpKCYHv0SYIe773T3buBBYMUw9a8Dvh++vhz4qbvvC4P7T4Flx9Lg4eRzwQpTFRkFehGRgmIC/VxgV2S7JSw7ipm9BWgEfj6aY81slZk1m1lza2trMe0eVD7bQ86NshJNaCYiUlBMoB/sEVMfou5KYJ2750ZzrLvf5+5N7t5UX19fRJMGl8tlyZIkndRTsSIiBcUE+hZgXmS7AdgzRN2V9KVtRnvsMcvnsuRIUpKK12AiEZFjUUxE3AgsNLNGMyshCObrB1YyszOBWuCpSPFjwFIzqzWzWmBpWHZc5HM9ZEmQSijQi4gUjDjqxt2zZnYjQYBOAmvdfYuZ3Q40u3sh6F8HPOjuHjl2n5l9jeCXBcDt7r5vfE+hTz4b9OiVuhER6VPUXL7uvgHYMKDslgHbtw5x7Fpg7RjbNyqez5IlodSNiEhErCKi53rIklLqRkQkIlYRMbgZm1DqRkQkIlaB3nM9ZD1JWqkbEZFesYqIXujRK3UjItIrXhExrwemREQGilWg93w4vFKpGxGRXrGKiJ4LhlcqdSMi0ideEbG3R6/UjYhIQewCvaZAEBHpL14RMZ8LJjVLxuu0RESORbwiYj5L1hNK3YiIRMQq0FteUyCIiAwUr4iYzwWTmil1IyLSK1YR0TTqRkTkKEVNU3yyMM9p1I3IFNXT00NLSwtdXV0T3ZTjKpPJ0NDQQDpd/NrYsQr0vePoNQWCyJTT0tJCVVUVCxYswCyeMcDdaWtro6WlhcbGxqKPK6rra2bLzGybme0ws9VD1LnWzLaa2RYzeyBSnjOzzeHXUUsQjifzHHlLxvY/WUSG1tXVxYwZM2L9829mzJgxY9R/tYzYozezJHA3cBnBYt8bzWy9u2+N1FkIrAEucPc3zWxm5C0Ou/uiUbVqjBKeI0/yRHwrEZmE4hzkC8ZyjsX06JcAO9x9p7t3Aw8CKwbU+Thwt7u/CeDub4y6JePAPIsnFOhFRKKKCfRzgV2R7ZawLOoM4Awz+3cze9rMlkX2ZcysOSy/+hjbO6yE53CL120HETk57N+/n29961ujPm758uXs37//OLSoTzGBfrC/E3zAdgpYCFwCXAfcb2Y14b757t4EXA/caWanHfUNzFaFvwyaW1tbi278QAnP4qYevYiceEMF+lwuN+xxGzZsoKamZtg6x6qY7m8LMC+y3QDsGaTO0+7eA7xsZtsIAv9Gd98D4O47zewXwGLgpejB7n4fcB9AU1PTwF8iRUt4jnxCPXqRqe62f9nC1j0Hx/U9z54zja9+4Jwh969evZqXXnqJRYsWkU6nqaysZPbs2WzevJmtW7dy9dVXs2vXLrq6uvjMZz7DqlWrAFiwYAHNzc20t7dzxRVX8N73vpcnn3ySuXPn8sgjj1BWVnbMbS+mR78RWGhmjWZWAqwEBo6e+RHwPgAzqyNI5ew0s1ozK42UXwBs5ThR6kZEJsodd9zBaaedxubNm/nGN77BM888w9e//nW2bg1C3tq1a9m0aRPNzc3cddddtLW1HfUe27dv51Of+hRbtmyhpqaGhx9+eFzaNmJUdPesmd0IPAYkgbXuvsXMbgea3X19uG+pmW0FcsAX3L3NzN4D/J2Z5Ql+qdwRHa0z3hKeA/XoRaa84XreJ8qSJUv6jXW/6667+OEPfwjArl272L59OzNmzOh3TGNjI4sWBYMU3/nOd/LKK6+MS1uKioruvgHYMKDslshrBz4XfkXrPAm8/dibWZwEOY26EZFJoaKiovf1L37xC372s5/x1FNPUV5eziWXXDLoWPjS0tLe18lkksOHD49LW+IzV0A+TwJX6kZEJkRVVRWHDh0adN+BAweora2lvLycF154gaeffvqEti0+UTGfBcCUuhGRCTBjxgwuuOAC3va2t1FWVsasWbN69y1btox7772Xc889lzPPPJN3vetdJ7Rt8YmKYaAnqdSNiEyMBx54YNDy0tJSHn300UH3FfLwdXV1PPfcc73lN99887i1K0apmyDQK3UjItJf7AK9UjciIv3FKNDn6CYFSQV6EZGo+AT6ynqWVqzjqenXTHRLREQmlfgEeqAn56S1XqyISD+xioo9uTwlWi9WRKSf2AV6rRcrIhNhrNMUA9x55510dnaOc4v6xCoqZpW6EZEJMpkDfayGqHTn8loYXETg0dXw2rPj+56nvB2uuGPI3dFpii+77DJmzpzJQw89xJEjR7jmmmu47bbb6Ojo4Nprr6WlpYVcLsdXvvIVXn/9dfbs2cP73vc+6urqeOKJJ8a33cQs0Pfk8urRi8iEuOOOO3juuefYvHkzjz/+OOvWreOZZ57B3bnqqqv45S9/SWtrK3PmzOEnP/kJEMyBU11dzTe/+U2eeOIJ6urqjkvbYhPoc3kn7yjQi8iwPe8T4fHHH+fxxx9n8eLFALS3t7N9+3YuvPBCbr75Zr74xS9y5ZVXcuGFF56Q9sQm0Pfk8gCklLoRkQnm7qxZs4ZPfOITR+3btGkTGzZsYM2aNSxdupRbbrllkHcYX7Hp/hYCfYl69CIyAaLTFF9++eWsXbuW9vZ2AHbv3s0bb7zBnj17KC8v58Mf/jA333wzv/nNb4469niITY8+mwuWmtXNWBGZCNFpiq+44gquv/563v3udwNQWVnJ9773PXbs2MEXvvAFEokE6XSae+65B4BVq1ZxxRVXMHv27ONyM9aCxaFGqGS2DPjfBEsJ3u/uRyXAzOxa4FbAgd+5+/Vh+Q3Al8Nq/9XdvzPc92pqavLm5ubRnAMABw738Nc/fJZrm+Zx8Rn1oz5eRE5uzz//PGedddZEN+OEGOxczWyTuzcNVn/EHr2ZJYG7gcuAFmCjma2Prv1qZguBNcAF7v6mmc0My6cDXwWaCH4BbAqPfXNMZzeM6rI0d19/3ni/rYjISa+YhPYSYIe773T3buBBYMWAOh8H7i4EcHd/Iyy/HPipu+8L9/0UWDY+TRcRkWIUE+jnArsi2y1hWdQZwBlm9u9m9nSY6in2WMxslZk1m1lza2tr8a0XEYkoJhV9shvLORYT6Ae7uznwO6WAhcAlwHXA/WZWU+SxuPt97t7k7k319cqvi8joZTIZ2traYh3s3Z22tjYymcyojitm1E0LMC+y3QDsGaTO0+7eA7xsZtsIAn8LQfCPHvuLUbVQRKQIDQ0NtLS0EPesQCaToaGhYVTHFBPoNwILzawR2A2sBK4fUOdHBD35b5tZHUEqZyfwEvDfzKw2rLeU4KatiMi4SqfTNDY2TnQzJqURA727Z83sRuAxguGVa919i5ndDjS7+/pw31Iz2wrkgC+4exuAmX2N4JcFwO3uvu94nIiIiAyuqHH0J9JYx9GLiExlw42j13wBIiIxN+l69GbWCvzhGN6iDtg7Ts2JE12XoenaDE7XZXCT9bq8xd0HHbY46QL9sTKz5qH+fJnKdF2GpmszOF2XwZ2M10WpGxGRmFOgFxGJuTgG+vsmugGTlK7L0HRtBqfrMriT7rrELkcvIiL9xbFHLyIiEQr0IiIxF5tAb2bLzGybme0ws9UT3Z7jzczmmdkTZva8mW0xs8+E5dPN7Kdmtj38tzYsNzO7K7w+vzez8yLvdUNYf3u4IlgsmFnSzH5rZj8OtxvN7Nfhef7AzErC8tJwe0e4f0HkPdaE5dvM7PKJOZPxY2Y1ZrbOzF4IPzvv1mcmYGY3hT/wHvhCAAADVUlEQVRLz5nZ980sE5vPjLuf9F8Ec/C8BJwKlAC/A86e6HYd53OeDZwXvq4CXgTOBv4HsDosXw38Tfh6OfAowdTR7wJ+HZZPJ5iAbjpQG76unejzG6dr9DngAeDH4fZDwMrw9b3AfwlffxK4N3y9EvhB+Prs8LNUCjSGn7HkRJ/XMV6T7wAfC1+XADX6zDgE62S8DJRFPisfictnJi49+mJWwYoVd3/V3X8Tvj4EPE/wYV1B8MNM+O/V4esVwP/1wNNAjZnNJqargJlZA/AfgPvDbQPeD6wLqwy8NoVrtg64NKy/AnjQ3Y+4+8vADoLP2knJzKYBFwH/AODu3e6+H31mClJAmZmlgHLgVWLymYlLoC9qJau4Cv9sXAz8Gpjl7q9C8MsAmBlWG+oaxfXa3Qn8FZAPt2cA+909G25Hz7P3GoT7D4T143ZtTgVagf8TprTuN7MK9JnB3XcD/xP4I0GAPwBsIiafmbgE+qJWsoojM6sEHgY+6+4Hh6s6SJkPU37SMrMrgTfcfVO0eJCqPsK+uF2bFHAecI+7LwY6CFI1Q5kq14XwvsQKgnTLHKACuGKQqiflZyYugb6YVbBix8zSBEH+H939n8Pi18M/rwn/LSzUPtQ1iuO1uwC4ysxeIUjjvZ+gh18T/lkO/c+z9xqE+6uBfcTv2rQALe7+63B7HUHg12cG/hR42d1bPVgp75+B9xCTz0xcAn3vKljhXfGVwPoJbtNxFeYD/wF43t2/Gdm1HiiMgrgBeCRS/hfhSIp3AQfCP9MLi8bUhr2apWHZScvd17h7g7svIPgs/Nzd/xx4AvhQWG3gtSlcsw+F9T0sXxmOsGgkWB7zmRN0GuPO3V8DdpnZmWHRpcBW9JmBIGXzLjMrD3+2CtcmHp+Zib4bPF5fBCMEXiS4y/2liW7PCTjf9xL8Sfh7YHP4tZwgT/ivwPbw3+lhfQPuDq/Ps0BT5L3+M8FNox3Af5rocxvn63QJfaNuTiX4odsB/BNQGpZnwu0d4f5TI8d/Kbxm24ArJvp8xuF6LAKaw8/NjwhGzegzE5zTbcALwHPAdwlGzsTiM6MpEEREYi4uqRsRERmCAr2ISMwp0IuIxJwCvYhIzCnQi4jEnAK9iEjMKdCLiMTc/wcyG4tuI5ybUQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(episode, train_acc, label='train')\n",
    "plt.plot(episode, test_acc, label='test')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
